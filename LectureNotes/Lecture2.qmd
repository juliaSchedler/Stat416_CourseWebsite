---
title: "Lecture 2"
author: "Julia Schedler"
format: revealjs
slide-number: true
scrollable: true
---

::: hidden
$$
\newcommand\E{{\mathbb{E}}}
$$
:::

## Recap from last time

-   Several examples of time series data sets

-   Experience plotting the time series

-   Exposure to some common time series models

## Today

-   Notation review
-   Mean and covariance function of a time series
-   R code activity
-   Stationarity (if time)

## Coming up/notices

-   I combined the Canvas sections (applies to section 2)
-   Quiz 1 posted today, due tomorrow at midnight (20 minutes to do it)
-   Assignment 1 will also be posted today, due Monday at midnight (boundary between Monday and Tuesday)
-   Next week's office hours: M 4-5, T 12-2

# Review of notation

## Notation and Data- White noise {.smaller}

"Let $w_t$ be a white noise series"

| t        | Random Variable             | Example data   |
|----------|-----------------------------|----------------|
| 1        | $w_1 \sim N(0, \sigma_w^2)$ | `{r} rnorm(1)` |
| 2        | $w_2 \sim N(0, \sigma_w^2)$ | `{r} rnorm(1)` |
|          | $\vdots$                    | $\vdots$       |
| t        | $w_t \sim N(0, \sigma^2_w)$ | `{r} rnorm(1)` |
| $\vdots$ | $\vdots$                    | $\vdots$       |
| n        | $w_n \sim N(0, \sigma_w^2)$ | `{r} rnorm(1)` |

If we interpret the collection of $w_t$ as a random vector, then $w_t \sim MVN(\vec{0}, I)$ (why $I$?)

Note: sometimes $w_t$ could mean a (univariate) value of a white noise series for a particular time $t$ (kind of like how you refer to an arbitrary $x_i$ when you have a sample $x_1, \dots, x_n$).

## [(Aside) The Multivariate normal distribution]{.r-fit-text}

Let's look on [Wikipedia](https://en.wikipedia.org/wiki/Multivariate_normal_distribution). What are the parameters?

-   mean **vector**

-   variance (covariance) **matrix**

    -   If the covariance matrix is the identity matrix, the the covariances are 0

## [(Aside) Bivariate normal distribution for uncorrelated case]{.r-fit-text}

```{r}
#| code-fold: true
#| echo: true
# install.packages("ggplot2")
# install.packages("ggExtra")
library(ggplot2)
library(ggExtra)

x1 <- rnorm(100, 10, 5)
x2 <- rnorm(100, .1, .5)

x <- data.frame(x1, x2)
# Save the scatter plot in a variable
p <- ggplot(x, aes(x = x1, y = x2)) +
  geom_point()

# Arguments for each marginal histogram
ggMarginal(p, type = "histogram", 
           xparams = list(fill = 4),
           yparams = list(fill = 3))
```

## [(Aside) Bivariate normal distribution for correlated case]{.r-fit-text}

```{r}
#| code-fold: true
#| echo: true
#install.packages("MASS")
library(MASS)

mu <- c(10, .1)
varcov <- matrix(c(5, 1, 1, .5), 
                 ncol = 2)
x<- mvrnorm(100, mu = mu, Sigma =varcov)
x <- data.frame(x1 = x[,1], x2 = x[,2])
# Save the scatter plot in a variable
p <- ggplot(x, aes(x = x1, y = x2)) +
  geom_point()

# Arguments for each marginal histogram
ggMarginal(p, type = "histogram", 
           xparams = list(fill = 4),
           yparams = list(fill = 3))
```

## [Building time series models from White Noise]{.r-fit-text} {.smaller}

| Model                          | Inputs                                                          | Output |
|------------------------|------------------------|------------------------|
| White noise                    | probability distribution, independence assumption, $\sigma_w^2$ |        |
| Moving average with $p$ points | $w_1, w_2, \dots, w_n$                                          |        |
| Autoregression of order $p$    | $w_1, w_2, \dots, w_n$ and $\phi = (\phi_1, \dots, \phi_p)$     |        |
| Random walk with drift         | $w_1, w_2, \dots, w_n$ and $\delta$                             |        |
| Signal plus noise              | $w_1, w_2, \dots, w_n$ and a function $f(t)$                    |        |

Identify which of the inputs are random variables, pre-specified constants, pre-specified functions, or parameters to be estimated.

## [Building time series models from White Noise]{.r-fit-text} {.smaller}

| Model                          | Inputs                                                          | Output                                                                                 |
|------------------------|------------------------|------------------------|
| White noise                    | probability distribution, independence assumption, $\sigma_w^2$ | $w_1, w_2, \dots, w_n$; for each $t = 1, \dots, n$ we have $w_t \sim N(0, \sigma^2_w)$ |
| Moving average with $p$ points | $w_1, w_2, \dots, w_n$                                          | $v_t = \frac{1}{p}\sum_{i = 1}^{p} w_{t-(p-i)}$                                        |
| Autoregression of order $p$    | $w_1, w_2, \dots, w_n$ and $\phi = (\phi_1, \dots, \phi_p)$     | $x_t = \sum_{i = 1}^p \phi_ix_{t-i} + w_t$                                             |
| Random walk with drift         | $w_1, w_2, \dots, w_n$ and $\delta$                             | $x_t = \delta + x_{t-1} + w_t$                                                         |
| Signal plus noise              | $w_1, w_2, \dots, w_n$ and a function $f(t)$                    | $x_t = f(t) + w_t$                                                                     |

Identify which of the inputs are random variables, pre-specified constants, pre-specified functions, or parameters to be estimated.

## Notation and Data

Consider the general version of the autoregressive model of order 1:

$$
x_t = \phi_1x_{t-1} + \phi_2x_{t-2} + w_t
$$

If you had data representing this process, what would it look like in R?

## Notation and Data

Suppose $\phi_1 = 1.5$ and $\phi_2 = -0.75$.

```{r}
#| code-fold: true
#| echo: true

set.seed(90210)
w = rnorm(250 + 50) # 50 extra to avoid startup problems
x = filter(w, filter=c(1.5,-.75), method="recursive")[-(1:50)]
x
```

## R example - Moving Average {.smaller}

::: columns
::: {.column width="40%"}
```{r}
#| echo: true
#| code-fold: show

set.seed(70)

# generate white noise
w_t <- rnorm(10, 0, 1)

## manually lag terms
w_t1 <- c(NA, w_t[1:9])
w_t2 <- c(NA, NA, w_t[1:8])

## manually compute MA(3)
v_t <- (w_t + w_t1 + w_t2)/3

## compare the vectors
ma_3 <- cbind(v_t, w_t, w_t1, w_t2)
round(ma_3, 3)
```
:::

::: {.column width="60%"}
```{r}
## plot
#par(mfrow = 2:1)
plot(1:10, w_t, type = "b", lwd = 2, pch = 16, col = "darkgrey")
points(1:10, v_t, type = "b", lwd = 2, pch = 17, col = "blueviolet")
```
:::
:::

## R example - Moving Average {.smaller}

::: columns
::: {.column width="40%"}
```{r}
#| echo: true
#| code-fold: show

# generate white noise
n = 50
w_t <- rnorm(n, 0, 1)

## manually lag terms
w_t1 <- c(NA, w_t[1:(n-1)])
w_t2 <- c(NA, NA, w_t[1:(n-2)])

## manually compute MA(3)
v_t <- (w_t + w_t1 + w_t2)/3

## compare the vectors
ma_3 <- cbind(v_t, w_t, w_t1, w_t2)
round(ma_3, 3)
```
:::

::: {.column width="60%"}
```{r}
## plot
#par(mfrow = 2:1)
plot(1:n, w_t, type = "b", lwd = 2, pch = 16, col = "darkgrey")
points(1:n, v_t, type = "b", lwd = 2, pch = 17, col = "blueviolet")
```
:::
:::

# Chapter 2: Correlation and Stationary Time Series

## Motivation

How do we summarize characteristics of a distribution?

```{r}
set.seed(2024)
x <- rnorm(1000, 10, 1)
y <- rnorm(1000, 8, .75)
z <- c(x,y)
hist(z)
```

## Motivation

How do we summarize characteristics of a distribution?

-   mean

-   variance(standard deviation)

```{r}
hist(z)
abline(v = mean(z), col = "red", lwd = 2)
abline(v = mean(z) + sd(z), col = "blue", lty = 2, lwd = 2)
abline(v = mean(z) - sd(z), col = "blue", lty = 2, lwd = 2)
text(x = 11, y = 225, 
     labels = paste("mean = ", round(mean(z),2), 
                    "\n sd = ", round(sd(z),2)))
```

## How do we summarize the characteristics of a distribution that changes over time?

-   mean function (of time)

-   (auto)(co)variance function (of time)

# Mean function

## Mean Function

The mean function of a time series $x_t$ is defined as:

$$
\mu_{xt} = \E(x_t) = \int_{-\infty}^\infty x_tf(x_t)dx_t,
$$

where $\E$ is the expected value operator, shown here for the case of a continuous $x_t$.\

So, for example, if $x_t$ is normally distributed then $f$ here would be the normal probability density function (p.d.f.).

## Visual examples

```{r}
library(astsa)
set.seed(314159265) # so you can reproduce the results
w  = rnorm(200)  ## Gaussian white noise
x  = cumsum(w)
wd = w +.3 
xd = cumsum(wd)
tsplot(xd, ylim=c(-2,80), main="random walk", ylab="", col=4)
 clip(0, 200, 0, 80)
 abline(a=0, b=.3, lty=2, col=4) # drift
lines(x, col=6)
 clip(0, 200, 0, 80)
 abline(h=0, col=6, lty=2)
```

## Visual examples

```{r}
# cs = 2*cos(2*pi*(1:500)/50 + .6*pi)    # as in the text
cs = 2*cos(2*pi*(1:500+15)/50)           # same thing 
w  = rnorm(500,0,1)
par(mfrow=c(3,1))   
tsplot(cs + w, ylab="", main = expression(x[t]==2*cos(2*pi*t/50+.6*pi)+N(0,1)))
points(cs, ylab="", main = expression(x[t]==2*cos(2*pi*t/50+.6*pi)), type = "l", col = "blueviolet", lwd = 2)
tsplot(cs + 5*w, ylab="", main = expression(x[t]==2*cos(2*pi*t/50+.6*pi)+N(0,25)))
points(cs, ylab="", main = expression(x[t]==2*cos(2*pi*t/50+.6*pi)), type = "l", col = "blueviolet", lwd = 2)

```

## Notating the mean function

*"When no confusion exists about which time series we are referring to, we will drop a subscript and write* $\mu_{xt}$ *as* $\mu_t$*."*

Confusion might exist if we have two time series e.g. if

-   $x_t$ is the SOI for a given month and

-   $y_t$ is the estimated new fish for a given month

we would have two mean functions, $\mu_{yt}$ and $\mu_{xt}$.

## Example 2.2 Mean Function of a Moving Average Series

Let $w_t$ denote a white noise series.

-   What is $\mu_{wt} = \E(w_t)$ ?

Let $v_t = \frac{1}{3}(w_{t-1} + w_{t} + w_{t+1})$ .

-   What is $\mu_{vt} = \E(v_t)$?

(why not just write $\mu_t$ on this slide?)

## Example 2.3 Mean Function of a Random Walk with drift

Look, it's our friend the random walk with drift (maybe):

$$
x_t = \delta t + \sum_{j = 1}^t w_j
$$

What is the mean function of $x_t$?

## Break

-   I saw several turkeys this morning
-   I saw someone almost die on a skateboard

# Autocovariance

## Does the mean function tell us anything about the (in)dependence of the time series?

No (expected value is such a friendly operator!)

## Review: Variance and Covariance

If $X$ is a random variable and then $\E(X) = \mu$,

$$
Var(X) = \E((X-\mu)^2)
$$ If we have two random variables $X_\alpha$ and $X_\beta$, the covariance between these is $$
Cov(X_\alpha, X_\beta) = \E[(X_\alpha - \mu_\alpha)(X_\beta - \mu_\beta)]
$$ - remember correlation (scaled covariance)??

## Autocovariance function

The autocovariance function is defined as the second moment product $$
\gamma_x(s, t) = cov(x_s, x_t) = \E[(x_s - \mu_s)(x_t - \mu_t)]
$$ for all $s$ and $t$.

-   When no confusion exists, we will drop the $x$ as with the mean function i.e. $\gamma(s, t)$ instead of $\gamma_x(s, t)$
-   How can we write $var(x_t)$ in terms of $\gamma$? $$
    \gamma_x(t,t) = \E[(x_t - \mu_t)^2] = var(x_t) 
    $$

## Example 2.6 Autocovariance of White Noise

$w_t$ ⬅️ white noise series

$\gamma_w(s, t) = cov(w_s, w_t) = \text{ }?$

## Example 2.6 Autocovariance of White Noise

$w_t$ ⬅️ white noise series

$\gamma_w(s, t) = cov(w_s, w_t) =  \begin{cases} \sigma^2_w & \text{ if } s = t\\ 0 & \text{ if } s \ne t \end{cases}$

## Example 2.8 Autocovariance of a Moving Average

Consider three point moving average $v_t = \frac{1}{3}(w_{t-1} + w_t + w_{t+1})$

$\gamma_v(s, t) = cov(v_s, v_t) =  \begin{cases}a & \text{ if } s = t\\ b & \text{ if } \vert s-t \vert = 1 \\c& \text{ if } \vert s-t \vert =2 \\ d & \text{ if } \vert s - t\vert > 2\end{cases}$

-   Which one of $a, b, c, d$ is 0
-   Are $a, b, c$ the same? If not, which is largest?

## Example 2.8 Autocovariance of a Moving Average

Consider three point moving average $v_t = \frac{1}{3}(w_{t-1} + w_t + w_{t+1})$

$\gamma_v(s, t) = cov(v_s, v_t) =  \begin{cases}\frac{3}{9}\sigma^2_w & \text{ if } s = t\\ \frac{2}{9}\sigma^2_w & \text{ if } \vert s-t \vert = 1 \\\frac{1}{9}\sigma^2_w & \text{ if } \vert s-t \vert =2 \\0 & \text{ if } \vert s - t\vert > 2\end{cases}$

Does this equation make intuitive sense?

## Example 2.9 Autocovariance of a Random Walk

$x_t = \sum_{j = 1}^t w_j$

$$
\gamma_x(s, t) = cov(x_s, x_t) = cov\left ( \sum_{j=1}^s w_j, \sum_{k = 1}^t w_k\right ) = \min\{s,t\}\sigma^2_w
$$ If you waaaaant, property 2.7 and figuring out what $\E(w_jw_k)$ is for $j \ne k$

# In-class R problems

## R example - Moving Average {.smaller}

```{r}
#| echo: true
#| code-fold: show
#| eval: false

# generate white noise
n = 50
w_t <- rnorm(n, 0, 1)

## manually lag terms
w_t1 <- c(NA, w_t[1:(n-1)])
w_t2 <- c(NA, NA, w_t[1:(n-2)])

## manually compute MA(3)
v_t <- ?

## compare the vectors
ma_3 <- cbind(v_t, w_t, w_t1, w_t2, w_t3)
round(ma_3, 3)

## also compute MA(3) using stats::filter
v_t_alt <- ?
  
## plot both
```

## R example - Moving Average {.smaller}

::: columns
::: {.column width="40%"}
```{r}
#| echo: true
#| code-fold: show

# generate white noise
n = 50
w_t <- rnorm(n, 0, 1)

## manually lag terms
w_t1 <- c(NA, w_t[1:(n-1)])
w_t2 <- c(NA, NA, w_t[1:(n-2)])
w_t3 <- c(NA, NA, NA, w_t[1:(n-3)])
## manually compute MA(3)
v_t <- (w_t + w_t1 + w_t2 + w_t3)/4

## compare the vectors
ma_3 <- cbind(v_t, w_t, w_t1, w_t2, w_t3)
round(ma_3, 3)

## also compute using stats::filter
v_t_alt <- stats::filter(w_t, sides = 2, filter = rep(0.25, times = 4))
```
:::

::: {.column width="60%"}
```{r}
## plot
#par(mfrow = 2:1)
plot(1:n, w_t, type = "b", lwd = 2, pch = 16, col = "darkgrey")
points(1:n, v_t, type = "b", lwd = 2, pch = 17, col = "blueviolet")
plot(1:n, w_t, type = "b", lwd = 2, pch = 16, col = "darkgrey")
points(1:n, v_t_alt, type = "b", lwd = 2, pch = 17, col = "darkgreen")

```
:::
:::

# Problem 1.1

## Moving Averages (Problem 1.1)

-   using a method similar to the code in Example 1.9, generate 100 observations from the autoregression $$
    x_t = -0.9x_{t-2} + w_t\text{, }\\ w_t\sim N(0, 1)
    $$
-   Write down an expression for $\phi$ for this autoregression. How is $\phi$ different from the autoregression in Example 1.9?

## Moving Averages (Problem 1.1 Part a)

-   Apply the moving average filter to the autoregression data you generated $$
    v_t = (x_t + x_{t-1} + x_{t-2} + x_{t-4})
    $$

-   Plot $x_t$ as points and lines and $v_t$ as a line.

## Moving Averages (Problem 1.1)

```{r}
#| code-fold: true
#| echo: true
library(astsa)
w = rnorm(150,0,1) # 50 extra to avoid startup problems
xa = filter(w, filter=c(0,-.9), method="recursive")[-(1:50)] # AR
va = filter(xa, rep(1,4)/4, sides=1) # moving average
tsplot(xa, main="autoregression", type = "b", pch = 16)
lines(va, col="blueviolet", lwd = 2)
```

## Moving Averages (Problem 1.1 part b)

-   Repeat the application of the MA filter but instead of starting with an autoregression, generate data $x_t$ according to the signal plus noise model $$
    x_t = 2\cos(2\pi t/4) + w_t\\ w_t \sim N(0,1)
    $$

## Moving Averages (Problem 1.1 part b)

```{r}
#| code-fold: true
#| echo: true
xb = 2*cos(2*pi*(1:100)/4) + rnorm(100,0,1) # sinusoid + noise
vb = filter(xb, rep(1,4)/4, sides=1) # moving average
tsplot(xb, main="sinusoid + noise", type = "b", pch = 16)
lines(vb, col="blueviolet", lwd = 2)
```

## Moving Averages (Problem 1.1 part c)

-   Repeat the application of the MA filter but instead of starting with an autoregression, use the Johnson and Johnson data from Lecture 1.

## Moving Averages (Problem 1.1 part c)

```{r}
#| code-fold: true
#| echo: true
xc = log(jj)
vc = stats::filter(xc, filter = rep(1,4)/4, sides=1, method = "convolution") # moving average
tsplot(xc, main="johnson and johnson (log scale)", type = "b", pch = 16)
lines(vc, col="blueviolet", lwd = 2)
```

## Stationarity

A time series is stationary if

-   the mean function ($\mu_t$) is constant and does not depend on time $t$
-   the autocovariance function ($\gamma(s,t)$) depends on $s$ and $t$ only though their difference

## Example 2.14 Stationarity of a Random Walk

Look, it's our friend the random walk:

$$
x_t = \delta t + \sum_{j = 1}^t w_j
$$ Their mean function is $\E(x_t) = 0$, and their covariance function is $\gamma_x(s, t) = \min\{s,t\}\sigma^2_w$

Is a random walk stationary?
