[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Time Series!",
    "section": "",
    "text": "Welcome to Time Series!\nThe lecture slides and code here are heavily drawn from examples in book I have selected for this course: Time Series Analysis: A Data Analysis Approach Using R. I am extremely grateful to the authors for doing the heavy lifting of curating the data sets, code, and problem sets. Please consider purchasing this book for reference— it’s wonderful, and there’s a more technical graduate version as well for those of you that yearn for more technical detail.\n\nI have also added some additional examples from my own work and visualizations of simulations, covariance functions, etc., that I think would be helpful for learning these important concepts.\nAlso, big thanks to our Lab Assistant, Bena Smith, for serving as editor for these notes (assuming I get them to her in time. Don’t blame Bena for missed typos).",
    "crumbs": [
      "Home"
    ]
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series",
    "href": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-1",
    "href": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-1",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 1",
    "text": "Marginal and Joint Distributions t=8 and s = 1"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-1",
    "href": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-1",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-2",
    "href": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-2",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 2",
    "text": "Marginal and Joint Distributions t=8 and s = 2"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-2",
    "href": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-2",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-3",
    "href": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-3",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 3",
    "text": "Marginal and Joint Distributions t=8 and s = 3"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-3",
    "href": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-3",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-4",
    "href": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-4",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 4",
    "text": "Marginal and Joint Distributions t=8 and s = 4"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-4",
    "href": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-4",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-5",
    "href": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-5",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 5",
    "text": "Marginal and Joint Distributions t=8 and s = 5"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-5",
    "href": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-5",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-6",
    "href": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-6",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 6",
    "text": "Marginal and Joint Distributions t=8 and s = 6"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-6",
    "href": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-6",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-7",
    "href": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-7",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 7",
    "text": "Marginal and Joint Distributions t=8 and s = 7"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-7",
    "href": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-7",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-8",
    "href": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-8",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 8",
    "text": "Marginal and Joint Distributions t=8 and s = 8"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-8",
    "href": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-8",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-9",
    "href": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-9",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 9",
    "text": "Marginal and Joint Distributions t=8 and s = 9"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-9",
    "href": "LectureNotes/Slides/SimulateRW/index.html#all-simulated-time-series-9",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-10",
    "href": "LectureNotes/Slides/SimulateRW/index.html#marginal-and-joint-distributions-t8-and-s-10",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 10",
    "text": "Marginal and Joint Distributions t=8 and s = 10"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#visualizing-the-correlations",
    "href": "LectureNotes/Slides/SimulateRW/index.html#visualizing-the-correlations",
    "title": "Moving average autocovariance",
    "section": "Visualizing the correlations",
    "text": "Visualizing the correlations"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#are-these-all-the-correlations",
    "href": "LectureNotes/Slides/SimulateRW/index.html#are-these-all-the-correlations",
    "title": "Moving average autocovariance",
    "section": "Are these all the correlations?",
    "text": "Are these all the correlations?\nNo, just pairwise with \\(x_8\\). We could do all possible pairs:"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#distribution-of-all-the-correlations",
    "href": "LectureNotes/Slides/SimulateRW/index.html#distribution-of-all-the-correlations",
    "title": "Moving average autocovariance",
    "section": "Distribution of all the correlations:",
    "text": "Distribution of all the correlations:"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#d-version-of-histogram-includes-s-t-plane",
    "href": "LectureNotes/Slides/SimulateRW/index.html#d-version-of-histogram-includes-s-t-plane",
    "title": "Moving average autocovariance",
    "section": "3D version of histogram (includes \\(s-t\\) plane)",
    "text": "3D version of histogram (includes \\(s-t\\) plane)"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#simulated-auto-correlation-function-hatgamma_xh-with-a-blanket",
    "href": "LectureNotes/Slides/SimulateRW/index.html#simulated-auto-correlation-function-hatgamma_xh-with-a-blanket",
    "title": "Moving average autocovariance",
    "section": "Simulated auto correlation function \\(\\hat{\\gamma}_x(h)\\) (with a blanket)",
    "text": "Simulated auto correlation function \\(\\hat{\\gamma}_x(h)\\) (with a blanket)"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateRW/index.html#theoretical-hatgamma_xh-using-the-derived-formula-with-a-blanket",
    "href": "LectureNotes/Slides/SimulateRW/index.html#theoretical-hatgamma_xh-using-the-derived-formula-with-a-blanket",
    "title": "Moving average autocovariance",
    "section": "Theoretical \\(\\hat{\\gamma}_x(h)\\) (using the derived formula (with a blanket)",
    "text": "Theoretical \\(\\hat{\\gamma}_x(h)\\) (using the derived formula (with a blanket)"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-1",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-1",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 1",
    "text": "Marginal and Joint Distributions t=8 and s = 1"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-1",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-1",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-2",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-2",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 2",
    "text": "Marginal and Joint Distributions t=8 and s = 2"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-2",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-2",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-3",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-3",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 3",
    "text": "Marginal and Joint Distributions t=8 and s = 3"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-3",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-3",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-4",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-4",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 4",
    "text": "Marginal and Joint Distributions t=8 and s = 4"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-4",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-4",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-5",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-5",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 5",
    "text": "Marginal and Joint Distributions t=8 and s = 5"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-5",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-5",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-6",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-6",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 6",
    "text": "Marginal and Joint Distributions t=8 and s = 6"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-6",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-6",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-7",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-7",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 7",
    "text": "Marginal and Joint Distributions t=8 and s = 7"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-7",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-7",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-8",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-8",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 8",
    "text": "Marginal and Joint Distributions t=8 and s = 8"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-8",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-8",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-9",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-9",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 9",
    "text": "Marginal and Joint Distributions t=8 and s = 9"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-9",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#all-simulated-time-series-9",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-10",
    "href": "LectureNotes/Slides/SimulateAutocov/index.html#marginal-and-joint-distributions-t8-and-s-10",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 10",
    "text": "Marginal and Joint Distributions t=8 and s = 10"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#recap",
    "href": "LectureNotes/Slides/Lecture4/index.html#recap",
    "title": "Lecture 4",
    "section": "Recap",
    "text": "Recap\n\n\\[\n\\newcommand\\E{{\\mathbb{E}}}\n\\]\n\n\nDecomposing time series\nStationarity (theoretically and with data)\nSome activities"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#today",
    "href": "LectureNotes/Slides/Lecture4/index.html#today",
    "title": "Lecture 4",
    "section": "Today",
    "text": "Today\n\nFinish up activities from Lecture 3\nTrend stationarity\nVisualizing autocovariance (third attempt)\n“Office hours”"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#reminders",
    "href": "LectureNotes/Slides/Lecture4/index.html#reminders",
    "title": "Lecture 4",
    "section": "Reminders",
    "text": "Reminders\n\nSyllabus participation policy\nAssignments: Going forward, must submit rendered pdf of code portion. (if you want to be nice to me, do this for Assignment 2, but starts with Assignment 3)\nLate quizzes: Going forward: Email me ahead of time, otherwise it’s a 0\nExam details: No use of computer, code will be covered but basic, notes sheet is allowed, practice test will be provided"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#activity-1-export-price-of-salmon-example-3.1",
    "href": "LectureNotes/Slides/Lecture4/index.html#activity-1-export-price-of-salmon-example-3.1",
    "title": "Lecture 4",
    "section": "Activity 1: Export Price of Salmon (Example 3.1)",
    "text": "Activity 1: Export Price of Salmon (Example 3.1)\n\n\nCode\nlibrary(astsa)\n\nsummary(fit &lt;- lm(salmon~time(salmon), na.action=NULL))\n## \n## Call:\n## lm(formula = salmon ~ time(salmon), na.action = NULL)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.69187 -0.62453 -0.07024  0.51561  2.34959 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  -503.08947   34.44164  -14.61   &lt;2e-16 ***\n## time(salmon)    0.25290    0.01713   14.76   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8814 on 164 degrees of freedom\n## Multiple R-squared:  0.5706, Adjusted R-squared:  0.568 \n## F-statistic: 217.9 on 1 and 164 DF,  p-value: &lt; 2.2e-16\ntsplot(salmon, col=4, ylab=\"USD per KG\", main=\"Salmon Export Price\")\nabline(fit)"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#activity-1-export-price-of-salmon-example-3.1-1",
    "href": "LectureNotes/Slides/Lecture4/index.html#activity-1-export-price-of-salmon-example-3.1-1",
    "title": "Lecture 4",
    "section": "Activity 1: Export Price of Salmon (Example 3.1)",
    "text": "Activity 1: Export Price of Salmon (Example 3.1)\n\n\n\nDoes this time series appear stationary?\nThe (mathematical) equation in the book for the trend line above is:\n\\[\nx_t = \\beta_0 + \\beta_1z_t + w_t, z_t = 2003\\frac{8}{12}, 2001\\frac{8}{12}, \\dots, 2017\\frac{5}{12}\n\\]\n\nThere is a typo in this equation. Correct the typo. (hint: examine to the first few entries of time(salmon)\nWhy are there fractions of the year? Explain what the fractional values mean and describe how they appear in the data set within R.\n\nInterpret the estimate of the slope."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#activity-1-export-price-of-salmon-example-3.1-2",
    "href": "LectureNotes/Slides/Lecture4/index.html#activity-1-export-price-of-salmon-example-3.1-2",
    "title": "Lecture 4",
    "section": "Activity 1: Export Price of Salmon (Example 3.1)",
    "text": "Activity 1: Export Price of Salmon (Example 3.1)\n\nDoes this time series appear stationary?\n\nNo, the mean function is clearly increasing.\n\nThe (mathematical) equation in the book for the trend line above is:\n\\[\nx_t = \\beta_0 + \\beta_1z_t + w_t, z_t = 2003\\frac{8}{12}, 2001\\frac{8}{12}, \\dots, 2017\\frac{5}{12}\n\\]\n\nThere is a typo in this equation. Correct the typo. (hint: examine to the first few entries of time(salmon)\n\n\n[1] 2003.667 2003.750 2003.833 2003.917 2004.000 2004.083\n\n\n[1] 2003.667 2003.750 2003.833 2003.917 2004.000 2004.083\n\n\nThe typo is the second year, it should be \\(2003\\frac{9}{12}\\)\nWhy are there fractions of the year? Explain what the fractional values mean and describe how they appear in the data set within R.\nNote that we are dividing by 12, so the fractions represent the months. Based on the values of time(salmon), January corresponds to \\(\\frac{0}{12}\\), so for example September is represented as \\(\\frac{8}{12}\\).\n\nInterpret the estimate of the slope.\n\nEach year, the expected export price of Norwegian salmon increases by 0.2592 USD per kg\nEach month, the export price of Norwegian salmon increases by 0.2592/12 USD per kg on average"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#activity-2-trend-stationarity-example-2.19",
    "href": "LectureNotes/Slides/Lecture4/index.html#activity-2-trend-stationarity-example-2.19",
    "title": "Lecture 4",
    "section": "Activity 2: Trend Stationarity (Example 2.19)",
    "text": "Activity 2: Trend Stationarity (Example 2.19)\nConsider the time series model \\[x_t = \\beta t + y_t\\] Assume \\(y_t\\) is stationary with mean function \\(\\mu_y\\) and and autocovariance function \\(\\gamma_y(h)\\)\n\nCompare this equation to the regression equation in the last example.\nWhat are the mean function and autocovariance function of \\(x_t\\)?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#activity-2-solutions-trend-stationarity-example-2.19",
    "href": "LectureNotes/Slides/Lecture4/index.html#activity-2-solutions-trend-stationarity-example-2.19",
    "title": "Lecture 4",
    "section": "Activity 2 Solutions: Trend Stationarity (Example 2.19)",
    "text": "Activity 2 Solutions: Trend Stationarity (Example 2.19)\nConsider the time series model \\[x_t = \\beta t + y_t\\] Assume \\(y_t\\) is stationary with mean function \\(\\mu_y\\) and and autocovariance function \\(\\gamma_y(h)\\)\n\nCompare this equation to the regression equation in the last example.\n\nThe equations are similar, with \\(x_t\\) being the same, \\(\\beta_1 = \\beta\\), \\(\\beta_0 = 0\\), and \\(w_t = y_t\\), and \\(z_t = t\\). - What are the mean function and autocovariance function of \\(x_t\\)?\nFor the Mean\n\\[\n\\E(x_t)  = \\E(\\beta t + y_t) = \\E(\\beta t) + \\E(y_t) = \\beta t + \\mu_y\n\\] For the Autocovariance \\[\n\\begin{align}\n\\gamma_x(h) = cov(x_{t+h}, x_t) &= \\E[(x_{t+h} - \\mu_{x,t+h})(x_t - \\mu_{x,t})] \\\\\n&= \\E[\\left( [\\beta (t+h) + y_{t+h}] - [\\beta (t+h) + \\mu_y] \\right)\\left( [\\beta (t) + y_{t}] - [\\beta (t) + \\mu_y] \\right)] \\\\\n&=  \\E[(y_{t+h} - \\mu_y)(y_t - \\mu_y)] \\\\\n&= \\gamma_y(h)\n\\end{align}\n\\]"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#trend-stationarity-model",
    "href": "LectureNotes/Slides/Lecture4/index.html#trend-stationarity-model",
    "title": "Lecture 4",
    "section": "Trend stationarity model",
    "text": "Trend stationarity model\nA time series which is nonstationary in the mean but is stationary in the autocovariance is sometimes called Trend stationarity.\n\nI’m actually not sure if it just refers to linear trends?? I’ll ask people at my conference"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#visualizing-the-autocovariance-for-trend-stationarity",
    "href": "LectureNotes/Slides/Lecture4/index.html#visualizing-the-autocovariance-for-trend-stationarity",
    "title": "Lecture 4",
    "section": "Visualizing the autocovariance for trend stationarity",
    "text": "Visualizing the autocovariance for trend stationarity"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#simulate-many-time-series-from-the-trend-stationarity-model",
    "href": "LectureNotes/Slides/Lecture4/index.html#simulate-many-time-series-from-the-trend-stationarity-model",
    "title": "Lecture 4",
    "section": "Simulate many time series from the trend stationarity model",
    "text": "Simulate many time series from the trend stationarity model"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#simulate-many-time-series-from-the-trend-stationarity-model-1",
    "href": "LectureNotes/Slides/Lecture4/index.html#simulate-many-time-series-from-the-trend-stationarity-model-1",
    "title": "Lecture 4",
    "section": "Simulate many time series from the trend stationarity model",
    "text": "Simulate many time series from the trend stationarity model"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#simulate-many-time-series-from-the-trend-stationarity-model-2",
    "href": "LectureNotes/Slides/Lecture4/index.html#simulate-many-time-series-from-the-trend-stationarity-model-2",
    "title": "Lecture 4",
    "section": "Simulate many time series from the trend stationarity model",
    "text": "Simulate many time series from the trend stationarity model\n\n\nCode\ntsplot(all_series, spaghetti = TRUE, main = \"100 Simuated Trend Stationary Time Series\", type = \"b\")\nrect(xleft = 1.5, xright = 2.5, ybottom = -2, ytop = 4, border = \"blue\", lwd = 2)\nrect(xleft = 7.5, xright = 8.5, ybottom = -.5, ytop = 5, border = \"magenta\", lwd = 2)"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s2",
    "href": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s2",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s=2",
    "text": "Marginal and Joint Distributions t=8 and s=2"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions",
    "href": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions",
    "text": "Marginal and Joint Distributions"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series",
    "href": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-1",
    "href": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-1",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 1",
    "text": "Marginal and Joint Distributions t=8 and s = 1"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-1",
    "href": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-1",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-2",
    "href": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-2",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 2",
    "text": "Marginal and Joint Distributions t=8 and s = 2"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-2",
    "href": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-2",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-3",
    "href": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-3",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 3",
    "text": "Marginal and Joint Distributions t=8 and s = 3"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-3",
    "href": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-3",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-4",
    "href": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-4",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 4",
    "text": "Marginal and Joint Distributions t=8 and s = 4"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-4",
    "href": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-4",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-5",
    "href": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-5",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 5",
    "text": "Marginal and Joint Distributions t=8 and s = 5"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-5",
    "href": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-5",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-6",
    "href": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-6",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 6",
    "text": "Marginal and Joint Distributions t=8 and s = 6"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-6",
    "href": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-6",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-7",
    "href": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-7",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 7",
    "text": "Marginal and Joint Distributions t=8 and s = 7"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-7",
    "href": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-7",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-8",
    "href": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-8",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 8",
    "text": "Marginal and Joint Distributions t=8 and s = 8"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-8",
    "href": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-8",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-9",
    "href": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-9",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 9",
    "text": "Marginal and Joint Distributions t=8 and s = 9"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-9",
    "href": "LectureNotes/Slides/Lecture4/index.html#all-simulated-time-series-9",
    "title": "Lecture 4",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-10",
    "href": "LectureNotes/Slides/Lecture4/index.html#marginal-and-joint-distributions-t8-and-s-10",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s = 10",
    "text": "Marginal and Joint Distributions t=8 and s = 10"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#visualizing-the-correlations",
    "href": "LectureNotes/Slides/Lecture4/index.html#visualizing-the-correlations",
    "title": "Lecture 4",
    "section": "Visualizing the correlations",
    "text": "Visualizing the correlations"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#are-these-all-the-correlations",
    "href": "LectureNotes/Slides/Lecture4/index.html#are-these-all-the-correlations",
    "title": "Lecture 4",
    "section": "Are these all the correlations?",
    "text": "Are these all the correlations?\nNo, just pairwise with \\(x_8\\). We could do all possible pairs:"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#distribution-of-all-the-correlations",
    "href": "LectureNotes/Slides/Lecture4/index.html#distribution-of-all-the-correlations",
    "title": "Lecture 4",
    "section": "Distribution of all the correlations:",
    "text": "Distribution of all the correlations:"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#d-version-of-histogram-includes-s-t-plane",
    "href": "LectureNotes/Slides/Lecture4/index.html#d-version-of-histogram-includes-s-t-plane",
    "title": "Lecture 4",
    "section": "3D version of histogram (includes \\(s-t\\) plane)",
    "text": "3D version of histogram (includes \\(s-t\\) plane)"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#simulated-auto-correlation-function-hatgamma_xh-with-a-blanket",
    "href": "LectureNotes/Slides/Lecture4/index.html#simulated-auto-correlation-function-hatgamma_xh-with-a-blanket",
    "title": "Lecture 4",
    "section": "Simulated auto correlation function \\(\\hat{\\gamma}_x(h)\\) (with a blanket)",
    "text": "Simulated auto correlation function \\(\\hat{\\gamma}_x(h)\\) (with a blanket)"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#theoretical-hatgamma_xh-using-the-derived-formula-with-a-blanket",
    "href": "LectureNotes/Slides/Lecture4/index.html#theoretical-hatgamma_xh-using-the-derived-formula-with-a-blanket",
    "title": "Lecture 4",
    "section": "Theoretical \\(\\hat{\\gamma}_x(h)\\) (using the derived formula (with a blanket)",
    "text": "Theoretical \\(\\hat{\\gamma}_x(h)\\) (using the derived formula (with a blanket)"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#detrending-1",
    "href": "LectureNotes/Slides/Lecture4/index.html#detrending-1",
    "title": "Lecture 4",
    "section": "Detrending",
    "text": "Detrending\nIf a process is trend stationary (nonstationary in the mean, but stationary in the variance), can we just subtract off the trend and get back a stationary time series?\nYes, and that’s called detrending"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#activity-3-detrending-a-commodity-example-3.7",
    "href": "LectureNotes/Slides/Lecture4/index.html#activity-3-detrending-a-commodity-example-3.7",
    "title": "Lecture 4",
    "section": "Activity 3: Detrending a commodity (Example 3.7)",
    "text": "Activity 3: Detrending a commodity (Example 3.7)\n\nGiven the code to generate the plot with the trend line, how would you view the equation of the trend line?\nVisualize the de-trended series\nCompute the acf of the salmon series and the detrended series. What do you notice?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#activity-3-solution-detrending-a-commodity-example-3.7",
    "href": "LectureNotes/Slides/Lecture4/index.html#activity-3-solution-detrending-a-commodity-example-3.7",
    "title": "Lecture 4",
    "section": "Activity 3 Solution: Detrending a commodity (Example 3.7)",
    "text": "Activity 3 Solution: Detrending a commodity (Example 3.7)\n\nGiven the code to generate the plot with the trend line, how would you view the equation of the trend line?\nVisualize the de-trended series\n\n\n\n\nCall:\nlm(formula = salmon ~ time(salmon), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.69187 -0.62453 -0.07024  0.51561  2.34959 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -503.08947   34.44164  -14.61   &lt;2e-16 ***\ntime(salmon)    0.25290    0.01713   14.76   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8814 on 164 degrees of freedom\nMultiple R-squared:  0.5706,    Adjusted R-squared:  0.568 \nF-statistic: 217.9 on 1 and 164 DF,  p-value: &lt; 2.2e-16"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#detrending-a-commodity-example-3.7",
    "href": "LectureNotes/Slides/Lecture4/index.html#detrending-a-commodity-example-3.7",
    "title": "Lecture 4",
    "section": "Detrending a commodity (Example 3.7)",
    "text": "Detrending a commodity (Example 3.7)\n\nCompute the acf of the salmon series and the de-trended series"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture4/index.html#next-time",
    "href": "LectureNotes/Slides/Lecture4/index.html#next-time",
    "title": "Lecture 4",
    "section": "Next time",
    "text": "Next time\n\nCross-correlation and regression with multiple time series (\\(x_t\\) on x-axis instead of \\(t\\) on x-axis like with the salmon) (Activities at the end of Ch 2)\nActivities and examples From Chapter 3\nSmoothing (Section 3.3)"
  },
  {
    "objectID": "LectureNotes/Slides/Aside-Nile/index.html",
    "href": "LectureNotes/Slides/Aside-Nile/index.html",
    "title": "Fall 24 Stat 416 Lecture Notes",
    "section": "",
    "text": "title: Interesting note on the Nile data"
  },
  {
    "objectID": "LectureNotes/Slides/Aside-Nile/index.html#the-nile-data",
    "href": "LectureNotes/Slides/Aside-Nile/index.html#the-nile-data",
    "title": "Fall 24 Stat 416 Lecture Notes",
    "section": "The nile data",
    "text": "The nile data\n\nlibrary(astsa)\ntsplot(Nile)\n\n\n\n\n\n\n\n\nLink to article (needs Cal Poly login)\nThe data are the yearly volume of the Nile at"
  },
  {
    "objectID": "LectureNotes/Lecture7.html",
    "href": "LectureNotes/Lecture7.html",
    "title": "Lecture 7 Solutions",
    "section": "",
    "text": "\\[\n\\newcommand\\E{{\\mathbb{E}}}\n\\]",
    "crumbs": [
      "Week 4",
      "Lecture 7 Solutions"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture7.html#activity-1-solutions-fill-out-this-table",
    "href": "LectureNotes/Lecture7.html#activity-1-solutions-fill-out-this-table",
    "title": "Lecture 7 Solutions",
    "section": "Activity 1 Solutions: Fill out this table",
    "text": "Activity 1 Solutions: Fill out this table\n\n\n\n\n\n\n\n\n\nPhrase\nSymbols\nWords\nBonus content\n\n\n\n\n\\(x_t\\) are white noise\n\\(\\E(x_t) = 0\\)\\[\\gamma(s,t) = \\begin{cases} \\sigma^2_w& \\text{ if } s = t\\\\ 0& \\text{ if } s \\ne t \\end{cases}\\]\n\\(x_i\\) are i.i.d.\n“Marginally (for a given \\(t\\)), \\(x_t\\) (”\\(x\\) of \\(t\\)” or “\\(x\\) sub \\(t\\)”) follows some distribution follows some distribution that is zero on average and are uncorrelated sigma w (sigma sub w) (except for when \\(s = t\\), then the and \\(x_s\\) is independent of \\(x_t\\) for all $s, t4\n“When we talk about white noise, an assumption is independence which necessarily implies we have multiple values of \\(t\\), and for each value of \\(t\\) we have a random vector \\(x_t\\) which satisfies the distributional assumptions. Also \\[\\E(x_t) \\gamma(t,t) = \\sigma^2_w = \\text{marginal variance of }x_t  \\]”Gamma of t comma t is sigma squared w and is called the marginal variance of \\(x_t\\) , where we consider \\(x_t\\) to be a univariate random variable (think about how the word distribution was used in intro stats, that’s the setting)\n\n\n\\(x_t\\) is stationary in the mean\n\\(\\E(x_t)\\) is not a function of \\(t\\)\nThe expected value of \\(x_t\\) does not depend on time (have a \\(t\\) in the formula)\nThis basically equates to \\(x_t\\) having a horizontal trend, where the y-intercept can be any value. For white noise, for example, it’s 0.There is no temporal structure in the average behavior of \\(x\\).\n\n\n\\(x_t\\) is stationary in the auto-covariance\n\\(\\gamma_x(s,t) = \\gamma_x(h)\\) for all \\(s,t\\)\nThe autocovariance function depends only on the distance between time points, not the specific values of the time points.\nWhite noise is stationary, but if a time series \\(x_t\\) is stationary it is not necessarily white noise. For example, if \\(v_t\\) is a p-point moving average of white noise, the covariance function of \\(v_t\\) for the case when p = 3 is:\\[  \\gamma_v(s,t) = \\begin{cases}                                                                                                                                                                                                                                3/9 \\sigma^2_w & s-t = 0\\\\                                                                                                                                                                                                                                    2/9 \\sigma^2_w & \\vert s-t \\vert = 1\\\\ 1/9 \\sigma^2_w & \\vert s-t \\vert =2 \\\\                                                                                                                                                                                                                      0 & \\vert s-t \\vert &gt; 2                                                                                                                                                                                                                                     \\end{cases}                                                                                                                                                                                                                                                  \\]We can rewrite as a function of \\(h = s-t\\), meaning \\(v_t\\) is stationary in the autocovariance (or sometimes people just say covariance) This is not the same function as \\(\\gamma_x(s,t)\\) for white noise .Specifically, notice that the right hand side contains the “covariance structure”, the possible magnitudes of dependence (left of comma) for given combinations of two time points \\(s\\) and \\(t\\). This is the meaningful difference between the two functions. The left hand sides, while different because of the subscript, are just symbols for “covariance function of *” where * is the letter part of the time series in question, i.e. \\(\\gamma_v(s,t)\\) is the autocovariance function of \\(v_t\\).\n\n\n\\(x_t\\) is stationary\nBoth previous\nBoth previous\n“In general, a stationary time series will have no predictable patterns in the long-term.”- Forecasting Principles and Practice“A stationary time series is one whose statistical properties do not depend on the time at which the series is observed”- Forecasting Principles and Practice\n\n\n\\(x_t\\) has temporal structure\n\\(\\E(x_t)\\) is a function of \\(t\\) \\(\\gamma_x(s,t)\\) is nonzero for some \\(s \\ne t\\)\nOn average, the value of the time series \\(x_t\\) varies with respect to \\(t\\).\n\\(x_t\\) exhibits temporal autocorrelation, or, knowing the value of \\(x_t\\) tells us something about the likely value of \\(x_s\\) when \\(\\gamma(s,t) &gt; 0\\)\nNotice that when the time series is stationary in the mean, we say there is no temporal structure in the mean, but the autocovariance can still have temporal structure and be stationary. (If it didn’t it wouldn’t be a very interesting time series!)",
    "crumbs": [
      "Week 4",
      "Lecture 7 Solutions"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture7.html#activity-2-solutions",
    "href": "LectureNotes/Lecture7.html#activity-2-solutions",
    "title": "Lecture 7 Solutions",
    "section": "Activity 2 Solutions",
    "text": "Activity 2 Solutions\n\nCode solutions\n\nDownload the data time_series.csv from Canvas and create a sub-folder in your Lecture7 folder called Data. Read in the data. Extract y1, the first time series, and name it x_t. Save it in a data frame called all_ts.\n\n\nlibrary(astsa)\nlibrary(ggplot2)\ntime_series &lt;- read.csv(\"Data/time_series.csv\")\nx_t &lt;- time_series$y1\nall_ts &lt;- data.frame(x_t, Time = time(x_t))\n\n\nPlot the time series data set using both points and lines.\n\n\ntsplot(all_ts$x_t, type = \"b\", ylab = \"TS 1\", main = \"Observed Time Series\", pch = 16)\n\n\n\n\n\n\n\n\n\n\n(extra content)ggplot version\nlibrary(ggplot2)\nggplot(all_ts, aes(x = Time, y = x_t)) + geom_point() + geom_line() + ylab(\"TS 1\") + ggtitle(\"Observed Time Series #1\")\n\n\n\nIn separate plot, again plot the time series as just points and also plot a moving average smoother over the time series (use any \\(p\\) you’d like, but use a symmetric one). Create a data frame called all_ts with the original time series and the moving average smoother.\n\n\n\nCode\ntsplot(all_ts$x_t, type = \"b\", ylab = \"TS 1\", main = \"Moving Average Smoother (5 point)\", pch = 16 )\nall_ts$ma_x_t &lt;- stats::filter(x_t, filter =rep(1/5,5), sides = 2)\nlines(all_ts$ma_x_t, col = \"magenta\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n\n(extra content)ggplot version\nggplot(all_ts, aes(x = Time, y = x_t)) + geom_point() + geom_line() + \n  geom_line(aes(x= Time, y = ma_x_t), col = \"magenta\") +\n  ylab(\"TS 1\") + ggtitle(\"Detrended - Moving Average Smoother (5 point)\")\n\n\n\nDetrend the time series with respect to the moving average estimate and plot the de-trended time series as points and lines. Save the detrended series in all_ts.\n\n\nall_ts$detrended_ma &lt;- all_ts$x_t - all_ts$ma_x_t\n\ntsplot(all_ts$detrended, type = \"b\", ylab = \"TS 1\", main = \"Detrended - Moving Average Smoother (5 point)\", pch = 16 )\n\n\n\n\n\n\n\n\n\n\n(extra content)ggplot version\nggplot(all_ts, aes(x = Time, y = detrended_ma)) + geom_point() + geom_line() + \n  ylab(\"TS 1\") + ggtitle(\"Detrended - Moving Average Smoother (5 point)\")\n\n\n\nIn a separate plot, again plot the time series as points and also plot a simple linear regression line using time(x) where x is the time series you are analyzing. Add the fitted values to the all_ts data frame.\n\n\nlm_x_t &lt;- lm(x_t~Time, data = all_ts)\ntsplot(all_ts$x_t, type = \"b\", ylab = \"TS 1\", main = \"Linear Trend\", pch = 16)\nall_ts$reg_x_t &lt;- lm_x_t$fitted.values\nlines(all_ts$reg_x_t, col = \"turquoise\", lwd = 2)\n\n\n\n\n\n\n\n\n\n\n(extra content)ggplot version\nggplot(all_ts, aes(x = Time, y = x_t)) + geom_point() + geom_line() + \n  ylab(\"TS 1\") + ggtitle(\"Linear Trend\") + geom_line(aes(x= Time, y = reg_x_t), col = \"turquoise\") \n\n\n\nDetrend the time series with respect to the regression and plot and save the detrended series (save in the all_ts data frame).\n\n\nall_ts$detrended_reg &lt;- all_ts$x_t - all_ts$reg_x_t\ntsplot(all_ts$detrended_reg, type = \"b\", ylab = \"TS 1\", main = \"Detrended - Linear Trend\", pch = 16 )\n\n\n\n\n\n\n\n\n\n\n(extra content)ggplot version\nggplot(all_ts, aes(x = Time, y = detrended_reg)) + geom_point() + geom_line() + \n  ylab(\"TS 1\") + ggtitle(\"Detrended - Linear Trend\")\n\n\n\nDifference the time series, plot as points and lines and save the differenced series.\n\n\nall_ts$differenced &lt;- c(diff(all_ts$x_t), NA)\ntsplot(all_ts$differenced, type = \"b\", ylab = \"TS 1\", main = \"Differenced Series\", pch = 16 )\n\n\n\n\n\n\n\n\n\n\n(extra content)ggplot version\nggplot(all_ts, aes(x = Time, y = differenced)) + geom_point() + \n  ylab(\"TS 1\") + ggtitle(\"Differenced Series\")\n\n\n\nRun par(mfrow = c(2,3)) and then re-run the plotting code for all 6 plots you just made in steps 1-6.\n\n\npar(mfrow = c(2,3))\n \n# number 1\ntsplot(all_ts$x_t, type = \"b\", ylab = \"TS 1\", main = \"Observed Time Series\", pch = 16)\n  \n## number 2\ntsplot(all_ts$x_t, type = \"b\", ylab = \"TS 1\", main = \"MA(5) Smoother\", pch = 16 )\nlines(all_ts$ma_x_t, col = \"magenta\", lwd = 2)\n# number 3\ntsplot(all_ts$detrended_ma, type = \"b\", ylab = \"TS 1\", main = \"Detrended - MA(5)\", pch = 16 )\n# Number 4\ntsplot(all_ts$x_t, type = \"b\", ylab = \"TS 1\", main = \"Linear Trend\", pch = 16)\nlines(all_ts$reg_x_t, col = \"turquoise\", lwd = 2)\n  \n# number 5\ntsplot(all_ts$detrended_reg, type = \"b\", ylab = \"TS 1\", main = \"Detrended - Linear Trend\", pch = 16)\n\n# Number 6\ntsplot(all_ts$differenced, type = \"b\", ylab = \"TS 1\", main = \"Differenced\", pch = 16 )\n\n\n\n\n\n\n\n\n\n\nShort answer answers\n\nWhich plots look like stationary time series?\n\nIn terms of the “data” (black points/lines in upper left and middle and lower left), the plot looks fairly stationary– no increasing or cyclical trend, and looks fairly similar to white noise (which is stationary). The detrended (MA and Linear) and the differenced plots look very similar, so they seem stationary as well.\nIn terms of the time series generated from creating trend estimates from each time point, we have two: the MA(5) and the linear trend. The linear trend is very slightly negative, but essentially horizontal– stationary.\nThe MA(5) smoother has fluctuations but those don’t seem to be explicitly based directly on time (or for particular time lags, i.e. not seasonal), which would suggest stationarity (though perhaps some temporal structure– we can check the acf.)\n\n\n\n\n\n\nRegression significance tests that suggest white noise.\n\n\n\n\n\nRunning summary(lm_x_t) contains the test statistics and p-values for some hypothesis tests for the regression parameters.\n\nHere, we have a p-value of 0.369 for the test for a zero slope, meaning we do not have evidence the slope of the trend is not zero.\nThe default test of significance for the y-intercept is actually testing whether it’s 0, and that we also fail to reject that the y-intercept is different from 0 (p-value 0.285.\n\nIn other words, we fail to find evidence that the mean is not constant and 0, which is true for white noise. So it kinda seems like white noise.\n\n\n\n\nCan you guess the model/source of \\(x_t\\) from these plots?\n\nIt looks like white noise, but the acf can give us more insight into that in Activity 4.\n\nHow many trends have we estimated?\n\nWe have estimated two trends. Note that differencing a series does not give us an estimate of the trend.",
    "crumbs": [
      "Week 4",
      "Lecture 7 Solutions"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture7.html#activity-3",
    "href": "LectureNotes/Lecture7.html#activity-3",
    "title": "Lecture 7 Solutions",
    "section": "Activity 3",
    "text": "Activity 3\nUse the function process_ts to create the plots for the remaining series.\nNote that we estimate the same two trend models (MA(5) and linear) for each series (though the particular parameters will be different for each of the raw series).\n\nsource(\"Code/process_ts.r\")\n\n\nprocess_ts(time_series$y1, ptitle = \"TS1\")\n\n\n\n\n\n\n\n\n\nWhich plots look like stationary time series?\n\nsee above\n\nCan you guess the model/source of \\(x_t\\) from these plots?\n\nsee above\n\nprocess_ts(time_series$y2, ptitle = \"TS2\")\n\n\n\n\n\n\n\n\n\nWhich plots look like stationary time series?\nThe detrended MA and differenced series look very much like white noise, which implies stationary.\nThe detrended linear trend is almost identical to the raw data (which makes sense since the linear regression estimates essentially a constant mean of 0).\nThe raw data does not appear to have an overall increasing trend, nor any predictable seasonal patterns (not a linear trend stationary, unless I was being mean). However, it does not really look like white noise.\n\nSo that means y2 may be a moving average or perhaps real data.\n\n\n\n\n\n\nHow I could have been mean\n\n\n\n\n\nTechnically, a (deterministic) linear trend stationary model with the usual regression residuals with mean and y-intercept 0 is equivalent to white noise.\n\n\n\n\nCan you guess the model/source of \\(x_t\\) from these plots?\n\nNot exactly! The Acf will reveal more about the temporal structure.\n\nprocess_ts(time_series$y3, ptitle = \"TS3\")\n\n\n\n\n\n\n\n\n\nWhich plots look like stationary time series?\nCan you guess the model/source of \\(x_t\\) from these plots?\n\n(Answering both) The plots for y3 are somewhat similar to those for y2, so this may be the duplicated model (not white noise, though). However, there are some differences. The linear trend is more pronounced, but still appears somewhat shallow. Looking at the raw series, the appearance of temporal structure is even more pronounced. So, perhaps this the deterministic linear model with a very shallow trend (technically still nonstationary), or it’s a moving average, or real data.\n\nprocess_ts(time_series$y4, ptitle = \"TS4\")\n\n\n\n\n\n\n\n\n\nWhich plots look like stationary time series?\n\nThe detrended and differenced plots look stationary. The observed time series looks like an obvious deterministic linear trend, which the linear trend and moving average trend pick up on (though the moving average is a “bumpy” estimate of the linear trend– but there does not appear to be any cyclical structure.)\n\nCan you guess the model/source of \\(x_t\\) from these plots?\n\nThis could technically be real data, if that real data followed a perfect linear trend in time. It’s definitely not white noise or a pure moving average model (generated from white noise).\n\nprocess_ts(time_series$y5, ptitle = \"TS5\")\n\n\n\n\n\n\n\n\n\nWhich plots look like stationary time series?\n\nThe MA-detrended and differenced series look stationary. The raw data looks like it is at one level for the beginning of the series and then drops down for the remainder. That pattern is more pronounced in the moving average overaly plot. The linear trend is decreasing, but does not appear to capture the pattern of the data very well (the decrease in the data is abrupt, not linear and gradual).\n\nCan you guess the model/source of \\(x_t\\) from these plots?\n\nNone of the models (moving average, linear trend, white noise) can have an abrupt change in level like we see in this time series. By process of elimination, this appears to be the real data.",
    "crumbs": [
      "Week 4",
      "Lecture 7 Solutions"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture7.html#activity-4",
    "href": "LectureNotes/Lecture7.html#activity-4",
    "title": "Lecture 7 Solutions",
    "section": "Activity 4",
    "text": "Activity 4\n\nlibrary(patchwork)\nplot_acfs &lt;- function(dataset, main){\n  \n  all_ts &lt;- process_ts(dataset, ptitle = \"\", output = T, plots = F)\n  \n  p &lt;-  forecast::ggAcf(all_ts$x_t)  + ylim(-1,1) + ggtitle(names(all_ts)[1]) +  plot_layout(nrow = 2, ncol = 3)\nfor(i in names(all_ts)[!(names(all_ts)%in% c(\"Time\", \"x_t\"))]){\n\n  g &lt;- forecast::ggAcf(all_ts[, i])  + ylim(-1,1) + ggtitle(i) \n  p &lt;- p + g+ plot_layout(nrow = 2, ncol = 3) + plot_annotation(title = paste(main))\n}\n\np\n}\n\n\nplot_acfs(time_series$y1, main = \"TS 1\")\n\nRegistered S3 method overwritten by 'quantmod':\n  method            from\n  as.zoo.data.frame zoo \n\n\n\n\n\n\n\n\nplot_acfs(time_series$y2, main = \"TS 2\")\n\n\n\n\n\n\n\nplot_acfs(time_series$y3, main = \"TS 3\")\n\n\n\n\n\n\n\nplot_acfs(time_series$y4, main = \"TS 4\")\n\n\n\n\n\n\n\nplot_acfs(time_series$y5, main = \"TS 5\")\n\n\n\n\n\n\n\n\n\nACF of the y’s\n\ny1- looks like white noise\ny2- clearly has lag 1 autocorrelation, maybe a but at lag 2, then drops off. A bit of a pattern in later lags\ny3- has autocorrelation for lags 1-4, then drops off around 5. A bit of a pattern in later lags like y2, but more pronounced than y2.\ny4- This is the one we think is the linear trend model. Clearly there is nonstationarity in the acf of the data, which we’d expect to see from a linear trend.\ny5- This looks unlike any of the others. There appears to be moderate lag 1 autocorrelation which drops off slowly.\n\n\n\nACF of the MA(5) estimates\n\ny1- Since it appears we started with white noise, it makes sense that we can clearly identify a moving average structure in the time series that we made by applying a moving average. We are seeing the structure we built into our MA(5) trend estimate.\ny2- Looks very similar to y1.\ny3- Similar to the first two, but has larger autocorrelation for lags 1-4.\ny4- Looks the same as the acf of y4, and drops off very slowly. Our MA estimate essentially is a linear trend, so it’s not surprising to see this structure in the acf.\ny5- Again, this looks like none of the others. We see the larger autocorrelation at small lags induced by the MA smoothing, but we also see more persistent long-range lag relations.\n\n\n\nACF of the MA-5 detrended\nNone of the MA(5)- detrended series look particularly like white noise, which is what we would want to see if we had fully accounted for the temporal structure in our model.\n\ny1- has some temporal structure, likely due to the fact that we appear to have started with white noise and then subtracted out the MA structure\ny2- has even more structure than y1\ny3- looks like white noise, except for a peak at y5\ny4- looks almost like white noise, except for lag 1 and 2 showing some modest negative autocorrelation\ny5- looks kind of like the white noise, but perhaps has a bit of structure\n\n\n\nACF of the linear trends\nThese all look very similar. That is because each of our linear trend models are deterministic functions of \\(t\\)– they will show this autocorrelation regardless of the steepness of the slope (unless it’s literally 0).\n\n\nACF of the linearly detrended\n\ny1- Looks like white noise– unsurprising since we subtracted off a near horizontal trend of 0, and we suspect a linear model\ny2- has significant autocorrelation at lag 1– looks like the regression does not have independent errors, so probably not a great model\ny3- same as y2, but has a larger autocorrelation and for lags 2 and 3 as well\ny4- Looks like white noise– unsurprising since we linearly detrended and we suspect a linear model.\ny5- Not quite like white noise (most autocorrelations are small but positive), but doesn’t look too bad.\n\n\n\nACF of the differences\n\ny1- Significant lag 1 autocorrelation (makes sense since we differenced white noise, which added some dependence)\ny2- Significant lag 3 autocorrelation\ny3- Significant lag 5 autocorrelation\ny4- some lag 1 autocorrelation\ny5- some lag 1 autocorrelation",
    "crumbs": [
      "Week 4",
      "Lecture 7 Solutions"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture7.html#simulation-solutions.",
    "href": "LectureNotes/Lecture7.html#simulation-solutions.",
    "title": "Lecture 7 Solutions",
    "section": "Simulation solutions.",
    "text": "Simulation solutions.\n\nset.seed(0016)\nN &lt;- 100\n## Simulate white noise.\ny1 &lt;- rnorm(N)\n\n## Simulate an MA(3) \nw &lt;- rnorm(N+50)\ny2 &lt;- stats::filter(w, filter = rep(1/3,3), \n                    method = \"convolution\", sides = 2)[2:101]\n## Simulate an MA(5)\nw &lt;- rnorm(N+50)\ny3 &lt;- stats::filter(w, filter = rep(1/5,5), \n                     method = \"convolution\", sides = 2)[10:109]\n\n## Simulate trend stationary\nw &lt;- rnorm(N, 0, 20)\ntime_index &lt;- 1:N\ny4 &lt;- 2.718 + 1.96*time_index + w\n\n## real data \ndata(Nile, package = \"datasets\")\ny5 &lt;- as.vector(Nile)",
    "crumbs": [
      "Week 4",
      "Lecture 7 Solutions"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture7.html#why-doesnt-the-acf-of-the-ma5-look-like-white-noise",
    "href": "LectureNotes/Lecture7.html#why-doesnt-the-acf-of-the-ma5-look-like-white-noise",
    "title": "Lecture 7 Solutions",
    "section": "Why doesn’t the acf of the MA(5) look like white noise?",
    "text": "Why doesn’t the acf of the MA(5) look like white noise?\n\nI’m not exactly sure, but something about the estimation procedure in the stats::filter() function\nIf you fit using the arima function, it looks fine:\n\n\nma_5_mod &lt;- arima(time_series$y3, c(0,0,5))\nacf(ma_5_mod$resid)\n\n\n\n\n\n\n\n\n\nEstimation procedure matters!",
    "crumbs": [
      "Week 4",
      "Lecture 7 Solutions"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture7.html#a-bit-about-the-nile-data",
    "href": "LectureNotes/Lecture7.html#a-bit-about-the-nile-data",
    "title": "Lecture 7 Solutions",
    "section": "A bit about the Nile Data",
    "text": "A bit about the Nile Data\n\ntsplot(Nile, xlab = \"Year\", ylab = \"Discharge at Aswan, 10^8 m^3\", main = \"Annual Volume\")",
    "crumbs": [
      "Week 4",
      "Lecture 7 Solutions"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture7.html#wheres-aswan",
    "href": "LectureNotes/Lecture7.html#wheres-aswan",
    "title": "Lecture 7 Solutions",
    "section": "Where’s Aswan?",
    "text": "Where’s Aswan?\nSun directly overhead at noon on the summer solstice– where the circumference of the earth was calculated like 2000 years ago. So cool!!! (not related to this data set(?))",
    "crumbs": [
      "Week 4",
      "Lecture 7 Solutions"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture7.html#a-bit-about-the-nile-data-1",
    "href": "LectureNotes/Lecture7.html#a-bit-about-the-nile-data-1",
    "title": "Lecture 7 Solutions",
    "section": "A bit about the Nile Data",
    "text": "A bit about the Nile Data\n“Level shift visible in the data at 1898” – how do we model “abrupt changes in distribution”?",
    "crumbs": [
      "Week 4",
      "Lecture 7 Solutions"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture7.html#check-out-the-paper",
    "href": "LectureNotes/Lecture7.html#check-out-the-paper",
    "title": "Lecture 7 Solutions",
    "section": "Check out the paper!",
    "text": "Check out the paper!",
    "crumbs": [
      "Week 4",
      "Lecture 7 Solutions"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture7.html#whos-barbara-bell",
    "href": "LectureNotes/Lecture7.html#whos-barbara-bell",
    "title": "Lecture 7 Solutions",
    "section": "Who’s Barbara Bell?",
    "text": "Who’s Barbara Bell?\nShe’s not cited formally… (data sets now get their own DOIs, ideally)\n\nGot her PhD at Harvard in 1944 in Astrophysics\n\nHistory of women at Harvard/Radcliffe (a bit of a bummer to read)\n\n\nFrom (one of) her Obituary(ies):\n“Working with Donald Menzel, Harold Glazer, Giancarlo Noci and Robert Kurucz, her research dealt primarily with observations of solar phenomena. Barbara had a keen interest in history and travel, and in her later years traveled to all corners of the globe. She developed a particular interest in the ancient history of Egypt. Her research expanded to include the variations in the floods of the Nile for which she sought to connect to solar fluctuations. Using ancient accounts, she was first to propose a linkage between prolonged drought and the societal collapse she called ”The First Dark Age in Egypt” in the early 1970s. She published several journal articles on the connection of climate fluctuations to the fall of Egyptian dynasties which have been widely cited in the literature. Loved by all who knew her, Barbara had a sharp and curious mind, a kind heart and a cheerful disposition. Intensely interested in her family and those around her, she donated generously her entire life to Harvard University and numerous charities. A beloved sister and aunt…”",
    "crumbs": [
      "Week 4",
      "Lecture 7 Solutions"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html",
    "href": "LectureNotes/Lecture5.html",
    "title": "Lecture 5",
    "section": "",
    "text": "Slides with Activity Solutions\n\n\n\n\n\nView slides in full screen",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#link-to-slides-w-solutions",
    "href": "LectureNotes/Lecture5.html#link-to-slides-w-solutions",
    "title": "Lecture 5",
    "section": "",
    "text": "Slides with Activity Solutions\n\n\n\n\n\nView slides in full screen",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#link-to-template",
    "href": "LectureNotes/Lecture5.html#link-to-template",
    "title": "Lecture 5",
    "section": "Link to template",
    "text": "Link to template\n\n\\[\n\\newcommand\\E{{\\mathbb{E}}}\n\\]\n\n\n\n\n\n\n\nLink to Notes Template\n\n\n\n\n\nLecture5Template.qmd",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#last-time",
    "href": "LectureNotes/Lecture5.html#last-time",
    "title": "Lecture 5",
    "section": "Last time",
    "text": "Last time\n\nTrend Stationarity model\nSalmon price example\nSimulating a time series to understand the autocovariance function",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#today",
    "href": "LectureNotes/Lecture5.html#today",
    "title": "Lecture 5",
    "section": "Today",
    "text": "Today\n\nTime series at ENVR Conference\nDetrending\nActivities\nDifferencing",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#a-look-ahead",
    "href": "LectureNotes/Lecture5.html#a-look-ahead",
    "title": "Lecture 5",
    "section": "A look ahead",
    "text": "A look ahead\n\nWeeks 1-2: terminology of time series models, working with various R functions relating to time series (Chapters 1-2)\nWeek 3: Trends, trends, trends (smoothing) (Chapters 2-3 in Shumway and Stoffer, Ch 8 in FPP)\nWeek 4: Time series regression (trends that depend on predictor variables), Forecasting (Chapter 5 and 7 in FPP)\nWeek 5: Time series data science process, and midterm (Chapter 5 in FPP, various parts in Shumway and Stoffer)\nWeek 6: Partial correlation and ARMA models (Chapter 4 in Shumway and Stoffer)\nWeek 7: ARIMA models (what they are, when to use them, and how to know if yours is trash) (Chapter 4 in Shumway and Stoffer, Chapter 9 in FPP)\nWeek 8: Cross-correlation and Multiple time series (Ch 2, Ch 3 in Shumway and Stoffer)\nWeek 9: Wiggle room/class choice\nWeek 10: Wiggle room/class choice",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#is-a-trend-necessarily-linear",
    "href": "LectureNotes/Lecture5.html#is-a-trend-necessarily-linear",
    "title": "Lecture 5",
    "section": "Is a “trend” necessarily linear?",
    "text": "Is a “trend” necessarily linear?\n\n\nOh no, I have to talk to the intimidating experts\nThe very first talk: “A trend doesn’t have to be linear”- Robert Lund\nThe penultimate talk: “seasonal trend”\n\n\nConclusion:\n\n\nNope! Seems like “trend” = “mean function”",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#lots-of-examples-of-vocab-weve-learned",
    "href": "LectureNotes/Lecture5.html#lots-of-examples-of-vocab-weve-learned",
    "title": "Lecture 5",
    "section": "Lots of examples of vocab we’ve learned…",
    "text": "Lots of examples of vocab we’ve learned…",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#notes-on-robert-lunds-talk",
    "href": "LectureNotes/Lecture5.html#notes-on-robert-lunds-talk",
    "title": "Lecture 5",
    "section": "Notes on Robert Lund’s Talk",
    "text": "Notes on Robert Lund’s Talk",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#notes-on-robert-lunds-talk-1",
    "href": "LectureNotes/Lecture5.html#notes-on-robert-lunds-talk-1",
    "title": "Lecture 5",
    "section": "Notes on Robert Lund’s Talk",
    "text": "Notes on Robert Lund’s Talk",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#notes-from-matthias-katzfusss-talk",
    "href": "LectureNotes/Lecture5.html#notes-from-matthias-katzfusss-talk",
    "title": "Lecture 5",
    "section": "Notes from Matthias Katzfuss’s talk",
    "text": "Notes from Matthias Katzfuss’s talk",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#notes-from-dan-cooleys-talk",
    "href": "LectureNotes/Lecture5.html#notes-from-dan-cooleys-talk",
    "title": "Lecture 5",
    "section": "Notes from Dan Cooley’s talk",
    "text": "Notes from Dan Cooley’s talk\n\n\n\nMath anxiety rating: 70, 🫥👹🧎‍♀️‍➡️",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#notes-from-dan-cooleys-talk-1",
    "href": "LectureNotes/Lecture5.html#notes-from-dan-cooleys-talk-1",
    "title": "Lecture 5",
    "section": "Notes from Dan Cooley’s talk",
    "text": "Notes from Dan Cooley’s talk",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#detrending-1",
    "href": "LectureNotes/Lecture5.html#detrending-1",
    "title": "Lecture 5",
    "section": "Detrending",
    "text": "Detrending\nIf a process is trend stationary (nonstationary in the mean, but stationary in the variance), can we just subtract off the trend and get back a stationary time series?\nSometimes (assuming we are able to estimate it), and that’s called detrending.",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#goal",
    "href": "LectureNotes/Lecture5.html#goal",
    "title": "Lecture 5",
    "section": "Goal:",
    "text": "Goal:\nAssuming trend stationarity (\\(x_t = \\mu_t + y_t\\), where \\(y_t\\) is stationary), find an estimate \\(\\widehat{\\mu}_t\\) and compute\n\\[\n\\begin{align}\n\\widehat{y_t} &= x_t - \\widehat{\\mu_t}\\\\\n\\text{Estimated Stationary process} &= \\text{Data - trend estimate}\n\\end{align}\n\\] Note: Does \\(y_t\\) remind you of anything from regression?",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#example-subtracting-off-the-trend",
    "href": "LectureNotes/Lecture5.html#example-subtracting-off-the-trend",
    "title": "Lecture 5",
    "section": "Example: Subtracting off the trend",
    "text": "Example: Subtracting off the trend\n\n\n\nDark yellow line: the trend estimate\nblack: The observed data\nshaded region: 95% confidence bands on trend estimates.\n\nCan we make the time series stationary by subtracting off the trend?",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#example-subtracting-off-the-trend-1",
    "href": "LectureNotes/Lecture5.html#example-subtracting-off-the-trend-1",
    "title": "Lecture 5",
    "section": "Example: Subtracting off the trend",
    "text": "Example: Subtracting off the trend\nDoes this time series appear stationary? In the mean, yes.",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#example-subtracting-off-the-trend-2",
    "href": "LectureNotes/Lecture5.html#example-subtracting-off-the-trend-2",
    "title": "Lecture 5",
    "section": "Example: Subtracting off the trend",
    "text": "Example: Subtracting off the trend\nHave we captured the temporal structure in the time series? Yes (note: we will learn about ACF hypothesis tests/p-values during the “time series data analysis process”)",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#aside-managing-a-time-series-project-code-base",
    "href": "LectureNotes/Lecture5.html#aside-managing-a-time-series-project-code-base",
    "title": "Lecture 5",
    "section": "Aside: managing a time series project code base",
    "text": "Aside: managing a time series project code base\n\nI manage the GitHub for Houston Wastewater Epidemiology\nCheck out the “issues”\nSection 1: Demo adding new issue (“basic” time series methods)\nSection 2: Demo adding new issue (link to definition of online estimation)",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#activity-1-detrending-a-commodity-example-3.7",
    "href": "LectureNotes/Lecture5.html#activity-1-detrending-a-commodity-example-3.7",
    "title": "Lecture 5",
    "section": "Activity 1: Detrending a commodity (Example 3.7)",
    "text": "Activity 1: Detrending a commodity (Example 3.7)\n\nGiven the code to generate the plot with the trend line, how would you view the equation of the trend line?\nVisualize the de-trended series. Does it appear stationary?\nCompute the acf of the salmon series and the detrended series. What do you notice?",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#activity-1-solutions",
    "href": "LectureNotes/Lecture5.html#activity-1-solutions",
    "title": "Lecture 5",
    "section": "Activity 1 Solutions",
    "text": "Activity 1 Solutions\nUse Lecture5Template.qmd",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#activity-2-assuming-we-are-able-to-estimate-it",
    "href": "LectureNotes/Lecture5.html#activity-2-assuming-we-are-able-to-estimate-it",
    "title": "Lecture 5",
    "section": "Activity 2: “assuming we are able to estimate it”",
    "text": "Activity 2: “assuming we are able to estimate it”\n\nLook at pages 37-41 of the textbook\nwhat is “it” in this context? (what are we estimating?)\nIf this is review, where did you first see these ideas?\nPut a dot on the math anxiety rating distribution on the back board",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#activity-2solutions-assuming-we-are-able-to-estimate-it",
    "href": "LectureNotes/Lecture5.html#activity-2solutions-assuming-we-are-able-to-estimate-it",
    "title": "Lecture 5",
    "section": "Activity 2Solutions: “assuming we are able to estimate it”",
    "text": "Activity 2Solutions: “assuming we are able to estimate it”\nUse Lecture5Template.qmd",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#motivationmodel",
    "href": "LectureNotes/Lecture5.html#motivationmodel",
    "title": "Lecture 5",
    "section": "Motivation/model",
    "text": "Motivation/model\nConsider the trend stationary model (\\(y_t\\) is stationary). \\[\nx_t = \\mu_t + y_t\n\\] We saw how to estimate a fixed trend using a linear regression for the mean (\\(\\mu_t = \\beta_0 + \\beta_1t\\))\nWe then subtract off the estimate of the trend (detrend), \\(\\widehat{\\mu_t}\\) so that we are working with a stationary time series:\n\\[\n\\widehat{y_t} = x_t - \\widehat{\\mu_t}\n\\]\nWhat if the trend was not fixed? (dependent on \\(t\\) beyond just “\\(t\\) as a constant”)",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#a-stochastic-trend-model",
    "href": "LectureNotes/Lecture5.html#a-stochastic-trend-model",
    "title": "Lecture 5",
    "section": "A stochastic trend model",
    "text": "A stochastic trend model\nChange the model for the mean to incorporate a stochastic component (random walk with drift):\n\\[\n\\mu_t = \\delta + \\mu_{t-1} + w_t\n\\] Where \\(w_t\\) is white noise independent of \\(y_t\\).\nIs \\(\\mu_t\\) stationary? No (it’s a random walk, nonstationary in both mean and covariance)",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#how-to-get-back-to-a-stationary-time-series",
    "href": "LectureNotes/Lecture5.html#how-to-get-back-to-a-stationary-time-series",
    "title": "Lecture 5",
    "section": "How to “get back” to a stationary time series?",
    "text": "How to “get back” to a stationary time series?\nSince the stochastic component depends on just one past time point, consider the series \\(x_t - x_{t-1}\\).\nThis series is called the differenced series and the process is called differencing.",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#computing-the-difference-series-in-terms-of-the-stochastic-trend-model",
    "href": "LectureNotes/Lecture5.html#computing-the-difference-series-in-terms-of-the-stochastic-trend-model",
    "title": "Lecture 5",
    "section": "Computing the difference series in terms of the (stochastic) trend model",
    "text": "Computing the difference series in terms of the (stochastic) trend model\n\\[\n\\begin{align}\nx_{t} - x_{t-1} &= (\\mu_t + y_t) - (\\mu_{t-1} - y_{t-1})\\\\\n&= (\\delta + \\mu_{t-1} + w_t + y_t) - (\\mu_{t-1} - y_{t-1})\\\\\n& = \\delta + w_t + y_t - y_{t-1}\n\\end{align}\n\\] Need to compute mean function \\(\\E(x_t - x_{t-1}\\) and autocovariance function \\(cov(x_t - x_{t-1}, x_s- x_{s-1})\\) and check if they do not depend on \\(t\\) (mean) and just depend on the lag \\(h = s-t\\).\n…But the answer is we do get a stationary series!",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#activity-3-simulating-a-random-walk-and-then-differencing-it",
    "href": "LectureNotes/Lecture5.html#activity-3-simulating-a-random-walk-and-then-differencing-it",
    "title": "Lecture 5",
    "section": "Activity 3: Simulating a random walk and then differencing it",
    "text": "Activity 3: Simulating a random walk and then differencing it\n\nSimulate a random walk with no drift and plot it.\n\n\n\nCode\n## Simulate random walk w/ drift\n## your code here \n\n\n\nUse the diff function to difference the simulated series. Plot the result.\n\n\n\nCode\n# your code here\n\n\n\nDoes this series appear stationary? How do you know?\nVisualize the ACF of the differenced series. Does it look like white noise?\n\n\n\nCode\n# your code here",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#activity-4-differencing-salmon-prices",
    "href": "LectureNotes/Lecture5.html#activity-4-differencing-salmon-prices",
    "title": "Lecture 5",
    "section": "Activity 4: Differencing Salmon Prices",
    "text": "Activity 4: Differencing Salmon Prices\n\nCompute and plot the differenced salmon series.\n\n\n\nCode\n# your code here\n\n\n\nDoes the series appear stationary?\nVisualize the acf of the differenced series. Does it look like white noise?",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#activity-5-comparing-differencing-and-detrending",
    "href": "LectureNotes/Lecture5.html#activity-5-comparing-differencing-and-detrending",
    "title": "Lecture 5",
    "section": "Activity 5: Comparing Differencing and Detrending",
    "text": "Activity 5: Comparing Differencing and Detrending\nCompare the Acfs of the differenced and detrended salmon series. What do you notice?\n\n\nCode\n# your code here",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#next-time-smoothing",
    "href": "LectureNotes/Lecture5.html#next-time-smoothing",
    "title": "Lecture 5",
    "section": "Next time: Smoothing",
    "text": "Next time: Smoothing\nWe’ve seen three explicity ways of modeling a trend (moving average (hw 1), and regression with time and random walk).\nHow else could we model a trend?",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture5.html#visual-example",
    "href": "LectureNotes/Lecture5.html#visual-example",
    "title": "Lecture 5",
    "section": "Visual example:",
    "text": "Visual example:\n\n\nCode\ntsplot(soi, col=4)\nlines(ksmooth(time(soi), soi, \"normal\", bandwidth=1), lwd=2, col=6)\npar(fig = c(.65, 1, .75, 1), new = TRUE) # the insert\ncurve(dnorm, -3, 3,  xaxt='n', yaxt='n', ann=FALSE)",
    "crumbs": [
      "Week 3",
      "Lecture 5"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html",
    "href": "LectureNotes/Lecture3.html",
    "title": "Lecture 3",
    "section": "",
    "text": "Slides with Activity Solutions\n\n\n\n\n\nView slides in full screen",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#recap",
    "href": "LectureNotes/Lecture3.html#recap",
    "title": "Lecture 3",
    "section": "Recap",
    "text": "Recap\n\n\\[\n\\newcommand\\E{{\\mathbb{E}}}\n\\]\n\n\nVisualizing time series\nResearch questions involving time series\nMean and covariance functions\nMoving average examples\nAlmost got to stationarity",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#today",
    "href": "LectureNotes/Lecture3.html#today",
    "title": "Lecture 3",
    "section": "Today",
    "text": "Today\n\nDecomposing a time series\nStationarity\nAutocorrelation function\nTime series regression",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#first-participation-grade",
    "href": "LectureNotes/Lecture3.html#first-participation-grade",
    "title": "Lecture 3",
    "section": "First “participation” grade",
    "text": "First “participation” grade\n\nconfirm you are good to opt in or out of the textbook, you have to do it by Oct 2 so do it on Oct 1 (tomorrow).",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#lecture-template",
    "href": "LectureNotes/Lecture3.html#lecture-template",
    "title": "Lecture 3",
    "section": "Lecture Template",
    "text": "Lecture Template\n\nDownload “Lecture3Template.qmd” from Canvas\nhas some basic document structure set up to make it easier to follow along in lecture :)",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#another-time-series-model",
    "href": "LectureNotes/Lecture3.html#another-time-series-model",
    "title": "Lecture 3",
    "section": "Another time series model",
    "text": "Another time series model\nSimilar to the signal plus noise model,\n\\[\nX_t = T_t + S_t + W_t\n\\]\n\n\\(T_t\\) is the trend component\n\\(S_t\\) is the seasonal component\n\\(W_t\\) is the error component\n\nThe r function stats::decompose will split a time series \\(X_t\\) into these three components.",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#activity-1",
    "href": "LectureNotes/Lecture3.html#activity-1",
    "title": "Lecture 3",
    "section": "Activity 1",
    "text": "Activity 1\n\n\nCode\nlibrary(astsa)\n\n## use the decompose function on the jj series\njj_decomp &lt;- ## your code here\n  \n## plot the decomposition\n## your code here\n\n\n\nUse the decompose function on the jj series.\nMatch the terms in the equation on the previous slide to each of the components in the chart\nDescribe the trend.\nDoes the bottom plot (“error”) look like white noise?\nLook at the documentation for the decompose function. Can you determine how the “trend” component was computed?",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#activity-1-solution",
    "href": "LectureNotes/Lecture3.html#activity-1-solution",
    "title": "Lecture 3",
    "section": "Activity 1 (solution)",
    "text": "Activity 1 (solution)\nUse Lecture3Template.qmd",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#activity-2",
    "href": "LectureNotes/Lecture3.html#activity-2",
    "title": "Lecture 3",
    "section": "Activity 2",
    "text": "Activity 2\nRecall the (sinusoidal) signal plus noise model: \\[\nw_t \\sim \\text{iid } N(0, \\sigma^2_w)\\\\\nx_t = 2\\cos\\left (\\frac{2\\pi t}{50} - .6\\right) + w_t\n\\]\n\nSimulate 500 observations from the signal plus noise model\nApply the decompose function. Does the error portion look like white noise?\n\nHint: The below code gives an error. Compare the “frequency” of the jj series. Can you figure out how to use the ts function to specify the correct frequency?\n\n\nCode\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\nx_t = cs + w\n\nplot(decompose(x_t))",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#activity-2-solution",
    "href": "LectureNotes/Lecture3.html#activity-2-solution",
    "title": "Lecture 3",
    "section": "Activity 2 (solution)",
    "text": "Activity 2 (solution)\nUse Lecture3Template.qmd",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#comparing-math-perspective-to-data-perspective",
    "href": "LectureNotes/Lecture3.html#comparing-math-perspective-to-data-perspective",
    "title": "Lecture 3",
    "section": "Comparing “math perspective” to “data perspective”",
    "text": "Comparing “math perspective” to “data perspective”\n\n\n\\[\nw_t \\sim N(0, \\sigma^2_w), t = 1, \\dots, n\\\\\nx_t = 2\\cos\\left (\\frac{2\\pi t}{50} - .6\\right) + w_t\n\\]\n\n\nCode\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\ntsplot(cs + w)\nlines(cs, col = \"blueviolet\", type = \"l\", lwd = 4)\n\n\n\n\n\n\n\n\n\n\n\n\nCode\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\nx_t = ts(cs + w, frequency = 50)\n\nplot(decompose(x_t))\n\n\n\n\n\n\n\n\n\nDoes this function give us an estimate of the form of the mean function?",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#review-autocovariance-function",
    "href": "LectureNotes/Lecture3.html#review-autocovariance-function",
    "title": "Lecture 3",
    "section": "Review: autocovariance function",
    "text": "Review: autocovariance function\n\n\nCode\nlibrary(ggplot2)\n\nt &lt;- seq(1, 16, 1)\nx_t &lt;- 0.5*t \n#x &lt;- x - mean(x)\n#y &lt;- y - mean(y)\n\ndf &lt;- data.frame(t, x_t)\n\n# For every row in `df`, compute a rotated normal density centered at `y` and shifted by `x`\ncurves &lt;- lapply(seq_len(NROW(df)), function(i) {\n  mu &lt;- df$x_t[i]\n  range &lt;- mu + c(-1.5, 1.5)\n  seq &lt;- seq(range[1], range[2], length.out = 100)\n  data.frame(\n    t = -1 * dnorm(seq, mean = mu, sd = 0.5) + df$t[i],\n    x_t = seq,\n    grp = i\n  )\n})\n# Combine above densities in one data.frame\ncurves &lt;- do.call(rbind, curves)\n\nnew.x = seq(from = 1, to = 16, by = .1)\nnew.y = .5*new.x\ntrend_line &lt;- data.frame(x = new.x,\n                       y = new.y)\nggplot(df, aes(t, x_t)) +\n  geom_point(col = \"blueviolet\", pch = 17) +\n  #geom_line() +\n  # The path draws the curve\n  geom_path(data = curves, aes(group = grp)) +\n  geom_line(data = trend_line, aes(x=x,y=y), col = \"blueviolet\") +\n  lims(y = c(-2,10)) +\n  scale_x_continuous(breaks = seq(1, 16, by = 1)) +\n  theme_minimal() + \n  theme( # remove the vertical grid lines\n           panel.grid = element_blank() ,\n           # explicitly set the horizontal lines (or they will disappear too)\n           panel.grid.major.x = element_line( size=.1, color=\"black\" )) +   \n  geom_rect(aes(xmin = 3.1, xmax = 4.1, ymin = 0, ymax = 4), fill = NA, col = \"blue\")+   \n  geom_rect(aes(xmin = 7.1, xmax = 8.1, ymin = 2, ymax = 6), fill = NA, col = \"magenta\")\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n\nCode\n  # The polygon does the shading. We can use `oob_squish()` to set a range.\n  #geom_polygon(data = curves, aes(y = scales::oob_squish(y, c(0, Inf)),group = grp))",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#error-covariance-at-different-time-points",
    "href": "LectureNotes/Lecture3.html#error-covariance-at-different-time-points",
    "title": "Lecture 3",
    "section": "Error covariance at different time points",
    "text": "Error covariance at different time points\n\n\nCode\n# install.packages(\"ggplot2\")\n# install.packages(\"ggExtra\")\nlibrary(ggplot2)\nlibrary(ggExtra)\nset.seed(50)\nx1 &lt;- rnorm(100,x_t[4], .5)\nx2 &lt;- rnorm(100,x_t[8], .5)\n\nx &lt;- data.frame(x1, x2)\n# Save the scatter plot in a variable\np &lt;- ggplot(x, aes(x = x1, y = x2)) +\n  geom_point() + xlim(0,6) + ylim(0,6)+ \n  geom_text(aes(x = 4, y = 2, label = \"gamma(2,8) = \\n cov(x_2, x_8) = 0\"), size = 6) + coord_fixed() \n\n# Arguments for each marginal histogram\nggMarginal(p, type = \"density\", adjust = 2,\n           xparams = list(col = \"blue\", fill = \"blue\"),\n           yparams = list(col = \"magenta\", fill = \"magenta\"))\n\n\nWarning in geom_text(aes(x = 4, y = 2, label = \"gamma(2,8) = \\n cov(x_2, x_8) = 0\"), : All aesthetics have length 1, but the data has 100 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 100 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#error-covariance-at-different-time-points-time-dependence",
    "href": "LectureNotes/Lecture3.html#error-covariance-at-different-time-points-time-dependence",
    "title": "Lecture 3",
    "section": "Error Covariance at Different Time Points (time dependence)",
    "text": "Error Covariance at Different Time Points (time dependence)\n\n\nCode\n#install.packages(\"MASS\")\nlibrary(MASS)\nset.seed(100)\nmu &lt;- c(x_t[4], x_t[8])\nvarcov &lt;- matrix(c(.5, .3, .3, .5), \n                 ncol = 2)\nx&lt;- mvrnorm(100, mu = mu, Sigma =varcov)\nx &lt;- data.frame(x1 = x[,1], x2 = x[,2])\n# Save the scatter plot in a variable\np &lt;- ggplot(x, aes(x = x1, y = x2)) +\n  geom_point() + xlim(0,6) + ylim(0,6) + \n  geom_text(aes(x = 4, y = 2, label = \"gamma(2,8) = \\n cov(x_2, x_8) = 0.2\"), size = 6) + coord_fixed() \n\n# Arguments for each marginal histogram\nggMarginal(p, type = \"density\", adjust = 2,\n           xparams = list(col = \"blue\", fill = \"blue\"),\n           yparams = list(col = \"magenta\", fill = \"magenta\"))\n\n\nWarning in geom_text(aes(x = 4, y = 2, label = \"gamma(2,8) = \\n cov(x_2, x_8) = 0.2\"), : All aesthetics have length 1, but the data has 100 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 100 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#stationarity",
    "href": "LectureNotes/Lecture3.html#stationarity",
    "title": "Lecture 3",
    "section": "Stationarity",
    "text": "Stationarity\nA time series is stationary if\n\nthe mean function (\\(\\mu_t\\)) is constant and does not depend on time \\(t\\)\nthe autocovariance function (\\(\\gamma(s,t)\\)) depends on \\(s\\) and \\(t\\) only though their difference\n\nAnd nonstationary otherwise.",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#steps-to-determine-whether-a-time-series-x_t-is-stationary",
    "href": "LectureNotes/Lecture3.html#steps-to-determine-whether-a-time-series-x_t-is-stationary",
    "title": "Lecture 3",
    "section": "Steps to determine whether a time series \\(x_t\\) is stationary:",
    "text": "Steps to determine whether a time series \\(x_t\\) is stationary:\n\nCompute the mean function.\nCompute the autocovariance function.\nIf both do not depend on \\(t\\), then \\(x_t\\) is stationary. Otherwise, \\(x_t\\) is nonstationary.",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#activity-3-example-2.14-stationarity-of-a-random-walk",
    "href": "LectureNotes/Lecture3.html#activity-3-example-2.14-stationarity-of-a-random-walk",
    "title": "Lecture 3",
    "section": "Activity 3: Example 2.14 Stationarity of a Random Walk",
    "text": "Activity 3: Example 2.14 Stationarity of a Random Walk\n\\[\nx_t = x_{t-1} + w_t\n\\]\nLast, time, we saw that the mean function is \\(\\E(x_t) = 0\\), and the autocovariance function is \\(\\gamma_x(s, t) = \\min\\{s,t\\}\\sigma^2_w\\)\n\nIs \\(x_t\\) stationary?\nWhat if there was drift?",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#activity-3-solution",
    "href": "LectureNotes/Lecture3.html#activity-3-solution",
    "title": "Lecture 3",
    "section": "Activity 3 (solution)",
    "text": "Activity 3 (solution)\nUse Lecture3Template.qmd",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#gammast-for-a-random-walk",
    "href": "LectureNotes/Lecture3.html#gammast-for-a-random-walk",
    "title": "Lecture 3",
    "section": "\\(\\gamma(s,t)\\) for a random walk",
    "text": "\\(\\gamma(s,t)\\) for a random walk\n\n\nCode\nsigma_w &lt;- 5 #define variance of the white noise\ncoords &lt;- expand.grid(1:20, 1:20)\nnames(coords) &lt;- c(\"s\", \"t\") ## the coordinates represent different possible time points\ncoords$gamma &lt;- pmin(coords$s, coords$t)*sigma_w\n\n\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:MASS':\n\n    select\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nCode\nplot_ly(coords,\n  x= ~s, y=~t, z=~gamma, \n  type = 'scatter3d', mode = \"markers\", size = .1)",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#is-white-noise-stationary",
    "href": "LectureNotes/Lecture3.html#is-white-noise-stationary",
    "title": "Lecture 3",
    "section": "Is white noise stationary?",
    "text": "Is white noise stationary?\n\nMean function of white noise is \\(\\E(w_t) = 0\\)\nAutocovariance function is \\[\n\\gamma_w(s, t) = cov(w_s, w_t) =  \\begin{cases} \\sigma^2_w & \\text{ if } s = t\\\\ 0 & \\text{ if } s \\ne t \\end{cases}\n\\] Since neither depends on \\(t\\), white noise is stationary.",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#gammast-for-white-noise",
    "href": "LectureNotes/Lecture3.html#gammast-for-white-noise",
    "title": "Lecture 3",
    "section": "\\(\\gamma(s,t)\\) for white noise",
    "text": "\\(\\gamma(s,t)\\) for white noise\n\n\nCode\n# plot the autocov's we computed last time, show on model of time series\n\n\n\n\nCode\n## white noise\nsigma_w &lt;- 5 #define variance of the white noise\ncoords &lt;- expand.grid(1:20, 1:20)\nnames(coords) &lt;- c(\"s\", \"t\") ## the coordinates represent different possible time points\ncoords$gamma &lt;- 0\ncoords$gamma[coords[,1] == coords[,2]] &lt;- sigma_w ## covariance is sigma_w if s = t, 0 otherwise\n\n\nlibrary(plotly)\nplot_ly(coords,\n  x= ~s, y=~t, z=~gamma, \n  type = 'scatter3d', mode = \"markers\", size = .1)",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#break",
    "href": "LectureNotes/Lecture3.html#break",
    "title": "Lecture 3",
    "section": "Break",
    "text": "Break",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#activity-4",
    "href": "LectureNotes/Lecture3.html#activity-4",
    "title": "Lecture 3",
    "section": "Activity 4",
    "text": "Activity 4\nWhich of the following time series are stationary?\n\n\n\nFrom Forecasting Principles and Practice Chapter 9",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#activity-4-solution",
    "href": "LectureNotes/Lecture3.html#activity-4-solution",
    "title": "Lecture 3",
    "section": "Activity 4 (solution)",
    "text": "Activity 4 (solution)\nUse Lecture3Template.qmd",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#why-is-stationarity-important",
    "href": "LectureNotes/Lecture3.html#why-is-stationarity-important",
    "title": "Lecture 3",
    "section": "Why is stationarity important?",
    "text": "Why is stationarity important?\n\nIn order to measure correlation between contiguous time points\nTo avoid spurious correlations in a regression setting\nSimplifies how we can write the autocovariance and autocorrelation functions",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#autocorrelation-function",
    "href": "LectureNotes/Lecture3.html#autocorrelation-function",
    "title": "Lecture 3",
    "section": "Autocorrelation function",
    "text": "Autocorrelation function\nThe autocorrelation function (acf) of a time series is: \\[\n\\rho(s, t) = \\frac{\\gamma(s,t)}{\\sqrt{\\gamma(s,s)\\gamma(t,t)}}\n\\] i.e. the autocovariance divided by the standard deviation of the process at each time point.",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#autocovariance-and-autocorrelation-for-stationary-time-series",
    "href": "LectureNotes/Lecture3.html#autocovariance-and-autocorrelation-for-stationary-time-series",
    "title": "Lecture 3",
    "section": "Autocovariance and Autocorrelation for Stationary Time series",
    "text": "Autocovariance and Autocorrelation for Stationary Time series\nSince for stationary time series the autocovariance depends on \\(s\\) and \\(t\\) only through their difference, we can write the covariance as: \\[\n\\gamma(s,t) = \\gamma(h) = cov(x_{t+h}, x_t) = \\E[(x_{t+h} - \\mu)(x_t-\\mu)]\n\\] and the correlation as: \\[\n\\rho(s,t) = \\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)}\n\\] \\(h = s-t\\) is called the lag.",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#autocorrelation-function-of-a-three-point-moving-average",
    "href": "LectureNotes/Lecture3.html#autocorrelation-function-of-a-three-point-moving-average",
    "title": "Lecture 3",
    "section": "Autocorrelation function of a three-point moving average",
    "text": "Autocorrelation function of a three-point moving average\n\\(\\gamma_v(s, t) = cov(v_s, v_t) =  \\begin{cases}\\frac{3}{9}\\sigma^2_w & \\text{ if } s = t\\\\ \\frac{2}{9}\\sigma^2_w & \\text{ if } \\vert s-t \\vert = 1 \\\\\\frac{1}{9}\\sigma^2_w & \\text{ if } \\vert s-t \\vert =2 \\\\0 & \\text{ if } \\vert s - t\\vert &gt; 2\\end{cases}\\)\nSince \\(v\\) is stationary, we can write\n\\(\\gamma_v(h) = \\begin{cases}\\frac{3}{9}\\sigma^2_w & \\text{ if } h = 0\\\\ \\frac{2}{9}\\sigma^2_w & \\text{ if } h = \\pm1 \\\\\\frac{1}{9}\\sigma^2_w & \\text{ if }h = \\pm 2 \\\\0 & \\text{ if } h&gt; 2\\end{cases}\\)\nAnd the autocorrelation is:\n\\(\\rho(h) = \\begin{cases}1 & \\text{ if } h = 0\\\\ \\frac{2}{3} & \\text{ if } h = \\pm1 \\\\\\frac{1}{3}_w & \\text{ if }h = \\pm 2 \\\\0 & \\text{ if } h&gt; 2\\end{cases}\\)",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#autocorrelation-function-of-a-three-point-moving-average-1",
    "href": "LectureNotes/Lecture3.html#autocorrelation-function-of-a-three-point-moving-average-1",
    "title": "Lecture 3",
    "section": "Autocorrelation function of a three-point moving average",
    "text": "Autocorrelation function of a three-point moving average\nIn R, we can plot \\(\\rho(h)\\)\n\n\nCode\nACF = c(0,0,0,1,2,3,2,1,0,0,0)/3\nLAG = -5:5\ntsplot(LAG, ACF, type=\"h\", lwd=3, xlab=\"LAG\")   \nabline(h=0)\npoints(LAG[-(4:8)], ACF[-(4:8)], pch=20)\naxis(1, at=seq(-5, 5, by=2))",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#activity-5",
    "href": "LectureNotes/Lecture3.html#activity-5",
    "title": "Lecture 3",
    "section": "Activity 5",
    "text": "Activity 5\n\nPredict what the acf will look like for the ar(1) process?\nSimulate an ar(1) process and compute the acf. Were you correct?\nWhat is the lag 0 autocorrelation? Explain why its value makes sense.\n\n\n\nCode\n# simulate from an ar(1)\n\n# use acf() function to plot acf\n\n# save output of acf and inspect",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#activity-5-solution",
    "href": "LectureNotes/Lecture3.html#activity-5-solution",
    "title": "Lecture 3",
    "section": "Activity 5 (solution)",
    "text": "Activity 5 (solution)\nUse Lecture3Template.qmd",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#questions-on-the-quiz",
    "href": "LectureNotes/Lecture3.html#questions-on-the-quiz",
    "title": "Lecture 3",
    "section": "Questions on the quiz?",
    "text": "Questions on the quiz?",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#activity-6-problem-2.3",
    "href": "LectureNotes/Lecture3.html#activity-6-problem-2.3",
    "title": "Lecture 3",
    "section": "Activity 6 (Problem 2.3)",
    "text": "Activity 6 (Problem 2.3)\nWhen smoothing time series data, it is sometimes advantageous to give decreasing amounts of weights to values farther away from the center. Consider the simple two-sided moving average smoother of the form: \\[\nv_t = \\frac{1}{4}(w_{t-1} + w_t + w_{t+1})\n\\] Where \\(w_t\\) are white noise. The autocovariance as a function of \\(h\\) is: \\[\\gamma_v(s, t) = cov(v_s, v_t) =  \\begin{cases}\\frac{6}{16}\\sigma^2_w & \\text{ if } h = 0\\\\ \\frac{4}{16}\\sigma^2_w & \\text{ if } h = \\pm 1 \\\\\\frac{1}{16}\\sigma^2_w & \\text{ if } h = \\pm 2 \\\\0 & \\text{ if } h&gt; 2\\end{cases}\\] 1. Compare to the autocovariance equation for the unweighted 3 point moving average from Lecture 2. Comment on the differences.\n\nWrite down the autocorrelation function.",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#activity-6-solution",
    "href": "LectureNotes/Lecture3.html#activity-6-solution",
    "title": "Lecture 3",
    "section": "Activity 6 (solution)",
    "text": "Activity 6 (solution)\nUse Lecture3Template.qmd ## Activity 7 Recall the decomposition of the Johnson and Johnson quarterly earnings.\n\n\nCode\nplot(decompose(jj)) ## plot decomposition\n\n\n\n\n\n\n\n\n\n\nIs the series stationary?\nDoes the acf of the random component look like white noise?",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#activity-7-solution",
    "href": "LectureNotes/Lecture3.html#activity-7-solution",
    "title": "Lecture 3",
    "section": "Activity 7 (solution)",
    "text": "Activity 7 (solution)\nUse Lecture3Template.qmd",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture3.html#coming-up",
    "href": "LectureNotes/Lecture3.html#coming-up",
    "title": "Lecture 3",
    "section": "Coming up:",
    "text": "Coming up:\n\nAssignment 1 due at midnight\nAssignment 2 posted later\nPart of this will be involve “reading” the textbook! (collecting data on how you feel about the math)\nNext Lecture:\n\nRegression with time\nCross-correlation\nInducing stationarity",
    "crumbs": [
      "Week 2",
      "Lecture 3"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#agenda",
    "href": "LectureNotes/Lecture1.html#agenda",
    "title": "Lecture 1",
    "section": "Agenda",
    "text": "Agenda\n10:10 Introductions\n10:30 Course Rhythm/Roadmap\n10:45 Syllabus\n11:00 R Setup\n11:15 Activity\n11:45 Introduction to Time Series Models\nExtra time: Get started on Exercises",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#introductions",
    "href": "LectureNotes/Lecture1.html#introductions",
    "title": "Lecture 1",
    "section": "Introductions",
    "text": "Introductions\nArrange yourselves into groups of 3-4 and share the following:\n\nName\nMajor\nWhether you spend more time thinking about the past or the future\nWhether you are more certain when you think about the past or the future\n\nPlease be prepared to share summary data with the class!",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#about-me-professional",
    "href": "LectureNotes/Lecture1.html#about-me-professional",
    "title": "Lecture 1",
    "section": "About Me (Professional)",
    "text": "About Me (Professional)\n\nCal Poly SLO B.S. in Statistics and Pure Math\nPhD (and MA) in Statistics from Rice University\n\nExpertise: Spatial/Spatiotemporal Statistics (specifically spatial weight matrices)\nAlso did graduate certificate in teaching and learning\n\n1.5 years authoring online interactive course material for zyBooks/Wiley (EdTech portion Higher Education industry)\n1.5 years managing a team of Statistics authors at zyBooks/Wiley\n1.5 years as Research Scientist (wastewater epidemiology) at Rice University",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#teaching-philosophy",
    "href": "LectureNotes/Lecture1.html#teaching-philosophy",
    "title": "Lecture 1",
    "section": "Teaching Philosophy",
    "text": "Teaching Philosophy\n\nMinimize yapping\nPromote collaboration\nProvide varied opportunities for feedback",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#course-rhythm",
    "href": "LectureNotes/Lecture1.html#course-rhythm",
    "title": "Lecture 1",
    "section": "Course Rhythm",
    "text": "Course Rhythm\n\nAssignments due Monday nights at Midnight (11:59:59 PM)\nNew assignments posted Mondays at 5pm\nQuizzes due Thursday nights at midnight\nOne Midterm on Wednesday October 23, in class\nOne cumulative final exam\n\nSection 1 (10am): Wednesday, December 11 from 10am-1pm\nSection 2 (2pm): Friday, December 13 from 1pm-4pm",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#syllabus",
    "href": "LectureNotes/Lecture1.html#syllabus",
    "title": "Lecture 1",
    "section": "Syllabus",
    "text": "Syllabus\nSyllabus on Canvas (section 1)\nSyllabus on Canvas (section 2)",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#installing-r",
    "href": "LectureNotes/Lecture1.html#installing-r",
    "title": "Lecture 1",
    "section": "Installing R",
    "text": "Installing R\n\nGo to https://cran.r-project.org/\nSelect your operating system\nFollow the download instructions",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#installing-rstudio",
    "href": "LectureNotes/Lecture1.html#installing-rstudio",
    "title": "Lecture 1",
    "section": "Installing RStudio",
    "text": "Installing RStudio\n\nGo to https://posit.co/download/rstudio-desktop/\nStep 2 should have a clickable link with your operating system (auto-detected)\nFollow the download instructions",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#getting-started",
    "href": "LectureNotes/Lecture1.html#getting-started",
    "title": "Lecture 1",
    "section": "Getting Started",
    "text": "Getting Started\n\nIn R Studio, Click File –&gt; New –&gt; Quarto Document\nTitle it Lecture 1 Notes\nDelete the template material\nAdd setup chunk\n\n\nlibrary(astsa)",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#group-time",
    "href": "LectureNotes/Lecture1.html#group-time",
    "title": "Lecture 1",
    "section": "Group time!",
    "text": "Group time!\nSplit into groups of 4, I will come around and assign an example to you\n\nIn your quarto document, create a heading with a title of your example and “Equations” and “Visualizatons”\nRead over the description of the example (access the book through Canvas)\ninstall and load the astsa package.\nCopy the R code from https://github.com/nickpoison/tsda/blob/main/Rcode.md#chapter-1\nRun the R code and verify whether you can reproduce the figure from the text\nWrite down a research question that could be answered using the time series data for your example",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#example-1.1-quarterly-earnings",
    "href": "LectureNotes/Lecture1.html#example-1.1-quarterly-earnings",
    "title": "Lecture 1",
    "section": "Example 1.1 (Quarterly Earnings)",
    "text": "Example 1.1 (Quarterly Earnings)\n\n\nCode\npar(mfrow=2:1)\ntsplot(jj, ylab=\"QEPS\", type=\"o\", col=4, main=\"Johnson & Johnson Quarterly Earnings\")\ntsplot(log(jj), ylab=\"log(QEPS)\", type=\"o\", col=4)",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#example-1.2-climate-change",
    "href": "LectureNotes/Lecture1.html#example-1.2-climate-change",
    "title": "Lecture 1",
    "section": "Example 1.2 (Climate Change)",
    "text": "Example 1.2 (Climate Change)\n\n\nCode\ntsplot(cbind(gtemp_land,gtemp_ocean), spaghetti=TRUE, col = astsa.col(c(2,4), .5), \n        lwd=2, type=\"o\", pch=20, ylab=\"Temperature Deviations\", main=\"Global Warming\")\nlegend(\"topleft\", col=c(2,4), lty=1, lwd=2, pch=20,  bg=\"white\",\n        legend=c(\"Land Surface\", \"Sea Surface\"))",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#example-1.3-dow-jones-industrial-average",
    "href": "LectureNotes/Lecture1.html#example-1.3-dow-jones-industrial-average",
    "title": "Lecture 1",
    "section": "Example 1.3 (Dow Jones Industrial Average)",
    "text": "Example 1.3 (Dow Jones Industrial Average)\n\n\nCode\nlibrary(xts)     # install.packages(\"xts\") if you don't have it already \ndjia_return = diff(log(djia$Close))[-1]\npar(mfrow=2:1)\nplot(djia$Close, col=4)\nplot(djia_return, col=4)\n\n\n\n\n\n\n\n\n\nCode\ntsplot(diff(log(gdp)), type=\"o\", col=4)       # using diff log\npoints(diff(gdp)/lag(gdp,-1), pch=\"+\", col=2) # actual return",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#example-1.4-el-niño",
    "href": "LectureNotes/Lecture1.html#example-1.4-el-niño",
    "title": "Lecture 1",
    "section": "Example 1.4 El Niño",
    "text": "Example 1.4 El Niño\n\n\nCode\npar(mfrow = c(2,1))\ntsplot(soi, ylab=\"\", xlab=\"\", main=\"Southern Oscillation Index\", col=4)\ntext(1970, .91, \"COOL\", col=5)\ntext(1970,-.91, \"WARM\", col=6)\ntsplot(rec, ylab=\"\", main=\"Recruitment\", col=4)",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#example-1.5-predator-prey-interactions",
    "href": "LectureNotes/Lecture1.html#example-1.5-predator-prey-interactions",
    "title": "Lecture 1",
    "section": "Example 1.5 (Predator-Prey Interactions)",
    "text": "Example 1.5 (Predator-Prey Interactions)\nLink to more info!\n\n\nCode\ntsplot(cbind(Hare,Lynx), col = astsa.col(c(2,4), .6), lwd=2, type=\"o\", pch=c(0,2),\n        spaghetti=TRUE, ylab=expression(Number~~~(\"\"%*% 1000)))\nlegend(\"topright\", col=c(2,4), lty=1, lwd=2, pch=c(0,2), legend=c(\"Hare\", \"Lynx\"), bty=\"n\")",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#cute-animal-pictures",
    "href": "LectureNotes/Lecture1.html#cute-animal-pictures",
    "title": "Lecture 1",
    "section": "Cute animal pictures",
    "text": "Cute animal pictures",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#example-1.6-fmri-imaging",
    "href": "LectureNotes/Lecture1.html#example-1.6-fmri-imaging",
    "title": "Lecture 1",
    "section": "Example 1.6 fMRI Imaging",
    "text": "Example 1.6 fMRI Imaging\n\n\nCode\npar(mfrow=c(3,1))\nx = ts(fmri1[,4:9], start=0, freq=32)        # data\nnames = c(\"Cortex\",\"Thalamus\",\"Cerebellum\")\nu = ts(rep(c(rep(.6,16), rep(-.6,16)), 4), start=0, freq=32) # stimulus signal\n\nfor (i in 1:3){ \n j = 2*i-1\n tsplot(x[,j:(j+1)], ylab=\"BOLD\", xlab=\"\", main=names[i], col=5:6, ylim=c(-.6,.6), \n        lwd=2, xaxt=\"n\", spaghetti=TRUE)\n axis(seq(0,256,64), side=1, at=0:4)\n #lines(u, type=\"s\", col=gray(.3)) \n}\nmtext(\"seconds\", side=1, line=1.75, cex=.9)\n\n\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(3,1))\nx = ts(fmri1[,4:9], start=0, freq=32)        # data\nnames = c(\"Cortex\",\"Thalamus\",\"Cerebellum\")\nu = ts(rep(c(rep(.6,16), rep(-.6,16)), 4), start=0, freq=32) # stimulus signal\n\nfor (i in 1:3){ \n j = 2*i-1\n tsplot(x[,j:(j+1)], ylab=\"BOLD\", xlab=\"\", main=names[i], col=5:6, ylim=c(-.6,.6), \n        lwd=2, xaxt=\"n\", spaghetti=TRUE)\n axis(seq(0,256,64), side=1, at=0:4)\n lines(u, type=\"s\", col=gray(.3)) \n}\nmtext(\"seconds\", side=1, line=1.75, cex=.9)",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#white-noise",
    "href": "LectureNotes/Lecture1.html#white-noise",
    "title": "Lecture 1",
    "section": "White Noise",
    "text": "White Noise\n\nin general, a collection of random variables \\(w_t\\)\n\nuncorrelated\nmean 0, variance \\(\\sigma_w^2\\)\ndenoted \\(w_t \\sim wn(0, \\sigma_w^2)\\)\n\nfor us, usually independent and identically distributed (i.i.d.) normal\n\n\\(w_t \\sim \\text{iid } N(0, \\sigma_w^2)\\)",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#plotting-white-noise",
    "href": "LectureNotes/Lecture1.html#plotting-white-noise",
    "title": "Lecture 1",
    "section": "Plotting White Noise",
    "text": "Plotting White Noise\nWhich example does this bear the most resemblance to?\n\n\nCode\nw &lt;- rnorm(500, 0, 1)\nplot(w, type = \"l\", col = \"blue\", xlab = \"t (order of sampling)\")",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#what-white-noise-isnt",
    "href": "LectureNotes/Lecture1.html#what-white-noise-isnt",
    "title": "Lecture 1",
    "section": "What White Noise isn’t",
    "text": "What White Noise isn’t\n\nserially correlated – no temporal structure\nsmooth – “nice” trend/temporal structure\n\nHow can we build this “nice” structure into the model?",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#moving-averages-smoothing-and-filtering",
    "href": "LectureNotes/Lecture1.html#moving-averages-smoothing-and-filtering",
    "title": "Lecture 1",
    "section": "Moving Averages, Smoothing, and Filtering",
    "text": "Moving Averages, Smoothing, and Filtering\nReplace \\(w_t\\) with an average of its current value and two previous values:\n\\[\nv_t = \\frac{1}{3}(w_{t-2} + w_{t-1} + w_{t})\n\\]\n\nWhy do we divide by 3?\nIf \\(w_t \\sim \\text{iid } N(0, \\sigma_w^2)\\), what is the distribution of \\(v_t\\)?\nWhy only the previous two values? Why not one in the past and one in the future?",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#plotting-a-moving-average",
    "href": "LectureNotes/Lecture1.html#plotting-a-moving-average",
    "title": "Lecture 1",
    "section": "Plotting a Moving Average",
    "text": "Plotting a Moving Average\n\n\nCode\nv = stats::filter(w, sides = 2, filter = rep(1/3, 3))\nv_alt = stats::filter(w, sides = 1, filter = rep(1/3,3))\npar(mfrow=2:1)\ntsplot(v, ylim = c(-3, 3), col = 4, main=\"moving average\")\ntsplot(v_alt, ylim = c(-3, 3), col = 4, main=\"moving average\")\n\n\n\nCompare this moving average to the SOI and Recruitment series. How do they differ?",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#autoregressions",
    "href": "LectureNotes/Lecture1.html#autoregressions",
    "title": "Lecture 1",
    "section": "Autoregressions",
    "text": "Autoregressions\nStarting with white noise \\(w_t\\), consider the equation:\n\\[\nx_t = 1.5x_{t-1} - 0.75x_{t-2} + w_t\n\\]\n\na “second-order equation” (why?)\nA regression of the current value \\(x_t\\) of a time series as a function of the past two values of the series\n\n“auto” means self\nrecall (multiple) regression of \\(Y\\) on \\(X = (X_1, X_2)\\) is \\(Y = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\varepsilon\\) and compare to autoregression formula above\nSee (or hear) details in textbook page 11",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#plotting-autoregressions",
    "href": "LectureNotes/Lecture1.html#plotting-autoregressions",
    "title": "Lecture 1",
    "section": "Plotting Autoregressions",
    "text": "Plotting Autoregressions\n\n\nCode\nset.seed(90210)\nw = rnorm(250 + 50) # 50 extra to avoid startup problems\nx = filter(w, filter=c(1.5,-.75), method=\"recursive\")[-(1:50)]\ntsplot(x, main=\"autoregression\", col=4)",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#random-walk-with-drift",
    "href": "LectureNotes/Lecture1.html#random-walk-with-drift",
    "title": "Lecture 1",
    "section": "Random Walk with Drift",
    "text": "Random Walk with Drift\nAgain starting with white noise \\(w_t \\sim wn(0, \\sigma^2_2)\\), consider the time series\n\\[\nx_t = \\delta + x_{t-1} + w_t\n\\]\nThis is called the “random walk with drift” model.\n\n\\(\\delta\\) is the drift term (\\(\\delta = 0\\) corresponds to “random walk”- no drift)\ninitial condition \\(x_0 = 0\\)\n\nCan be rewritten\n\\[\nx_t = \\delta t + \\sum_{j=1}^t w_j\n\\]",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#plotting-a-random-walk-with-drift",
    "href": "LectureNotes/Lecture1.html#plotting-a-random-walk-with-drift",
    "title": "Lecture 1",
    "section": "Plotting a Random Walk with Drift",
    "text": "Plotting a Random Walk with Drift\n\n\nCode\nset.seed(314159265) # so you can reproduce the results\nw  = rnorm(200)  ## Gaussian white noise\nx  = cumsum(w)\nwd = w +.3 \nxd = cumsum(wd)\ntsplot(xd, ylim=c(-2,80), main=\"random walk\", ylab=\"\", col=4)\n clip(0, 200, 0, 80)\n abline(a=0, b=.3, lty=2, col=4) # drift\nlines(x, col=6)\n clip(0, 200, 0, 80)\n abline(h=0, col=6, lty=2)",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#signal-plus-noise",
    "href": "LectureNotes/Lecture1.html#signal-plus-noise",
    "title": "Lecture 1",
    "section": "Signal Plus Noise",
    "text": "Signal Plus Noise\nConsider the model:\n\\[\nx_t = 2\\cos(2\\pi\\frac{t + 15}{50}) + w_t\n\\]\n\n\\(2\\cos(2\\pi\\frac{t + 15}{50})\\) is the signal\n\\(w_t\\) is the noise",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#plotting-signal-plus-noise-two-scenarios",
    "href": "LectureNotes/Lecture1.html#plotting-signal-plus-noise-two-scenarios",
    "title": "Lecture 1",
    "section": "Plotting Signal Plus Noise (two scenarios)",
    "text": "Plotting Signal Plus Noise (two scenarios)\n\n\nCode\n# cs = 2*cos(2*pi*(1:500)/50 + .6*pi)    # as in the text\ncs = 2*cos(2*pi*(1:500+15)/50)           # same thing \nw  = rnorm(500,0,1)\npar(mfrow=c(3,1))   \ntsplot(cs, ylab=\"\", main = expression(x[t]==2*cos(2*pi*t/50+.6*pi)))\ntsplot(cs + w, ylab=\"\", main = expression(x[t]==2*cos(2*pi*t/50+.6*pi)+N(0,1)))\ntsplot(cs + 5*w, ylab=\"\", main = expression(x[t]==2*cos(2*pi*t/50+.6*pi)+N(0,25)))",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture1.html#next-time",
    "href": "LectureNotes/Lecture1.html#next-time",
    "title": "Lecture 1",
    "section": "Next Time",
    "text": "Next Time\n\nExercises at the end of chapter 1\nStart Chapter 2\n\nReview definition of covariance, correlation, expected value, and variance (good use of AI– prompt then Wikipedia?)",
    "crumbs": [
      "Week 1",
      "Lecture 1"
    ]
  },
  {
    "objectID": "Exams/Exam1.html",
    "href": "Exams/Exam1.html",
    "title": "Fall 24 Stat 416 Lecture Notes",
    "section": "",
    "text": "Gotcha!"
  },
  {
    "objectID": "Assignments/AssignmentSolutions/Assignment1_solutions.html",
    "href": "Assignments/AssignmentSolutions/Assignment1_solutions.html",
    "title": "Assignment 1",
    "section": "",
    "text": "A paper I worked on as a research scientist considered the time series of the concentration (measured as \\(\\log_{10}\\) copies per Liter) of the SARS-CoV-2 virus from 5 different locations in the City of Houston, visualized in parts (c)-(g) of the figure below.\nThe goal of this study was to see whether the information gleaned from sampling the lift stations, which represent smaller populations, was different than the information gleaned from sampling only the larger wastewater treatment plant. In other words, one research question was to determine whether the WWTP (dark blue) time series has different dynamics (behavior) than those that represent the lift stations.\nThe methods in this paper are touched on in chapter 8 of our textbook. For this assignment, we will use the wastewater data as an example and practice our plotting and time series data science skills.\n\n\n\n(a) The WWTP catchment areas for the City of Houston, with the WWTP of focus shaded. The box shows the extent of (b), the map showing the 4 lift stations considered in the analysis. (c–g) Plot the time series of Log10 Copies/L for the WWTP and the 4 lift station facilities, referred to as Lift Station A–D, with periods of missing values indicated by grey rectangles.\n\n\n\n[6 points] Which of the time series has the most missing data? Which appears to have the most variability? Does the overall behavior of the series seem to be similar?\n\nMissing data: The grey rectangles in the plots represent the missing data (per the caption). Lift station B has the most grey rectangles. In fact, it is missing data for almost half the study period, though thankfully not all in one “chunk”.\nVariability Note that the time series have different scales on the y-axis that makes it a little difficult to comment on the relative variability of the series. Lift station C and lift station D have the largest range of the y-axis, and the data span that range, so it would be reasonable to say that lift stations C and D have the most variation.\nThis was not part of the question, but you could compute the variability of each series (ignoring the fact that it is a time series to gain preliminary insight). Indeed, lift station D and C have the highest variability as estimated by standard deviation.\n\nlibrary(tidyverse)\nww &lt;- read.csv(\"https://raw.githubusercontent.com/hou-wastewater-epi-org/online_trend_estimation/refs/heads/main/Data/synthetic_ww_time_series.csv\", header = T)\n\nww %&gt;% group_by(name) %&gt;% \n  summarise(sd = sd(value, na.rm = T)) %&gt;% \n  arrange(desc(sd))\n\n# A tibble: 5 × 2\n  name              sd\n  &lt;chr&gt;          &lt;dbl&gt;\n1 Lift station D 0.872\n2 Lift station C 0.781\n3 Lift station B 0.776\n4 WWTP           0.506\n5 Lift station A 0.497\n\n\nTrend: The overall behavior of the series appears similar: the peaks and valleys seem to be aligned across the series, even if the height of the peaks is not exactly the same. We might say these time series have similar temporal structure.\n\n[5 points] Load the (synthetic) wastewater data from https://raw.githubusercontent.com/hou-wastewater-epi-org/online_trend_estimation/refs/heads/main/Data/synthetic_ww_time_series.csv using the read.csv function\n\n\n## Import the csv here into r: https://raw.githubusercontent.com/hou-wastewater-epi-org/online_trend_estimation/refs/heads/main/Data/synthetic_ww_time_series.csv\n\nww &lt;- read.csv(\"https://raw.githubusercontent.com/hou-wastewater-epi-org/online_trend_estimation/refs/heads/main/Data/synthetic_ww_time_series.csv\", header = T)\n\n## verify it worked\n\nhead(ww) ## the head() function prints the first 6 rows\n\n       dates           name    value ts_missing  colors\n1 2021-05-24 Lift station A 3.397031      FALSE #44AA99\n2 2021-05-31 Lift station A       NA       TRUE #44AA99\n3 2021-06-07 Lift station A       NA       TRUE #44AA99\n4 2021-06-14 Lift station A       NA       TRUE #44AA99\n5 2021-06-21 Lift station A 4.543146      FALSE #44AA99\n6 2021-06-28 Lift station A 4.356128      FALSE #44AA99\n\n\n\n[5 points] Inspect the data. Verify that each of the series from the map above are included in the .csv (hint: what are the unique values of the name field?)\n\n\nunique(ww$name)\n\n[1] \"Lift station A\" \"Lift station B\" \"Lift station C\" \"Lift station D\"\n[5] \"WWTP\"          \n\n\nThis technically answers the question, although if there were some issue with the data then running unique could be missleading– for example, if the .csv was cut off and there was only one observation for one fo the series, unique would return the same results as above. Instead, we can use group_by and summarize to gain more insight into the data for each series.\n\nww %&gt;% group_by(name) %&gt;% \n  summarise(n = n(), \n            mean = mean(value, na.rm = T), \n            num_missing = sum(ts_missing)) ## i included an indicator for whether a particular observation is missing for convenience.\n\n# A tibble: 5 × 4\n  name               n  mean num_missing\n  &lt;chr&gt;          &lt;int&gt; &lt;dbl&gt;       &lt;int&gt;\n1 Lift station A    95  4.83          28\n2 Lift station B    95  4.55          42\n3 Lift station C    95  4.57           9\n4 Lift station D    95  4.70          18\n5 WWTP              95  4.45           4\n\n\n\n[5 points ] Convert the date field to a Date format using the function as.Date.\n\n\nww$dates &lt;- as.Date(ww$dates)\nclass(ww$dates) ## verify it worked\n\n[1] \"Date\"\n\nhead(ww$dates) ## verify the format looks ok\n\n[1] \"2021-05-24\" \"2021-05-31\" \"2021-06-07\" \"2021-06-14\" \"2021-06-21\"\n[6] \"2021-06-28\"\n\n\nTelling R a character vector is a date allows us to reformat it using the strptime function by specifying a format argument– see the documentation for strptime for the options. Here I convert to “Month Name day, year” and also just the day of the week. Note that the time series is weekly so that we would expect all the days of the week to be the same (here Monday).\n\nhead(format(x= ww$dates, format = \"%B %d, %Y\"))\n\n[1] \"May 24, 2021\"  \"May 31, 2021\"  \"June 07, 2021\" \"June 14, 2021\"\n[5] \"June 21, 2021\" \"June 28, 2021\"\n\nhead(format(x= ww$dates, format = \"%a\"))\n\n[1] \"Mon\" \"Mon\" \"Mon\" \"Mon\" \"Mon\" \"Mon\"\n\n\n\n[2 points] Install and load the tidyverse package.\n\n\n#install.packages(\"tidyverse\") # comment this line after you run it the first time\nlibrary(tidyverse) \n\nNote that in these solutions, I already loaded tidyverse higher up since I was using some dplyr functions.\n\n[5 points] We will work with just the WWTP series for now. Use dplyr::filter to extract the values for just the WWTP series.\n\n\nww_WWTP &lt;- ww %&gt;% dplyr::filter(name == \"WWTP\")\n\n\n[10 points] What is the time interval between the observations?\n\n\nww_WWTP$dates[2] - ww_WWTP$dates[1] ## difference between first two obs\n\nTime difference of 7 days\n\ndiff(ww_WWTP$dates) ## difference between all obs-- recognize diff from lecture 5?? :)\n\nTime differences in days\n [1] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n[39] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n[77] 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7 7\n\n\n\n[10 points] Use the tsplot function from the astsa package to plot the WWTP series.\nMake sure to use the dates field for the x-axis and specify good axis and plot labels using the xlab/ylab, and main arguments. (see the documentation ?tsplot for more)\n\n\nlibrary(astsa) \ntsplot(x = ww_WWTP$dates, y = ww_WWTP$value, \"\")\n\n\n\n\n\n\n\n\n\n[10 points]Apply a moving average filter with 3 time points using the stats::filter function and save the result in a vector called ww_ma_3. You can choose the order of the moving average. (Similar to the final part of problem 1.1, see here in Lecture Notes).\n\n\nww_ma_3 &lt;- stats::filter(ww_WWTP$value, filter = rep(1/3, 3), sides = 1)\n\n10.[10 points] Plot the moving average you computed on top of the tsplot in a different color using the lines function (see linked Problem 1.1 above). In the call to the lines function, also use type = \"l\" and lwd = 2.\n\nlibrary(astsa)\ntsplot(x = ww_WWTP$dates, y = ww_WWTP$value, \n       main = \"WWTP Series and Moving Average filter with 3 time points\",\n       type = \"b\", xlab = \"Log10 Copies/L\")\nlines(x = ww_WWTP$dates, y = ww_ma_3, lwd = 2, \n      col = \"blueviolet\", type = \"l\")\n\n\n\n\n\n\n\n\n\n[15 points] Apply the moving average filter again, but this time use 5 time points, call it ww_ma_5. Plot just the wastewater series and the ww_ma_5 you just computed, and use a different color for this MA process than you used in question 10.\n\n\nww_ma_5 &lt;- stats::filter(ww_WWTP$value, filter = rep(1/5, 5), sides = 1)\ntsplot(x = ww_WWTP$dates, y = ww_WWTP$value, \n       main = \"Observed WWTP Series and Moving average smoother with 5 time points\",\n       type = \"b\")\nlines(x = ww_WWTP$dates, y = ww_ma_5, lwd = 2, \n      col = \"darkgreen\", type = \"l\")\n\n\n\n\n\n\n\n\n\n[5 points]Inspect the plot you generated in questions 10 and 11. Which MA process looks “smoother”?\n\nSince we are just visually inspecting time series, different people may notice different things when looking at the same plot. I would say that the smoother which uses five time points appears smoother: some of the places where the 3-point moving average appears particularly “jagged” is smoother for the 5-point. It’s a but easier to see when you plot them on the same plot.\n\ntsplot(x = ww_WWTP$dates, y = ww_WWTP$value, \n       main = \"Observed WWTP Series and both Moving Averages\", type = \"b\")\nlines(x = ww_WWTP$dates, y = ww_ma_5, \n      lwd = 2, col = \"darkgreen\", type = \"l\")\nlines(x = ww_WWTP$dates, y = ww_ma_5, \n      lwd = 2, col = \"darkgreen\", type = \"l\")\nlines(x = ww_WWTP$dates, y = ww_ma_3, \n      lwd = 2, col = \"blueviolet\", type = \"l\")\nlegend(\"bottomright\", legend = c(\"3-point\", \"5-point\"), \n       col = c(\"blueviolet\",\"darkgreen\"), lwd =2)\nrect(xleft = 19140, xright = 19161, \n     ytop = 4.7, ybottom = 4.1, \n     border = \"magenta\", lwd = 2)\n\n\n\n\n\n\n\n\n\n[10 points] Describe the different way that the missing data in the WWTP series impacts the moving average estimates for the case of 3 time points vs. 5 time points.\n\nConsider the right hand side of the missing data period near the beginning of 2022. The 3-point moving average (purple line) is able to “restart” sooner than the 5-point moving average (green line). This is because the 3-point moving average just has to wait until it has 3 points to re-start the estimation, wheras the 5-point must wait for 5 points.\n\n[5 points] Note that the data you used for this activity was “synthetic” wastewater data. Why might a researcher share a synthetic version of their data? What do you think that might mean?\n\nSynthetic data (at least how I use it) are data that are not directly observed, but rather simulated (or sampled) from a model that is fit using real data. In other words, it is made-up data that has similar properties to the real data. Researchers might decide to share synthetic data when they want people to be able to run their code for a paper, but the data are sensitive in nature, for example, have personally identifiable health information. Here, we followed CDC guidelines to not release the real data (or real names of the lift stations and WWTP) when the population served by the facility being sampled is less than 4,000."
  },
  {
    "objectID": "Assignments/Assignment2_updated.html",
    "href": "Assignments/Assignment2_updated.html",
    "title": "Assignment 2 Due 10/7 at Midnight",
    "section": "",
    "text": "NOTE: I forgot to include a relevant detail for Part 2, number 7. The change is bolded. Sorry!!! Part 2 number 8 should be easier to answer now.",
    "crumbs": [
      "Week 2",
      "Assignment 2"
    ]
  },
  {
    "objectID": "Assignments/Assignment2_updated.html#part-1-math",
    "href": "Assignments/Assignment2_updated.html#part-1-math",
    "title": "Assignment 2 Due 10/7 at Midnight",
    "section": "Part 1: Math",
    "text": "Part 1: Math\nIn class, we have worked with “Signal plus noise Model” (equation 1.5)\n\\[\n\\begin{aligned}\n\\text{Model: }& x_t = 2\\cos(2\\pi\\frac{t+15}{50}) + w_t\\\\\n\\text{Mean function: }& \\mathbb{E}(x_t) = 2\\cos(2\\pi\\frac{t+15}{50})\n\\end{aligned}\n\\]\n\n[5 points] The mean function is derived in Example 2.4. Describe what happens in each step of the computation [3 points], and provide a “math stress” rating (1 = effortless, 100 = nightmare) and 3 emojis[2 points]. This is personal and there is no right answer.\n[5 points] Is the signal plus noise model stationary in the mean?\n[5 points] Write down \\(\\gamma_x(s,t)\\), the autocovariance function of \\(x_t\\) [3 points]. You may accomplish this in any way, including asking me personally in office hours or asking a classmate. Just make sure you cite the source![2 points]\n[6 points] Consider the model:\n\\[\ny_t = x_t - 2\\cos(2\\pi\\frac{t+15}{50})\n\\]\nCompute the mean function of \\(y_t\\) [3 points]. Is \\(y_t\\) stationary in the mean?[1 point] How do you know?[2 points]",
    "crumbs": [
      "Week 2",
      "Assignment 2"
    ]
  },
  {
    "objectID": "Assignments/Assignment2_updated.html#part-2-code",
    "href": "Assignments/Assignment2_updated.html#part-2-code",
    "title": "Assignment 2 Due 10/7 at Midnight",
    "section": "Part 2: Code",
    "text": "Part 2: Code\nNote: I have set the code chunks here to have eval: false in the code chunk. Change that to true so that I can run your code easily.\n\n[5 points] All your code runs without errors (unless that’s the point), and if there is a message, explain what it means. (Bonus: to be nice to me, submit a rendered pdf)\n[5 points] Simulate from an AR(1) process with coefficient 0.7 and 10 data points.\n\n\nlibrary(astsa)\n\n# your code here\n\n\n[6 points] Look at the documentation for the stats::lag function (run ?lag in the console). State what package the function is in and what the function does[4 points]. Using k = 1 compute a lag(1) version of x_t that you simulated above[2 points].\n\n\nx_t_lag1 &lt;- # your code here\n\n\n[3 points] Run the following code and compare x_t and x_t_lag1.\n\n\ncbind(x_t, x_t_lag1)\n\n\nMake a time series plot of x_t and x_t_1. Do you notice the same features as when in the previous question?\n\n\n# your code here\n\n\nRun the below code. Why are the plots different? Are either particularly useful?\n\n\nplot(x_t, x_t_lag1)\nplot(as.vector(x_t), as.vector(x_t_lag1))\n\n\nInstead of using stats::lag, use dplyr::lag to create a new version of x_t_lag. Repeat the code from steps 2-5. Describe how the output has changed.\n\n\nx_t_lag1 &lt;- dplyr::lag(# your code here)\n\n\nRe-simulate an AR(1) process as in number 1, but this time with 100 observations. Also recompute x_t_lag1. Fit an intercept-free regression model to predict x_t from x_t_lag. Provide the value of the slope estimate and interpret the value in the context of this simulation.\n\n\nlinear_model &lt;- # your code here\n\n\n[11 points] Plot the acf of x_t[2 points] and the acf of the residuals from the regression model[4 points]. Which looks more like white noise?[2 points] What does this tell you about the temporal structure in x_t and its residuals?[3 points]\n\n\n# your code here",
    "crumbs": [
      "Week 2",
      "Assignment 2"
    ]
  },
  {
    "objectID": "Assignments/Assignment2_updated.html#part-3-reading",
    "href": "Assignments/Assignment2_updated.html#part-3-reading",
    "title": "Assignment 2 Due 10/7 at Midnight",
    "section": "Part 3: Reading",
    "text": "Part 3: Reading\n[9 points] Read sections 2.8 and 2.9 from Forecasting Principles and Practice. Make 3 connections [3 points each] to content from the course textbook (equations or similar examples.).",
    "crumbs": [
      "Week 2",
      "Assignment 2"
    ]
  },
  {
    "objectID": "Assignments/Assignment1.html",
    "href": "Assignments/Assignment1.html",
    "title": "Stat 416 Assignment 1 Due Monday, September 30 at 11:59:59PM",
    "section": "",
    "text": "A paper I worked on as a research scientist considered the time series of the concentration (measured as \\(\\log_{10}\\) copies per Liter) of the SARS-CoV-2 virus from 5 different locations in the City of Houston, visualized in parts (c)-(g) of the figure below.\nThe goal of this study was to see whether the information gleaned from sampling the lift stations, which represent smaller populations, was different than the information gleaned from sampling only the larger wastewater treatment plant. In other words, one research question was to determine whether the WWTP (dark blue) time series has different dynamics (behavior) than those that represent the lift stations.\nThe methods in this paper are touched on in chapter 8 of our textbook. For this assignment, we will use the wastewater data as an example and practice our plotting and time series data science skills.\n\n\n\n(a) The WWTP catchment areas for the City of Houston, with the WWTP of focus shaded. The box shows the extent of (b), the map showing the 4 lift stations considered in the analysis. (c–g) Plot the time series of Log10 Copies/L for the WWTP and the 4 lift station facilities, referred to as Lift Station A–D, with periods of missing values indicated by grey rectangles.\n\n\n\nWhich of the time series has the most missing data? Which appears to have the most variability? Does the overall behavior of the series seem to be similar?\nLoad the (synthetic) wastewater data from https://raw.githubusercontent.com/hou-wastewater-epi-org/online_trend_estimation/refs/heads/main/Data/synthetic_ww_time_series.csv using the read.csv function\nInspect the data. Verify that each of the series from the map above are included in the .csv (hint: what are the unique values of the name field?)\nConvert the date field to a Date format using the function as.Date.\nInstall and load the tidyverse package.\nWe will work with just the WWTP series for now. Use dplyr::filter to extract the values for just the WWTP series.\n\nww &lt;- read.csv(#your code here)\n\nww$dates &lt;- as.Date(# your code here)\n\n#install.packages(\"tidyverse\")\nlibrary(tidyverse)\nww_WWTP &lt;- ww %&gt;% dplyr::filter(#your code here)\n\nWhat is the time interval between the observations? How do you know?\nUse the tsplot function from the astsa package to plot the WWTP series.\nMake sure to use the dates field for the x-axis and specify good axis and plot labels using the xlab/ylab, and main arguments. (see the documentation ?tsplot for more)\nApply a moving average filter with 3 time points using the stats::filter function and save the result in a vector called ww_ma_3. You can choose the order of the moving average. (Similar to the final part of problem 1.1, see here in Lecture Notes).\n\nww_ma_3 &lt;- stats::filter(#your code here)\n\nPlot the moving average you computed on top of the tsplot in a different color using the lines function (see linked Problem 1.1 above). In the call to the lines function, also use type = l and lwd = 2.\n\ntsplot(# your code here)\nlines(# your code here)\n\nApply the moving average filter again, but this time use 5 time points, call it ww_ma_5. Plot just the WWTP series data and the ww_ma_5 you just computed, and use a different color for this MA process than you used in question 10.\nInspect the plot you generated in questions 10 and 11. Which MA process looks “smoother”?\nDescribe the different way that the missing data in the WWTP series impacts the moving average estimates for the case of 3 time points vs. 5 time points.",
    "crumbs": [
      "Week 1",
      "Assignment 1"
    ]
  },
  {
    "objectID": "Assignments/Assignment3.html",
    "href": "Assignments/Assignment3.html",
    "title": "Assignment 3 Due Friday, October 18 at midnight",
    "section": "",
    "text": "Download the .qmd source for this document here.",
    "crumbs": [
      "Week 3",
      "Assignment 3"
    ]
  },
  {
    "objectID": "Assignments/Assignment3.html#math",
    "href": "Assignments/Assignment3.html#math",
    "title": "Assignment 3 Due Friday, October 18 at midnight",
    "section": "1. Math",
    "text": "1. Math\nRecall the so-called “trend-stationary” model \\[\nx_t = \\mu_t + y_t\n\\]\n\n[3 points] There are three terms (symbols) in that equation. Which symbol corresponds to the trend, and which corresponds to the stationary part? What would you call the remaining symbol?\nWe have considered a few possible models for the trend: \\[\n\\begin{aligned}\n\\mu_{LM,t} &= \\beta_0 + \\beta_1 t  \\\\\n\\mu_{RW,t} &=  \\delta + \\mu_{RW, t-1} + w_t\n\\end{aligned}\n\\]\n[2 points] What do “RW” and “LM” stand for?\n[4 points] Between \\(\\mu_{LM,t}\\) and \\(\\mu_{RW,t}\\), which has “more interesting” temporal structure[2 points]? Why[2 points]?\n[4 points] Write down the equations for \\(x_{LM, t}\\) and \\(x_{RW,t}\\)\n\nConsider this (edited) excerpt from the textbook (page 49):\n\n\n\n\n\n\nDifferencing vs. Detrending\n\n\n\nOne advantage of differencing over detrending to remove trend is that no parameters are estimated in the differencing operation. One disadvantage, however, is that differencing does not yield and estimate of the stationary process \\(y_t\\).\nFor example, if we difference a random walk \\(x_t\\) with drift \\(\\delta\\),\n\\[\nx_t - x_{t-1} = \\delta + w_t + y_t - y_{t-1}.\n\\]\nIf an estimate of \\(y_t\\) is essential, then detrending may be more appropriate. This would be the case, for example, if we were interested in the business cycle of commodities.\nIf the goal is to coerce the data to stationarity, then differencing may be more appropriate. Differencing is also a viable tool if the trend is fixed, that is, when using \\(\\mu_{LM,t}\\) as the trend model, we have: \\[\nx_t - x_{t-1} = \\beta_1 + y_t - y_{t-1}\n\\] Because differencing plays a central role in time series analysis, it receives its own notaiton. The first difference is denoted: \\[\n\\nabla x_t = x_t - x_{t-1}\n\\] As we have seen, the first difference eliminates a linear trend. A second difference can eliminate a quadratic trend, and so on. Differences of order \\(d\\) are denoted: \\[\n\\nabla^d = (1-B)^d\n\\]\n\n\n\n[1 point] Take a look in the book. What is \\(B\\) called?\n[6 points] Why is it an “advantage” that no parameters are estimated in the differencing operation?\n[6 points] Connect part 2 question 9 below to a sentence in the above excerpt.\n[10 points] What kind of plot might you make to check if the data has been “coerced to stationarity”?\n[3 points] Rate your math anxiety (1 = effortless, 100 = nightmare) while working on this problem.",
    "crumbs": [
      "Week 3",
      "Assignment 3"
    ]
  },
  {
    "objectID": "Assignments/Assignment3.html#data-analysis-code",
    "href": "Assignments/Assignment3.html#data-analysis-code",
    "title": "Assignment 3 Due Friday, October 18 at midnight",
    "section": "2. Data Analysis (code)",
    "text": "2. Data Analysis (code)\n\n[10 points] Adapt the code from Example 1.2 in the book to plot just the global land temperature series in a time series plot. [2 points]. Describe the structure of the trend and/or seasonal components, if present[8 points].\n\n\n# your code here\n\n\n[4 points] How frequently were the observations collected?\n\n\n# your code here\n\n\n[5 points] Plot the autocorrelation function of the global land temperature series. Comment on the temporal structure.\n\n\n# your code here\n\n\n[15 points] Estimate the trend of the series using a symmetric, equally weighted 5-point moving average[4 points]. Plot the trend estimate on top of the data [4 points]. Comment on the trend– does it reveal any patterns difficult to see in the data?[7 points]\n\n\n# your code here\n\n\n[10 points] Estimate the trend of the series using a linear regression on time[3 points]. Plot the trend estimate on top of the data[3 points]. Is this trend estimate comparable to the moving average?[4 points]\n\n\n# your code here\n\n\n[15 points] De-trend the data with respect to each of the trends you estimated [4 points each]. Make a time series plot of each result [1 point each] and an acf of each result [1 point each]. Comment on the temporal structure in each of the plots.\n\n\n# your code here\n\n\n[20 points] Difference the time series and plot the result[5 points]. Also compute the acf and plot it[5 points]. Comment on whether the differencing has “coerced the data to stationarity”[10 points].\n\n\n# your code here\n\n\n[11 points] Estimate the sum of squared error for both the linear regression and the moving average [3 points each]. What do they suggest about which model is “better”?[3 points] Does this agree with your visual assessments in 3 and 4?[2 points]\n\n\n# your code here\n\n\n[10 points] Why can’t you do part 6 for the differenced series?\n[3 points] Ensure that when you render the document to .html to turn in that you have embed-resources: true in the options at the top of the document (you may also turn in a pdf). Also, set message to false for all the code chunks.",
    "crumbs": [
      "Week 3",
      "Assignment 3"
    ]
  },
  {
    "objectID": "Assignments/Assignment3.html#the-literature",
    "href": "Assignments/Assignment3.html#the-literature",
    "title": "Assignment 3 Due Friday, October 18 at midnight",
    "section": "3. The literature",
    "text": "3. The literature\nThe original paper for the data set you just analyzed can be found here.\n\n[2 points] Set a timer for 5 minutes. Find one sentence you feel you understand, and one you do not understand but would like to understand better.\n[4 points] Take a look at Figure 6. There are six time series plotted. One is the data plotted in number 1 of the coding portion, the other is the moving average estimate estimated in number 3. Which of the six series are they?\n[4 points] The textbook for our class states on page 3 that “the data are annual temperature anomalies averaged over the Earth’s land area.”. Does the book state what is specifically meant by “anomaly” here?\n[2 points] Consider the concept of averaging over the Earth’s land area, then take a look at Figure 1. Describe in therms of the “circles” how you might calculate that global average.\n[15 points] Figure 3 plots correlation coefficients between annual mean temperature changes for pairs of randomly selected stations having at least 50 common years in their records. How can you rephrase “annual mean tempearture changes” in terms of detrending or differencing?",
    "crumbs": [
      "Week 3",
      "Assignment 3"
    ]
  },
  {
    "objectID": "Assignments/Assignment3.html#weights",
    "href": "Assignments/Assignment3.html#weights",
    "title": "Assignment 3 Due Friday, October 18 at midnight",
    "section": "4. Weights",
    "text": "4. Weights\n\n[6 points] One might consider the points I have given to each numbered problem as “weights”. Based on the weights, which content is the most important? Answer on both the individual problem level and the section level (i.e. math, code, etc).",
    "crumbs": [
      "Week 3",
      "Assignment 3"
    ]
  },
  {
    "objectID": "Assignments/AssignmentSolutions/Assignment2_updated.html",
    "href": "Assignments/AssignmentSolutions/Assignment2_updated.html",
    "title": "Solutions: Assignment 2 Due 10/7 at Midnight",
    "section": "",
    "text": "NOTE: I forgot to include a relevant detail for Part 2, number 7. The change is bolded. Sorry!!! Part 2 number 8 should be easier to answer now."
  },
  {
    "objectID": "Assignments/AssignmentSolutions/Assignment2_updated.html#part-1-math",
    "href": "Assignments/AssignmentSolutions/Assignment2_updated.html#part-1-math",
    "title": "Solutions: Assignment 2 Due 10/7 at Midnight",
    "section": "Part 1: Math",
    "text": "Part 1: Math\nIn class, we have worked with “Signal plus noise Model” (equation 1.5)\n\\[\n\\begin{aligned}\n\\text{Model: }& x_t = 2\\cos(2\\pi\\frac{t+15}{50}) + w_t\\\\\n\\text{Mean function: }& \\mathbb{E}(x_t) = 2\\cos(2\\pi\\frac{t+15}{50})\n\\end{aligned}\n\\]\n\n[5 points] The mean function is derived in Example 2.4. Describe what happens in each step of the computation [3 points], and provide a “math stress” rating (1 = effortless, 100 = nightmare) and 3 emojis[2 points]. This is personal and there is no right answer.\n\n The first line of the computation plugs the equation for our particular time series model, \\(x_t\\), into the definition of the mean function (expected value of \\(x_t\\)), and also has the notation \\(\\mu_{xt}\\) for the mean function. Two steps occur in the second line: the linearity of expectation splits the formula into the sum of two terms, then rewrites \\(\\E(2\\cos(2\\pi\\frac{t+15}{50}))\\) as \\(2\\pi\\frac{t+15}{50}\\) since that expression is a constant with respect to the expectation. Finally, the steps between the second and third lines is to know that \\(\\E(w_t)\\) is 0 since \\(w_t\\) is white noise. \n\n[5 points] Is the signal plus noise model stationary in the mean?\n\nNo. There is a \\(t\\) in the equation, so the mean function depends on \\(t\\), which violates the definition of mean stationarity.\n\n[5 points] Write down \\(\\gamma_x(s,t)\\), the autocovariance function of \\(x_t\\) [3 points]. You may accomplish this in any way, including asking me personally in office hours or asking a classmate. Just make sure you cite the source![2 points] ** When \\(s \\ne t\\), the autocovariance function is computed by starting with the formula for our time series \\(x_t\\) and the definition of autocovariance:\n\n\\[\n\\begin{align}\n\\gamma_x(s,t) &= E[(x_t - \\mu_{xt})(x_s - \\mu_{xs})] \\\\\n&= E[(2\\cos(2\\pi\\frac{t+15}{50}) + w_t - \\mu_{xt})(2\\cos(2\\pi\\frac{s+15}{50}) + w_s - \\mu_{xs})] \\\\\n&= E[(2\\cos(2\\pi\\frac{t+15}{50}) + w_t - 2\\cos(2\\pi\\frac{t+15}{50}))(2\\cos(2\\pi\\frac{s+15}{50}) + w_s - 2\\cos(2\\pi\\frac{s+15}{50}))] \\\\\n& = E[(w_t)(w_s)] \\\\\n&= 0\n\\end{align}\n\\] Where the last step yields zero because \\(w_t\\) and \\(w_s\\) are independent. If \\(s = t\\), then the steps are the same, but at the last step we get \\[\n\\gamma_x(t,t) = E(w_t\\cdot w_t) = var(w_t) = \\sigma_w^2\n\\] in other words, the variance of the underlying white noise process.\nThe autocovariance function is then: \\[\n\\begin{cases}\n\\sigma^2_w & \\text{ if } s=t \\\\\n0 & \\text{ if } s \\ne t\n\\end{cases}\n\\] **\n\n[6 points] Consider the model:\n\\[\ny_t = x_t - 2\\cos(2\\pi\\frac{t+15}{50})\n\\]\nCompute the mean function of \\(y_t\\) [3 points]. Is \\(y_t\\) stationary in the mean?[1 point] How do you know?[2 points]"
  },
  {
    "objectID": "Assignments/AssignmentSolutions/Assignment2_updated.html#part-2-code",
    "href": "Assignments/AssignmentSolutions/Assignment2_updated.html#part-2-code",
    "title": "Solutions: Assignment 2 Due 10/7 at Midnight",
    "section": "Part 2: Code",
    "text": "Part 2: Code\nNote: I have set the code chunks here to have eval: false in the code chunk. Change that to true so that I can run your code easily.\n\n[5 points] All your code runs without errors (unless that’s the point), and if there is a message, explain what it means. (Bonus: to be nice to me, submit a rendered pdf)\n[5 points] Simulate from an AR(1) process with coefficient 0.7 and 10 data points.\n\n\nlibrary(astsa)\n\n# your code here\n\nw &lt;- rnorm(10) \nx_t &lt;- stats::filter(x = w, filter = 0.7, method = \"recursive\")\n\n\n[6 points] Look at the documentation for the stats::lag function (run ?lag in the console). State what package the function is in and what the function does[4 points]. Using k = 1 compute a lag(1) version of x_t that you simulated above[2 points].\n\n\nx_t_lag1 &lt;- stats::lag(x_t, k = 1)\n\n\n[3 points] Run the following code and compare x_t and x_t_lag1.\n\n\ncbind(x_t, x_t_lag1)\n\nTime Series:\nStart = 0 \nEnd = 10 \nFrequency = 1 \n          x_t   x_t_lag1\n 0         NA -0.3371633\n 1 -0.3371633 -0.9705690\n 2 -0.9705690 -0.6883777\n 3 -0.6883777 -0.6471764\n 4 -0.6471764 -0.9895124\n 5 -0.9895124 -1.4479744\n 6 -1.4479744 -1.9317659\n 7 -1.9317659 -1.0137444\n 8 -1.0137444 -0.4593553\n 9 -0.4593553 -0.7947935\n10 -0.7947935         NA\n\n\n\nMake a time series plot of x_t and x_t_1. Do you notice the same features as when in the previous question?\n\n\ntsplot(x_t)\n\n\n\n\n\n\n\ntsplot(x_t_lag1)\n\n\n\n\n\n\n\n\n\nRun the below code. Why are the plots different? Are either particularly useful?\n\n\nplot(x_t, x_t_lag1)\nplot(as.vector(x_t), as.vector(x_t_lag1))\n\n\nInstead of using stats::lag, use dplyr::lag to create a new version of x_t_lag. Repeat the code from steps 2-5. Describe how the output has changed.\n\n\nx_t_lag1 &lt;- dplyr::lag(as.vector(x_t), n = 1)\nplot(x_t, x_t_lag1)\n\n\n\n\n\n\n\nplot(as.vector(x_t), as.vector(x_t_lag1))\n\n\n\n\n\n\n\n\n\nRe-simulate an AR(1) process as in number 1, but this time with 100 observations. Also recompute x_t_lag1. Fit an intercept-free regression model to predict x_t from x_t_lag. Provide the value of the slope estimate and interpret the value in the context of this simulation.\n\n\nw &lt;- rnorm(100) \nx_t &lt;- stats::filter(x = w, filter = 0.7, method = \"recursive\")\n\nx_t_lag1 &lt;- dplyr::lag(as.vector(x_t), n = 1)\n\nlinear_model &lt;- lm(x_t ~ -1 + x_t_lag1)\nsummary(linear_model)\n\n\nCall:\nlm(formula = x_t ~ -1 + x_t_lag1)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.52302 -0.47667  0.07181  0.64663  2.11563 \n\nCoefficients:\n         Estimate Std. Error t value Pr(&gt;|t|)    \nx_t_lag1  0.70012    0.07204   9.719 4.94e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.9695 on 98 degrees of freedom\n  (1 observation deleted due to missingness)\nMultiple R-squared:  0.4908,    Adjusted R-squared:  0.4856 \nF-statistic: 94.45 on 1 and 98 DF,  p-value: 4.94e-16\n\n\n\n[11 points] Plot the acf of x_t[2 points] and the acf of the residuals from the regression model[4 points]. Which looks more like white noise?[2 points] What does this tell you about the temporal structure in x_t and its residuals?[3 points]\n\n\nacf(x_t)\nacf(residuals(linear_model))"
  },
  {
    "objectID": "Assignments/AssignmentSolutions/Assignment2_updated.html#part-3-reading",
    "href": "Assignments/AssignmentSolutions/Assignment2_updated.html#part-3-reading",
    "title": "Solutions: Assignment 2 Due 10/7 at Midnight",
    "section": "Part 3: Reading",
    "text": "Part 3: Reading\n[9 points] Read sections 2.8 and 2.9 from Forecasting Principles and Practice. Make 3 connections [3 points each] to content from the course textbook (equations or similar examples.)."
  },
  {
    "objectID": "Exams/PracticeMidterm.html",
    "href": "Exams/PracticeMidterm.html",
    "title": "Fall 24 Stat 416 Lecture Notes",
    "section": "",
    "text": "idea: a moving average itself is stationary. Is a moving average of an arbitrary time series stationary?\ntwo moving average, with different weights"
  },
  {
    "objectID": "LectureNotes/Lecture2.html#recap-from-last-time",
    "href": "LectureNotes/Lecture2.html#recap-from-last-time",
    "title": "Lecture 2",
    "section": "Recap from last time",
    "text": "Recap from last time\n\n\\[\n\\newcommand\\E{{\\mathbb{E}}}\n\\]\n\n\nSeveral examples of time series data sets\nExperience plotting the time series\nExposure to some common time series models",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#today",
    "href": "LectureNotes/Lecture2.html#today",
    "title": "Lecture 2",
    "section": "Today",
    "text": "Today\n\nNotation review\nMean and covariance function of a time series\nR code activity\nStationarity (if time)",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#coming-upnotices",
    "href": "LectureNotes/Lecture2.html#coming-upnotices",
    "title": "Lecture 2",
    "section": "Coming up/notices",
    "text": "Coming up/notices\n\nI combined the Canvas sections (applies to section 2)\nQuiz 1 posted today, due tomorrow at midnight (20 minutes to do it)\nAssignment 1 will also be posted today, due Monday at midnight (boundary between Monday and Tuesday)\nNext week’s office hours: M 4-5, T 12-2",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#notation-and-data--white-noise",
    "href": "LectureNotes/Lecture2.html#notation-and-data--white-noise",
    "title": "Lecture 2",
    "section": "Notation and Data- White noise",
    "text": "Notation and Data- White noise\n“Let \\(w_t\\) be a white noise series”\n\n\n\nt\nRandom Variable\nExample data\n\n\n\n\n1\n\\(w_1 \\sim N(0, \\sigma_w^2)\\)\n-0.2917993\n\n\n2\n\\(w_2 \\sim N(0, \\sigma_w^2)\\)\n0.8280409\n\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nt\n\\(w_t \\sim N(0, \\sigma^2_w)\\)\n-0.697145\n\n\n\\(\\vdots\\)\n\\(\\vdots\\)\n\\(\\vdots\\)\n\n\nn\n\\(w_n \\sim N(0, \\sigma_w^2)\\)\n0.9557592\n\n\n\nIf we interpret the collection of \\(w_t\\) as a random vector, then \\(w_t \\sim MVN(\\vec{0}, I)\\) (why \\(I\\)?)\nNote: sometimes \\(w_t\\) could mean a (univariate) value of a white noise series for a particular time \\(t\\) (kind of like how you refer to an arbitrary \\(x_i\\) when you have a sample \\(x_1, \\dots, x_n\\)).",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#aside-the-multivariate-normal-distribution",
    "href": "LectureNotes/Lecture2.html#aside-the-multivariate-normal-distribution",
    "title": "Lecture 2",
    "section": "(Aside) The Multivariate normal distribution",
    "text": "(Aside) The Multivariate normal distribution\nLet’s look on Wikipedia. What are the parameters?\n\nmean vector\nvariance (covariance) matrix\n\nIf the covariance matrix is the identity matrix, the the covariances are 0",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#aside-bivariate-normal-distribution-for-uncorrelated-case",
    "href": "LectureNotes/Lecture2.html#aside-bivariate-normal-distribution-for-uncorrelated-case",
    "title": "Lecture 2",
    "section": "(Aside) Bivariate normal distribution for uncorrelated case",
    "text": "(Aside) Bivariate normal distribution for uncorrelated case\n\n\nCode\n# install.packages(\"ggplot2\")\n# install.packages(\"ggExtra\")\nlibrary(ggplot2)\nlibrary(ggExtra)\n\nx1 &lt;- rnorm(100, 10, 5)\nx2 &lt;- rnorm(100, .1, .5)\n\nx &lt;- data.frame(x1, x2)\n# Save the scatter plot in a variable\np &lt;- ggplot(x, aes(x = x1, y = x2)) +\n  geom_point()\n\n# Arguments for each marginal histogram\nggMarginal(p, type = \"histogram\", \n           xparams = list(fill = 4),\n           yparams = list(fill = 3))",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#aside-bivariate-normal-distribution-for-correlated-case",
    "href": "LectureNotes/Lecture2.html#aside-bivariate-normal-distribution-for-correlated-case",
    "title": "Lecture 2",
    "section": "(Aside) Bivariate normal distribution for correlated case",
    "text": "(Aside) Bivariate normal distribution for correlated case\n\n\nCode\n#install.packages(\"MASS\")\nlibrary(MASS)\n\nmu &lt;- c(10, .1)\nvarcov &lt;- matrix(c(5, 1, 1, .5), \n                 ncol = 2)\nx&lt;- mvrnorm(100, mu = mu, Sigma =varcov)\nx &lt;- data.frame(x1 = x[,1], x2 = x[,2])\n# Save the scatter plot in a variable\np &lt;- ggplot(x, aes(x = x1, y = x2)) +\n  geom_point()\n\n# Arguments for each marginal histogram\nggMarginal(p, type = \"histogram\", \n           xparams = list(fill = 4),\n           yparams = list(fill = 3))",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#building-time-series-models-from-white-noise",
    "href": "LectureNotes/Lecture2.html#building-time-series-models-from-white-noise",
    "title": "Lecture 2",
    "section": "Building time series models from White Noise",
    "text": "Building time series models from White Noise\n\n\n\n\n\n\n\n\nModel\nInputs\nOutput\n\n\n\n\nWhite noise\nprobability distribution, independence assumption, \\(\\sigma_w^2\\)\n\n\n\nMoving average with \\(p\\) points\n\\(w_1, w_2, \\dots, w_n\\)\n\n\n\nAutoregression of order \\(p\\)\n\\(w_1, w_2, \\dots, w_n\\) and \\(\\phi = (\\phi_1, \\dots, \\phi_p)\\)\n\n\n\nRandom walk with drift\n\\(w_1, w_2, \\dots, w_n\\) and \\(\\delta\\)\n\n\n\nSignal plus noise\n\\(w_1, w_2, \\dots, w_n\\) and a function \\(f(t)\\)\n\n\n\n\nIdentify which of the inputs are random variables, pre-specified constants, pre-specified functions, or parameters to be estimated.",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#building-time-series-models-from-white-noise-1",
    "href": "LectureNotes/Lecture2.html#building-time-series-models-from-white-noise-1",
    "title": "Lecture 2",
    "section": "Building time series models from White Noise",
    "text": "Building time series models from White Noise\n\n\n\n\n\n\n\n\nModel\nInputs\nOutput\n\n\n\n\nWhite noise\nprobability distribution, independence assumption, \\(\\sigma_w^2\\)\n\\(w_1, w_2, \\dots, w_n\\); for each \\(t = 1, \\dots, n\\) we have \\(w_t \\sim N(0, \\sigma^2_w)\\)\n\n\nMoving average with \\(p\\) points\n\\(w_1, w_2, \\dots, w_n\\)\n\\(v_t = \\frac{1}{p}\\sum_{i = 1}^{p} w_{t-(p-i)}\\)\n\n\nAutoregression of order \\(p\\)\n\\(w_1, w_2, \\dots, w_n\\) and \\(\\phi = (\\phi_1, \\dots, \\phi_p)\\)\n\\(x_t = \\sum_{i = 1}^p \\phi_ix_{t-i} + w_t\\)\n\n\nRandom walk with drift\n\\(w_1, w_2, \\dots, w_n\\) and \\(\\delta\\)\n\\(x_t = \\delta + x_{t-1} + w_t\\)\n\n\nSignal plus noise\n\\(w_1, w_2, \\dots, w_n\\) and a function \\(f(t)\\)\n\\(x_t = f(t) + w_t\\)\n\n\n\nIdentify which of the inputs are random variables, pre-specified constants, pre-specified functions, or parameters to be estimated.",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#notation-and-data",
    "href": "LectureNotes/Lecture2.html#notation-and-data",
    "title": "Lecture 2",
    "section": "Notation and Data",
    "text": "Notation and Data\nConsider the general version of the autoregressive model of order 1:\n\\[\nx_t = \\phi_1x_{t-1} + \\phi_2x_{t-2} + w_t\n\\]\nIf you had data representing this process, what would it look like in R?",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#notation-and-data-1",
    "href": "LectureNotes/Lecture2.html#notation-and-data-1",
    "title": "Lecture 2",
    "section": "Notation and Data",
    "text": "Notation and Data\nSuppose \\(\\phi_1 = 1.5\\) and \\(\\phi_2 = -0.75\\).\n\n\nCode\nset.seed(90210)\nw = rnorm(250 + 50) # 50 extra to avoid startup problems\nx = filter(w, filter=c(1.5,-.75), method=\"recursive\")[-(1:50)]\nx\n\n\n  [1] -0.635871231 -3.159366457 -3.336983558 -0.670017029  1.928041062\n  [6]  6.262719337  8.811276769  6.994297589  6.964249838  4.172630149\n [11]  0.109387891 -2.838470465 -3.650839732 -5.293859716 -4.924149166\n [16] -3.496962661 -0.001206165  3.982335012  5.166059171  5.391303364\n [21]  4.598152813  2.726933281 -0.656314289 -2.634587218 -3.070392399\n [26] -2.447369835 -2.377035961 -3.272222376 -2.212579163 -1.609152064\n [31]  0.088151906  1.834292884  1.566977751  1.162326919  1.731270484\n [36]  0.452095019  0.751851590  2.197474589  2.037090911  2.053962776\n [41]  1.057859241  0.173798276 -1.559467228 -1.275347235 -1.934447794\n [46] -0.637721288  1.108281203  1.703590245  2.757116948  3.535041828\n [51]  2.785518718 -0.255025902 -3.601017151 -5.665073618 -5.320832378\n [56] -3.801870752 -1.843797185  0.540063136  1.577259338  2.719389114\n [61]  2.386440948  0.360417214  0.130240105  1.213682241  0.970840444\n [66]  1.672645132  1.169230978 -0.197824215 -0.552895930  0.483295378\n [71]  2.002207259  2.483139041  4.761206339  7.166338800  7.329547964\n [76]  5.238955522  1.955859515 -1.445155254 -3.624225029 -3.976740747\n [81] -2.522488940 -0.560280191  1.716462129  2.956985039  3.167747954\n [86]  2.655920142  2.503263867  0.243980727 -1.733850533  0.218414375\n [91]  1.212655465  1.188737220 -0.024525903  0.824315000 -0.929797989\n [96] -3.643408960 -4.872924684 -5.365789994 -4.379073769 -0.816292614\n[101]  2.069716217  3.317790830  4.024356559  4.445225438  4.807260941\n[106]  3.077726417  0.597309443 -1.889650709 -3.193428803 -4.189085934\n[111] -5.056410971 -5.113692514 -1.701862879  0.197898712  1.872046685\n[116]  2.519653174  1.995693810  1.375972346  2.342728546  1.412737664\n[121] -1.604693814 -4.224595521 -6.808370201 -8.238970434 -7.267053979\n[126] -5.073807901 -0.614874790  1.926334410  3.620792660  4.301297376\n[131]  2.938794190  2.482699730  2.062144627  0.145550378 -2.263580334\n[136] -3.515516041 -5.626740964 -5.675586843 -5.285219511 -3.877662550\n[141] -2.843191932 -2.159754220 -1.134175851  0.621526810  2.144676177\n[146]  1.301986893  1.090681772  0.483465932 -1.699760373 -0.907358670\n[151] -1.964189610 -2.083464483 -2.401372850 -1.102177741  0.090984198\n[156]  1.539763874  1.675986590  1.340872200 -0.451023892 -1.070116007\n[161] -1.485934032  0.223487236  2.011408533  1.630095949  3.323091734\n[166]  4.997983168  5.156394449  4.241727271  0.336603262 -1.668930450\n[171] -3.056412007 -2.346607210  0.799083342  0.765047225  0.480923824\n[176]  0.301524245  1.422952841  2.820001236  3.981388964  2.988835261\n[181] -0.058956147 -2.066827932 -4.518505369 -5.447774381 -5.746818410\n[186] -5.473607376 -3.515892394  0.432262861  4.283988479  6.685899229\n[191]  6.379550991  5.781828167  5.127569880  2.228597185  1.512254758\n[196]  1.407053783 -0.275040161 -2.623401872 -4.707758722 -6.845203817\n[201] -8.189848947 -8.441072069 -5.100352049 -1.929194703 -0.289395357\n[206]  2.511067946  4.007902754  2.638931037  0.953911823 -0.914044608\n[211] -3.131803887 -4.574239309 -4.239263041 -1.278975512 -1.720543477\n[216] -1.393708189 -0.978071153 -0.052109821 -0.479546542 -1.072444773\n[221] -1.940146902 -2.221618511 -1.892476988 -0.145214604  1.941929437\n[226]  2.662695670  1.548128421  1.266366609 -1.415637008 -2.255649300\n[231] -2.492380384 -1.758574495 -0.272146596  1.472164787  1.788881267\n[236]  0.946614002 -0.426152284  0.059796487 -2.263388225 -4.255693202\n[241] -5.023127496 -5.240398677 -5.705131625 -3.494170488 -0.385992861\n[246]  1.270003055  3.142585019  5.720389808  5.393790259  1.711581565",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#r-example---moving-average",
    "href": "LectureNotes/Lecture2.html#r-example---moving-average",
    "title": "Lecture 2",
    "section": "R example - Moving Average",
    "text": "R example - Moving Average\n\n\n\n\nCode\nset.seed(70)\n\n# generate white noise\nw_t &lt;- rnorm(10, 0, 1)\n\n## manually lag terms\nw_t1 &lt;- c(NA, w_t[1:9])\nw_t2 &lt;- c(NA, NA, w_t[1:8])\n\n## manually compute MA(3)\nv_t &lt;- (w_t + w_t1 + w_t2)/3\n\n## compare the vectors\nma_3 &lt;- cbind(v_t, w_t, w_t1, w_t2)\nround(ma_3, 3)\n\n\n         v_t    w_t   w_t1   w_t2\n [1,]     NA -1.542     NA     NA\n [2,]     NA  0.347 -1.542     NA\n [3,] -0.032  1.099  0.347 -1.542\n [4,]  0.316 -0.499  1.099  0.347\n [5,] -0.112 -0.938 -0.499  1.099\n [6,] -0.523 -0.132 -0.938 -0.499\n [7,] -0.265  0.276 -0.132 -0.938\n [8,] -0.087 -0.405  0.276 -0.132\n [9,] -0.609 -1.696 -0.405  0.276\n[10,] -0.569  0.394 -1.696 -0.405",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#r-example---moving-average-1",
    "href": "LectureNotes/Lecture2.html#r-example---moving-average-1",
    "title": "Lecture 2",
    "section": "R example - Moving Average",
    "text": "R example - Moving Average\n\n\n\n\nCode\n# generate white noise\nn = 50\nw_t &lt;- rnorm(n, 0, 1)\n\n## manually lag terms\nw_t1 &lt;- c(NA, w_t[1:(n-1)])\nw_t2 &lt;- c(NA, NA, w_t[1:(n-2)])\n\n## manually compute MA(3)\nv_t &lt;- (w_t + w_t1 + w_t2)/3\n\n## compare the vectors\nma_3 &lt;- cbind(v_t, w_t, w_t1, w_t2)\nround(ma_3, 3)\n\n\n         v_t    w_t   w_t1   w_t2\n [1,]     NA -0.834     NA     NA\n [2,]     NA  0.799 -0.834     NA\n [3,]  0.043  0.163  0.799 -0.834\n [4,]  0.752  1.292  0.163  0.799\n [5,]  0.491  0.018  1.292  0.163\n [6,]  0.435 -0.006  0.018  1.292\n [7,]  0.163  0.476 -0.006  0.018\n [8,]  0.652  1.486  0.476 -0.006\n [9,]  0.592 -0.186  1.486  0.476\n[10,]  0.778  1.034 -0.186  1.486\n[11,] -0.016 -0.896  1.034 -0.186\n[12,]  0.006 -0.121 -0.896  1.034\n[13,] -0.475 -0.408 -0.121 -0.896\n[14,] -0.170  0.019 -0.408 -0.121\n[15,] -0.164 -0.102  0.019 -0.408\n[16,] -0.727 -2.098 -0.102  0.019\n[17,] -0.798 -0.195 -2.098 -0.102\n[18,] -0.996 -0.697 -0.195 -2.098\n[19,] -0.145  0.457 -0.697 -0.195\n[20,]  0.225  0.914  0.457 -0.697\n[21,]  0.895  1.314  0.914  0.457\n[22,]  0.410 -0.998  1.314  0.914\n[23,] -0.048 -0.459 -0.998  1.314\n[24,] -0.546 -0.181 -0.459 -0.998\n[25,] -0.252 -0.116 -0.181 -0.459\n[26,] -0.105 -0.017 -0.116 -0.181\n[27,] -0.227 -0.547 -0.017 -0.116\n[28,] -0.052  0.408 -0.547 -0.017\n[29,] -0.231 -0.555  0.408 -0.547\n[30,] -0.168 -0.356 -0.555  0.408\n[31,] -0.328 -0.074 -0.356 -0.555\n[32,] -0.608 -1.393 -0.074 -0.356\n[33,] -0.604 -0.345 -1.393 -0.074\n[34,] -1.214 -1.904 -0.345 -1.393\n[35,] -0.917 -0.503 -1.904 -0.345\n[36,] -0.564  0.715 -0.503 -1.904\n[37,]  0.173  0.306  0.715 -0.503\n[38,]  0.572  0.694  0.306  0.715\n[39,]  0.361  0.083  0.694  0.306\n[40,]  0.281  0.065  0.083  0.694\n[41,]  0.308  0.776  0.065  0.083\n[42,]  0.413  0.397  0.776  0.065\n[43,]  0.162 -0.686  0.397  0.776\n[44,] -0.371 -0.824 -0.686  0.397\n[45,] -0.745 -0.725 -0.824 -0.686\n[46,]  0.026  1.627 -0.725 -0.824\n[47,]  0.733  1.298  1.627 -0.725\n[48,]  0.424 -1.653  1.298  1.627\n[49,] -0.927 -2.427 -1.653  1.298\n[50,] -1.123  0.710 -2.427 -1.653",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#motivation",
    "href": "LectureNotes/Lecture2.html#motivation",
    "title": "Lecture 2",
    "section": "Motivation",
    "text": "Motivation\nHow do we summarize characteristics of a distribution?",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#motivation-1",
    "href": "LectureNotes/Lecture2.html#motivation-1",
    "title": "Lecture 2",
    "section": "Motivation",
    "text": "Motivation\nHow do we summarize characteristics of a distribution?\n\nmean\nvariance(standard deviation)",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#how-do-we-summarize-the-characteristics-of-a-distribution-that-changes-over-time",
    "href": "LectureNotes/Lecture2.html#how-do-we-summarize-the-characteristics-of-a-distribution-that-changes-over-time",
    "title": "Lecture 2",
    "section": "How do we summarize the characteristics of a distribution that changes over time?",
    "text": "How do we summarize the characteristics of a distribution that changes over time?\n\nmean function (of time)\n(auto)(co)variance function (of time)",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#mean-function-1",
    "href": "LectureNotes/Lecture2.html#mean-function-1",
    "title": "Lecture 2",
    "section": "Mean Function",
    "text": "Mean Function\nThe mean function of a time series \\(x_t\\) is defined as:\n\\[\n\\mu_{xt} = \\E(x_t) = \\int_{-\\infty}^\\infty x_tf(x_t)dx_t,\n\\]\nwhere \\(\\E\\) is the expected value operator, shown here for the case of a continuous \\(x_t\\).\n\nSo, for example, if \\(x_t\\) is normally distributed then \\(f\\) here would be the normal probability density function (p.d.f.).",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#visual-examples",
    "href": "LectureNotes/Lecture2.html#visual-examples",
    "title": "Lecture 2",
    "section": "Visual examples",
    "text": "Visual examples",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#visual-examples-1",
    "href": "LectureNotes/Lecture2.html#visual-examples-1",
    "title": "Lecture 2",
    "section": "Visual examples",
    "text": "Visual examples",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#notating-the-mean-function",
    "href": "LectureNotes/Lecture2.html#notating-the-mean-function",
    "title": "Lecture 2",
    "section": "Notating the mean function",
    "text": "Notating the mean function\n“When no confusion exists about which time series we are referring to, we will drop a subscript and write \\(\\mu_{xt}\\) as \\(\\mu_t\\).”\nConfusion might exist if we have two time series e.g. if\n\n\\(x_t\\) is the SOI for a given month and\n\\(y_t\\) is the estimated new fish for a given month\n\nwe would have two mean functions, \\(\\mu_{yt}\\) and \\(\\mu_{xt}\\).",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#example-2.2-mean-function-of-a-moving-average-series",
    "href": "LectureNotes/Lecture2.html#example-2.2-mean-function-of-a-moving-average-series",
    "title": "Lecture 2",
    "section": "Example 2.2 Mean Function of a Moving Average Series",
    "text": "Example 2.2 Mean Function of a Moving Average Series\nLet \\(w_t\\) denote a white noise series.\n\nWhat is \\(\\mu_{wt} = \\E(w_t)\\) ?\n\nLet \\(v_t = \\frac{1}{3}(w_{t-1} + w_{t} + w_{t+1})\\) .\n\nWhat is \\(\\mu_{vt} = \\E(v_t)\\)?\n\n(why not just write \\(\\mu_t\\) on this slide?)",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#example-2.3-mean-function-of-a-random-walk-with-drift",
    "href": "LectureNotes/Lecture2.html#example-2.3-mean-function-of-a-random-walk-with-drift",
    "title": "Lecture 2",
    "section": "Example 2.3 Mean Function of a Random Walk with drift",
    "text": "Example 2.3 Mean Function of a Random Walk with drift\nLook, it’s our friend the random walk with drift (maybe):\n\\[\nx_t = \\delta t + \\sum_{j = 1}^t w_j\n\\]\nWhat is the mean function of \\(x_t\\)?",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#break",
    "href": "LectureNotes/Lecture2.html#break",
    "title": "Lecture 2",
    "section": "Break",
    "text": "Break\n\nI saw several turkeys this morning\nI saw someone almost die on a skateboard",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#does-the-mean-function-tell-us-anything-about-the-independence-of-the-time-series",
    "href": "LectureNotes/Lecture2.html#does-the-mean-function-tell-us-anything-about-the-independence-of-the-time-series",
    "title": "Lecture 2",
    "section": "Does the mean function tell us anything about the (in)dependence of the time series?",
    "text": "Does the mean function tell us anything about the (in)dependence of the time series?\nNo (expected value is such a friendly operator!)",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#review-variance-and-covariance",
    "href": "LectureNotes/Lecture2.html#review-variance-and-covariance",
    "title": "Lecture 2",
    "section": "Review: Variance and Covariance",
    "text": "Review: Variance and Covariance\nIf \\(X\\) is a random variable and then \\(\\E(X) = \\mu\\),\n\\[\nVar(X) = \\E((X-\\mu)^2)\n\\] If we have two random variables \\(X_\\alpha\\) and \\(X_\\beta\\), the covariance between these is \\[\nCov(X_\\alpha, X_\\beta) = \\E[(X_\\alpha - \\mu_\\alpha)(X_\\beta - \\mu_\\beta)]\n\\] - remember correlation (scaled covariance)??",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#autocovariance-function",
    "href": "LectureNotes/Lecture2.html#autocovariance-function",
    "title": "Lecture 2",
    "section": "Autocovariance function",
    "text": "Autocovariance function\nThe autocovariance function is defined as the second moment product \\[\n\\gamma_x(s, t) = cov(x_s, x_t) = \\E[(x_s - \\mu_s)(x_t - \\mu_t)]\n\\] for all \\(s\\) and \\(t\\).\n\nWhen no confusion exists, we will drop the \\(x\\) as with the mean function i.e. \\(\\gamma(s, t)\\) instead of \\(\\gamma_x(s, t)\\)\nHow can we write \\(var(x_t)\\) in terms of \\(\\gamma\\)? \\[\n\\gamma_x(t,t) = \\E[(x_t - \\mu_t)^2] = var(x_t)\n\\]",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#example-2.6-autocovariance-of-white-noise",
    "href": "LectureNotes/Lecture2.html#example-2.6-autocovariance-of-white-noise",
    "title": "Lecture 2",
    "section": "Example 2.6 Autocovariance of White Noise",
    "text": "Example 2.6 Autocovariance of White Noise\n\\(w_t\\) ⬅️ white noise series\n\\(\\gamma_w(s, t) = cov(w_s, w_t) = \\text{ }?\\)",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#example-2.6-autocovariance-of-white-noise-1",
    "href": "LectureNotes/Lecture2.html#example-2.6-autocovariance-of-white-noise-1",
    "title": "Lecture 2",
    "section": "Example 2.6 Autocovariance of White Noise",
    "text": "Example 2.6 Autocovariance of White Noise\n\\(w_t\\) ⬅️ white noise series\n\\(\\gamma_w(s, t) = cov(w_s, w_t) =  \\begin{cases} \\sigma^2_w & \\text{ if } s = t\\\\ 0 & \\text{ if } s \\ne t \\end{cases}\\)",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#example-2.8-autocovariance-of-a-moving-average",
    "href": "LectureNotes/Lecture2.html#example-2.8-autocovariance-of-a-moving-average",
    "title": "Lecture 2",
    "section": "Example 2.8 Autocovariance of a Moving Average",
    "text": "Example 2.8 Autocovariance of a Moving Average\nConsider three point moving average \\(v_t = \\frac{1}{3}(w_{t-1} + w_t + w_{t+1})\\)\n\\(\\gamma_v(s, t) = cov(v_s, v_t) =  \\begin{cases}a & \\text{ if } s = t\\\\ b & \\text{ if } \\vert s-t \\vert = 1 \\\\c& \\text{ if } \\vert s-t \\vert =2 \\\\ d & \\text{ if } \\vert s - t\\vert &gt; 2\\end{cases}\\)\n\nWhich one of \\(a, b, c, d\\) is 0\nAre \\(a, b, c\\) the same? If not, which is largest?",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#example-2.8-autocovariance-of-a-moving-average-1",
    "href": "LectureNotes/Lecture2.html#example-2.8-autocovariance-of-a-moving-average-1",
    "title": "Lecture 2",
    "section": "Example 2.8 Autocovariance of a Moving Average",
    "text": "Example 2.8 Autocovariance of a Moving Average\nConsider three point moving average \\(v_t = \\frac{1}{3}(w_{t-1} + w_t + w_{t+1})\\)\n\\(\\gamma_v(s, t) = cov(v_s, v_t) =  \\begin{cases}\\frac{3}{9}\\sigma^2_w & \\text{ if } s = t\\\\ \\frac{2}{9}\\sigma^2_w & \\text{ if } \\vert s-t \\vert = 1 \\\\\\frac{1}{9}\\sigma^2_w & \\text{ if } \\vert s-t \\vert =2 \\\\0 & \\text{ if } \\vert s - t\\vert &gt; 2\\end{cases}\\)\nDoes this equation make intuitive sense?",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#example-2.9-autocovariance-of-a-random-walk",
    "href": "LectureNotes/Lecture2.html#example-2.9-autocovariance-of-a-random-walk",
    "title": "Lecture 2",
    "section": "Example 2.9 Autocovariance of a Random Walk",
    "text": "Example 2.9 Autocovariance of a Random Walk\n\\(x_t = \\sum_{j = 1}^t w_j\\)\n\\[\n\\gamma_x(s, t) = cov(x_s, x_t) = cov\\left ( \\sum_{j=1}^s w_j, \\sum_{k = 1}^t w_k\\right ) = \\min\\{s,t\\}\\sigma^2_w\n\\] If you waaaaant, property 2.7 and figuring out what \\(\\E(w_jw_k)\\) is for \\(j \\ne k\\)",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#r-example---moving-average-2",
    "href": "LectureNotes/Lecture2.html#r-example---moving-average-2",
    "title": "Lecture 2",
    "section": "R example - Moving Average",
    "text": "R example - Moving Average\n\n\nCode\n# generate white noise\nn = 50\nw_t &lt;- rnorm(n, 0, 1)\n\n## manually lag terms\nw_t1 &lt;- c(NA, w_t[1:(n-1)])\nw_t2 &lt;- c(NA, NA, w_t[1:(n-2)])\n\n## manually compute MA(3)\nv_t &lt;- ?\n\n## compare the vectors\nma_3 &lt;- cbind(v_t, w_t, w_t1, w_t2, w_t3)\nround(ma_3, 3)\n\n## also compute MA(3) using stats::filter\nv_t_alt &lt;- ?\n  \n## plot both",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#r-example---moving-average-3",
    "href": "LectureNotes/Lecture2.html#r-example---moving-average-3",
    "title": "Lecture 2",
    "section": "R example - Moving Average",
    "text": "R example - Moving Average\n\n\n\n\nCode\n# generate white noise\nn = 50\nw_t &lt;- rnorm(n, 0, 1)\n\n## manually lag terms\nw_t1 &lt;- c(NA, w_t[1:(n-1)])\nw_t2 &lt;- c(NA, NA, w_t[1:(n-2)])\nw_t3 &lt;- c(NA, NA, NA, w_t[1:(n-3)])\n## manually compute MA(3)\nv_t &lt;- (w_t + w_t1 + w_t2 + w_t3)/4\n\n## compare the vectors\nma_3 &lt;- cbind(v_t, w_t, w_t1, w_t2, w_t3)\nround(ma_3, 3)\n\n\n         v_t    w_t   w_t1   w_t2   w_t3\n [1,]     NA  1.063     NA     NA     NA\n [2,]     NA  0.150  1.063     NA     NA\n [3,]     NA -1.161  0.150  1.063     NA\n [4,] -0.227 -0.958 -1.161  0.150  1.063\n [5,] -0.619 -0.507 -0.958 -1.161  0.150\n [6,] -0.807 -0.600 -0.507 -0.958 -1.161\n [7,] -0.913 -1.587 -0.600 -0.507 -0.958\n [8,] -0.710 -0.144 -1.587 -0.600 -0.507\n [9,] -0.790 -0.827 -0.144 -1.587 -0.600\n[10,] -0.460  0.717 -0.827 -0.144 -1.587\n[11,]  0.041  0.419  0.717 -0.827 -0.144\n[12,]  0.198  0.483  0.419  0.717 -0.827\n[13,]  0.463  0.235  0.483  0.419  0.717\n[14,]  0.128 -0.624  0.235  0.483  0.419\n[15,] -0.186 -0.837 -0.624  0.235  0.483\n[16,]  0.053  1.438 -0.837 -0.624  0.235\n[17,]  0.271  1.107  1.438 -0.837 -0.624\n[18,]  0.512  0.342  1.107  1.438 -0.837\n[19,]  0.468 -1.014  0.342  1.107  1.438\n[20,]  0.577  1.871 -1.014  0.342  1.107\n[21,]  0.100 -0.802  1.871 -1.014  0.342\n[22,] -0.041 -0.221 -0.802  1.871 -1.014\n[23,]  0.402  0.758 -0.221 -0.802  1.871\n[24,]  0.390  1.823  0.758 -0.221 -0.802\n[25,]  0.661  0.282  1.823  0.758 -0.221\n[26,]  0.755  0.157  0.282  1.823  0.758\n[27,]  0.805  0.956  0.157  0.282  1.823\n[28,]  0.850  2.003  0.956  0.157  0.282\n[29,]  0.892  0.453  2.003  0.956  0.157\n[30,]  0.928  0.300  0.453  2.003  0.956\n[31,]  0.822  0.531  0.300  0.453  2.003\n[32,]  0.790  1.877  0.531  0.300  0.453\n[33,]  0.551 -0.503  1.877  0.531  0.300\n[34,]  0.239 -0.950 -0.503  1.877  0.531\n[35,]  0.383  1.110 -0.950 -0.503  1.877\n[36,] -0.425 -1.357  1.110 -0.950 -0.503\n[37,] -0.033  1.063 -1.357  1.110 -0.950\n[38,] -0.151 -1.422  1.063 -1.357  1.110\n[39,] -0.310  0.475 -1.422  1.063 -1.357\n[40,] -0.133 -0.647  0.475 -1.422  1.063\n[41,] -0.338  0.242 -0.647  0.475 -1.422\n[42,] -0.002 -0.077  0.242 -0.647  0.475\n[43,] -0.160 -0.159 -0.077  0.242 -0.647\n[44,] -0.031 -0.129 -0.159 -0.077  0.242\n[45,] -0.437 -1.383 -0.129 -0.159 -0.077\n[46,]  0.237  2.620 -1.383 -0.129 -0.159\n[47,] -0.023 -1.201  2.620 -1.383 -0.129\n[48,]  0.182  0.690 -1.201  2.620 -1.383\n[49,]  0.455 -0.290  0.690 -1.201  2.620\n[50,]  0.288  1.954 -0.290  0.690 -1.201\n\n\nCode\n## also compute using stats::filter\nv_t_alt &lt;- stats::filter(w_t, sides = 2, filter = rep(0.25, times = 4))",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#moving-averages-problem-1.1",
    "href": "LectureNotes/Lecture2.html#moving-averages-problem-1.1",
    "title": "Lecture 2",
    "section": "Moving Averages (Problem 1.1)",
    "text": "Moving Averages (Problem 1.1)\n\nusing a method similar to the code in Example 1.9, generate 100 observations from the autoregression \\[\nx_t = -0.9x_{t-2} + w_t\\text{, }\\\\ w_t\\sim N(0, 1)\n\\]\nWrite down an expression for \\(\\phi\\) for this autoregression. How is \\(\\phi\\) different from the autoregression in Example 1.9?",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#moving-averages-problem-1.1-part-a",
    "href": "LectureNotes/Lecture2.html#moving-averages-problem-1.1-part-a",
    "title": "Lecture 2",
    "section": "Moving Averages (Problem 1.1 Part a)",
    "text": "Moving Averages (Problem 1.1 Part a)\n\nApply the moving average filter to the autoregression data you generated \\[\nv_t = (x_t + x_{t-1} + x_{t-2} + x_{t-4})\n\\]\nPlot \\(x_t\\) as points and lines and \\(v_t\\) as a line.",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#moving-averages-problem-1.1-1",
    "href": "LectureNotes/Lecture2.html#moving-averages-problem-1.1-1",
    "title": "Lecture 2",
    "section": "Moving Averages (Problem 1.1)",
    "text": "Moving Averages (Problem 1.1)\n\n\nCode\nlibrary(astsa)\nw = rnorm(150,0,1) # 50 extra to avoid startup problems\nxa = filter(w, filter=c(0,-.9), method=\"recursive\")[-(1:50)] # AR\nva = filter(xa, rep(1,4)/4, sides=1) # moving average\ntsplot(xa, main=\"autoregression\", type = \"b\", pch = 16)\nlines(va, col=\"blueviolet\", lwd = 2)",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#moving-averages-problem-1.1-part-b",
    "href": "LectureNotes/Lecture2.html#moving-averages-problem-1.1-part-b",
    "title": "Lecture 2",
    "section": "Moving Averages (Problem 1.1 part b)",
    "text": "Moving Averages (Problem 1.1 part b)\n\nRepeat the application of the MA filter but instead of starting with an autoregression, generate data \\(x_t\\) according to the signal plus noise model \\[\nx_t = 2\\cos(2\\pi t/4) + w_t\\\\ w_t \\sim N(0,1)\n\\]",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#moving-averages-problem-1.1-part-b-1",
    "href": "LectureNotes/Lecture2.html#moving-averages-problem-1.1-part-b-1",
    "title": "Lecture 2",
    "section": "Moving Averages (Problem 1.1 part b)",
    "text": "Moving Averages (Problem 1.1 part b)\n\n\nCode\nxb = 2*cos(2*pi*(1:100)/4) + rnorm(100,0,1) # sinusoid + noise\nvb = filter(xb, rep(1,4)/4, sides=1) # moving average\ntsplot(xb, main=\"sinusoid + noise\", type = \"b\", pch = 16)\nlines(vb, col=\"blueviolet\", lwd = 2)",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#moving-averages-problem-1.1-part-c",
    "href": "LectureNotes/Lecture2.html#moving-averages-problem-1.1-part-c",
    "title": "Lecture 2",
    "section": "Moving Averages (Problem 1.1 part c)",
    "text": "Moving Averages (Problem 1.1 part c)\n\nRepeat the application of the MA filter but instead of starting with an autoregression, use the Johnson and Johnson data from Lecture 1.",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#moving-averages-problem-1.1-part-c-1",
    "href": "LectureNotes/Lecture2.html#moving-averages-problem-1.1-part-c-1",
    "title": "Lecture 2",
    "section": "Moving Averages (Problem 1.1 part c)",
    "text": "Moving Averages (Problem 1.1 part c)\n\n\nCode\nxc = log(jj)\nvc = stats::filter(xc, filter = rep(1,4)/4, sides=1, method = \"convolution\") # moving average\ntsplot(xc, main=\"johnson and johnson (log scale)\", type = \"b\", pch = 16)\nlines(vc, col=\"blueviolet\", lwd = 2)",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#stationarity",
    "href": "LectureNotes/Lecture2.html#stationarity",
    "title": "Lecture 2",
    "section": "Stationarity",
    "text": "Stationarity\nA time series is stationary if\n\nthe mean function (\\(\\mu_t\\)) is constant and does not depend on time \\(t\\)\nthe autocovariance function (\\(\\gamma(s,t)\\)) depends on \\(s\\) and \\(t\\) only though their difference",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture2.html#example-2.14-stationarity-of-a-random-walk",
    "href": "LectureNotes/Lecture2.html#example-2.14-stationarity-of-a-random-walk",
    "title": "Lecture 2",
    "section": "Example 2.14 Stationarity of a Random Walk",
    "text": "Example 2.14 Stationarity of a Random Walk\nLook, it’s our friend the random walk:\n\\[\nx_t = \\delta t + \\sum_{j = 1}^t w_j\n\\] Their mean function is \\(\\E(x_t) = 0\\), and their covariance function is \\(\\gamma_x(s, t) = \\min\\{s,t\\}\\sigma^2_w\\)\nIs a random walk stationary?",
    "crumbs": [
      "Week 1",
      "Lecture 2"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html",
    "href": "LectureNotes/Lecture4.html",
    "title": "Lecture 4",
    "section": "",
    "text": "Slides with Activity Solutions\n\n\n\n\n\nView slides in full screen",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#recap",
    "href": "LectureNotes/Lecture4.html#recap",
    "title": "Lecture 4",
    "section": "Recap",
    "text": "Recap\n\n\\[\n\\newcommand\\E{{\\mathbb{E}}}\n\\]\n\n\nDecomposing time series\nStationarity (theoretically and with data)\nSome activities",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#today",
    "href": "LectureNotes/Lecture4.html#today",
    "title": "Lecture 4",
    "section": "Today",
    "text": "Today\n\nFinish up activities from Lecture 3\nTrend stationarity\nVisualizing autocovariance (third attempt)\nDetrending\n“Office hours”",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#reminders",
    "href": "LectureNotes/Lecture4.html#reminders",
    "title": "Lecture 4",
    "section": "Reminders",
    "text": "Reminders\n\nSyllabus participation policy\nAssignments: Going forward, must submit rendered pdf of code portion. (if you want to be nice to me, do this for Assignment 2, but starts with Assignment 3)\nLate quizzes: Going forward: Email me ahead of time, otherwise it’s a 0\nExam details: No use of computer, code will be covered but basic, notes sheet is allowed, practice test will be provided",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#activity-1-export-price-of-salmon-example-3.1",
    "href": "LectureNotes/Lecture4.html#activity-1-export-price-of-salmon-example-3.1",
    "title": "Lecture 4",
    "section": "Activity 1: Export Price of Salmon (Example 3.1)",
    "text": "Activity 1: Export Price of Salmon (Example 3.1)\n\n\nCode\nlibrary(astsa)\n\nsummary(fit &lt;- lm(salmon~time(salmon), na.action=NULL))\n## \n## Call:\n## lm(formula = salmon ~ time(salmon), na.action = NULL)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -1.69187 -0.62453 -0.07024  0.51561  2.34959 \n## \n## Coefficients:\n##                Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  -503.08947   34.44164  -14.61   &lt;2e-16 ***\n## time(salmon)    0.25290    0.01713   14.76   &lt;2e-16 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.8814 on 164 degrees of freedom\n## Multiple R-squared:  0.5706, Adjusted R-squared:  0.568 \n## F-statistic: 217.9 on 1 and 164 DF,  p-value: &lt; 2.2e-16\ntsplot(salmon, col=4, ylab=\"USD per KG\", main=\"Salmon Export Price\")\nabline(fit)",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#activity-1-export-price-of-salmon-example-3.1-1",
    "href": "LectureNotes/Lecture4.html#activity-1-export-price-of-salmon-example-3.1-1",
    "title": "Lecture 4",
    "section": "Activity 1: Export Price of Salmon (Example 3.1)",
    "text": "Activity 1: Export Price of Salmon (Example 3.1)\n\nDoes this time series appear stationary?\nThe (mathematical) equation in the book for the trend line above is:\n\\[\nx_t = \\beta_0 + \\beta_1z_t + w_t, z_t = 2003\\frac{8}{12}, 2001\\frac{8}{12}, \\dots, 2017\\frac{5}{12}\n\\]\n\nThere is a typo in this equation. Correct the typo. (hint: examine to the first few entries of time(salmon)\nWhy are there fractions of the year? Explain what the fractional values mean and describe how they appear in the data set within R.\n\nInterpret the estimate of the slope.",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#activity-2-trend-stationarity-example-2.19",
    "href": "LectureNotes/Lecture4.html#activity-2-trend-stationarity-example-2.19",
    "title": "Lecture 4",
    "section": "Activity 2: Trend Stationarity (Example 2.19)",
    "text": "Activity 2: Trend Stationarity (Example 2.19)\nConsider the time series model \\[x_t = \\beta t + y_t\\] Assume \\(y_t\\) is stationary with mean function \\(\\mu_y\\) and and autocovariance function \\(\\gamma_y(h)\\)\n\nCompare this equation to the regression equation in the last example.\nWhat are the mean function and autocovariance function of \\(x_t\\)?",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#trend-stationarity-model",
    "href": "LectureNotes/Lecture4.html#trend-stationarity-model",
    "title": "Lecture 4",
    "section": "Trend stationarity model",
    "text": "Trend stationarity model\nA time series which is nonstationary in the mean but is stationary in the autocovariance is sometimes called Trend stationarity.\n\nI’m actually not sure if it just refers to linear trends?? I’ll ask people at my conference",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#visualizing-the-autocovariance-for-trend-stationarity",
    "href": "LectureNotes/Lecture4.html#visualizing-the-autocovariance-for-trend-stationarity",
    "title": "Lecture 4",
    "section": "Visualizing the autocovariance for trend stationarity",
    "text": "Visualizing the autocovariance for trend stationarity\n\n\nCode\nlibrary(ggplot2)\nset.seed(807)\nt &lt;- seq(1, 10, 1)\nx_t &lt;- 0.5*t \n#x &lt;- x - mean(x)\n#y &lt;- y - mean(y)\n\ndf &lt;- data.frame(t, x_t)\n\n# For every row in `df`, compute a rotated normal density centered at `y` and shifted by `x`\ncurves &lt;- lapply(seq_len(NROW(df)), function(i) {\n  mu &lt;- df$x_t[i]\n  range &lt;- mu + c(-1.5, 1.5)\n  seq &lt;- seq(range[1], range[2], length.out = 100)\n  data.frame(\n    t = -1 * dnorm(seq, mean = mu, sd = 0.5) + df$t[i],\n    x_t = seq,\n    grp = i\n  )\n})\n# Combine above densities in one data.frame\ncurves &lt;- do.call(rbind, curves)\n\nnew.x = seq(from = 1, to = 10, by = .1)\nnew.y = .5*new.x\ntrend_line &lt;- data.frame(x = new.x,\n                       y = new.y)\nggplot(df, aes(t, x_t)) +\n  geom_point(col = \"blueviolet\", pch = 17) +\n  #geom_line() +\n  # The path draws the curve\n  geom_path(data = curves, aes(group = grp)) +\n  geom_line(data = trend_line, aes(x=x,y=y), col = \"blueviolet\") +\n  lims(y = c(-2,10)) +\n  scale_x_continuous(breaks = seq(1, 10, by = 1)) +\n  theme_minimal() + \n  theme( # remove the vertical grid lines\n           panel.grid = element_blank() ,\n           # explicitly set the horizontal lines (or they will disappear too)\n           panel.grid.major.x = element_line( size=.1, color=\"black\" )) +   \n  geom_rect(aes(xmin = 1.1, xmax = 2.1, ymin = -1, ymax = 3), fill = NA, col = \"blue\")+   \n  geom_rect(aes(xmin = 7.1, xmax = 8.1, ymin = 2, ymax = 6), fill = NA, col = \"magenta\")\n\n\nWarning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\n\nCode\n  # The polygon does the shading. We can use `oob_squish()` to set a range.\n  #geom_polygon(data = curves, aes(y = scales::oob_squish(y, c(0, Inf)),group = grp))",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#simulate-many-time-series-from-the-trend-stationarity-model",
    "href": "LectureNotes/Lecture4.html#simulate-many-time-series-from-the-trend-stationarity-model",
    "title": "Lecture 4",
    "section": "Simulate many time series from the trend stationarity model",
    "text": "Simulate many time series from the trend stationarity model\n\n\nCode\nset.seed(807)\nTime &lt;- 10\nn_sim &lt;- 100\nall_series = matrix(rep(NA, times = n_sim*Time), nrow = Time)\nfor(i in 1:n_sim){\n  cs = 0.25*1:Time        # same thing \n  w  = rnorm(Time + 50,0,1)\n  #w_dep = stats::filter(w, filter = rep(1/3,3))[2:(Time + 1)]\n  #all_series[,i] &lt;- cs + w_dep\n  all_series[,i] &lt;- cs + w[2:(Time+1)]\n  #names(all_series)[i] &lt;- paste(\"sim_\", i)\n  \n}\nfit &lt;- lm(all_series[,1]~time(all_series[,1]), na.action=NULL)\ntsplot(all_series[,1], main = \"One simulated Time series\", type = \"b\", col = c(\"black\", \"blue\", rep(\"black\", times = 5), \"magenta\", \"black\", \"black\"), pch = 16, cex = 3)\nabline(fit)",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#simulate-many-time-series-from-the-trend-stationarity-model-1",
    "href": "LectureNotes/Lecture4.html#simulate-many-time-series-from-the-trend-stationarity-model-1",
    "title": "Lecture 4",
    "section": "Simulate many time series from the trend stationarity model",
    "text": "Simulate many time series from the trend stationarity model\n\n\nCode\npar(mfrow = c(2,2))\nfor(i in 1:4){\n  fit &lt;- lm(all_series[,i]~time(all_series[,i]), na.action=NULL)\n  tsplot(all_series[,i], type = \"b\", col = c(\"black\", \"blue\", rep(\"black\", times = 5), \"magenta\", \"black\", \"black\"), pch = 16, cex = 3, ylim = c(-3,6))\n  abline(fit)\n\n}",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#simulate-many-time-series-from-the-trend-stationarity-model-2",
    "href": "LectureNotes/Lecture4.html#simulate-many-time-series-from-the-trend-stationarity-model-2",
    "title": "Lecture 4",
    "section": "Simulate many time series from the trend stationarity model",
    "text": "Simulate many time series from the trend stationarity model\n\n\nCode\ntsplot(all_series, spaghetti = TRUE, main = \"100 Simuated Trend Stationary Time Series\", type = \"b\")\nrect(xleft = 1.5, xright = 2.5, ybottom = -2, ytop = 4, border = \"blue\", lwd = 2)\nrect(xleft = 7.5, xright = 8.5, ybottom = -.5, ytop = 5, border = \"magenta\", lwd = 2)",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#marginal-and-joint-distributions-t8-and-s2",
    "href": "LectureNotes/Lecture4.html#marginal-and-joint-distributions-t8-and-s2",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions t=8 and s=2",
    "text": "Marginal and Joint Distributions t=8 and s=2\n\n\nCode\ni = 2\nx_comp &lt;- all_series[i,]\nx_8 &lt;- all_series[8,]\n\npar(mfrow = c(2,2))\nhist(x_comp, col = \"blue\", main = paste(\"Histogram of simulations at t=\", i), xlab = paste(\"x_\",i))\nhist(x_8, col = \"magenta\", main = paste(\"Histogram of simulations at t=8\"), xlab = \"x_8\")\nplot(x_comp, x_8, col = \"purple\", pch = 16, main = \"Joint Distribution\")\nabline(lm(x_8~x_comp))",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#marginal-and-joint-distributions",
    "href": "LectureNotes/Lecture4.html#marginal-and-joint-distributions",
    "title": "Lecture 4",
    "section": "Marginal and Joint Distributions",
    "text": "Marginal and Joint Distributions\n\n\nCode\nlibrary(ggplot2)\nlibrary(ggExtra)\nx &lt;- data.frame(x_comp, x_8)\n# Save the scatter plot in a variable\np &lt;- ggplot(x, aes(x = x_comp, y = x_8)) +\n  geom_point(col = \"purple\") + xlim(-3,6) + ylim(-3,6)+ \n  geom_text(aes(x = 2, y = -1, label = paste(\"rho(\",i,\",8) = \\n cor(x_\",i,\", x_8) = \", round(cor(x_comp, x_8),3)), size = 6)) + coord_fixed() \n\n# Arguments for each marginal histogram\nggMarginal(p, type = \"density\", adjust = 2,\n           xparams = list(col = \"blue\", fill = \"blue\"),\n           yparams = list(col = \"magenta\", fill = \"magenta\"))\n\n\nWarning in geom_text(aes(x = 2, y = -1, label = paste(\"rho(\", i, \",8) = \\n cor(x_\", : All aesthetics have length 1, but the data has 100 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\nAll aesthetics have length 1, but the data has 100 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#do-this-for-all-possible-combos-of-s-and-t",
    "href": "LectureNotes/Lecture4.html#do-this-for-all-possible-combos-of-s-and-t",
    "title": "Lecture 4",
    "section": "Do this for all possible combos of \\(s\\) and \\(t\\)",
    "text": "Do this for all possible combos of \\(s\\) and \\(t\\)\nView slides in full screen",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#visualizing-the-correlations",
    "href": "LectureNotes/Lecture4.html#visualizing-the-correlations",
    "title": "Lecture 4",
    "section": "Visualizing the correlations",
    "text": "Visualizing the correlations\n\n\nCode\ncors &lt;- apply(all_series, 1, function(x){cor(x, all_series[8,])})\nhist(cors, breaks = seq(-1, 1, by = .1), main = \"Correlation between simulations for x_8 and x_i\")",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#are-these-all-the-correlations",
    "href": "LectureNotes/Lecture4.html#are-these-all-the-correlations",
    "title": "Lecture 4",
    "section": "Are these all the correlations?",
    "text": "Are these all the correlations?\nNo, just pairwise with \\(x_8\\). We could do all possible pairs:\n\n\nCode\nlibrary(GGally)\n\n\nRegistered S3 method overwritten by 'GGally':\n  method from   \n  +.gg   ggplot2\n\n\nCode\nggpairs(data.frame(t(all_series)))",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#distribution-of-all-the-correlations",
    "href": "LectureNotes/Lecture4.html#distribution-of-all-the-correlations",
    "title": "Lecture 4",
    "section": "Distribution of all the correlations:",
    "text": "Distribution of all the correlations:\n\n\nCode\ncoords &lt;- t(combn(10,2))\ncors &lt;- NULL\nfor(i in 1:nrow(coords)){\n  cors &lt;- c(cors, cor(all_series[coords[i,1],], all_series[coords[i,2],]))\n}\ncors &lt;- c(cors, rep(1, times = 10))\nhist(cors, breaks = seq(-1, 1, by = .1), main = \"Correlation between simulations for x_s and x_t\")",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#d-version-of-histogram-includes-s-t-plane",
    "href": "LectureNotes/Lecture4.html#d-version-of-histogram-includes-s-t-plane",
    "title": "Lecture 4",
    "section": "3D version of histogram (includes \\(s-t\\) plane)",
    "text": "3D version of histogram (includes \\(s-t\\) plane)\n\n\nCode\ncoords &lt;- expand.grid(1:Time, 1:Time)\nnames(coords) &lt;- c(\"s\", \"t\") \ncoords$cor &lt;- NA\ncoords$pval &lt;- NA\ncor_mat_theoretical &lt;- cor_mat &lt;- matrix(rep(NA, times = Time*Time), nrow = Time)\nfor(i in 1:nrow(coords)){\n  out &lt;- cor.test((all_series[coords[i,1],]), (all_series[coords[i,2],]))\n  if(coords[i,1]==coords[i,2]){\n    coords$cor[i] &lt;- 1\n    coords$pval[i] &lt;- 0\n    cor_mat[coords[i,1], coords[i,2]] &lt;- 1\n    cor_mat_theoretical[coords[i,1], coords[i,2]] &lt;- 1\n  }else{\n    coords$cor[i] &lt;- out$estimate\n    coords$pval[i] &lt;- out$p.value\n    cor_mat[coords[i,1], coords[i,2]] &lt;- out$estimate\n    cor_mat_theoretical[coords[i,1], coords[i,2]] &lt;- 0\n    \n  }\n}\n\nlibrary(plotly)\n\n\n\nAttaching package: 'plotly'\n\n\nThe following object is masked from 'package:ggplot2':\n\n    last_plot\n\n\nThe following object is masked from 'package:stats':\n\n    filter\n\n\nThe following object is masked from 'package:graphics':\n\n    layout\n\n\nCode\nplot_ly(coords,\n        x= ~s, y=~t, z=~cor, \n        type = 'scatter3d', mode = \"markers\", size = .1)",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#simulated-auto-correlation-function-hatgamma_xh-with-a-blanket",
    "href": "LectureNotes/Lecture4.html#simulated-auto-correlation-function-hatgamma_xh-with-a-blanket",
    "title": "Lecture 4",
    "section": "Simulated auto correlation function \\(\\hat{\\gamma}_x(h)\\) (with a blanket)",
    "text": "Simulated auto correlation function \\(\\hat{\\gamma}_x(h)\\) (with a blanket)\n\n\nCode\n## \"drape a blanket over it to see the pattern better\"\nfig &lt;- plot_ly(z = ~cor_mat)\nfig %&gt;% add_surface()",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#theoretical-hatgamma_xh-using-the-derived-formula-with-a-blanket",
    "href": "LectureNotes/Lecture4.html#theoretical-hatgamma_xh-using-the-derived-formula-with-a-blanket",
    "title": "Lecture 4",
    "section": "Theoretical \\(\\hat{\\gamma}_x(h)\\) (using the derived formula (with a blanket)",
    "text": "Theoretical \\(\\hat{\\gamma}_x(h)\\) (using the derived formula (with a blanket)\n\n\nCode\n# Theoretical function\nfig &lt;- plot_ly(z = ~cor_mat_theoretical)\nfig %&gt;% add_surface()",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#detrending-1",
    "href": "LectureNotes/Lecture4.html#detrending-1",
    "title": "Lecture 4",
    "section": "Detrending",
    "text": "Detrending\nIf a process is trend stationary (nonstationary in the mean, but stationary in the variance), can we just subtract off the trend and get back a stationary time series?\nYes, and that’s called detrending",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#activity-3-detrending-a-commodity-example-3.7",
    "href": "LectureNotes/Lecture4.html#activity-3-detrending-a-commodity-example-3.7",
    "title": "Lecture 4",
    "section": "Activity 3: Detrending a commodity (Example 3.7)",
    "text": "Activity 3: Detrending a commodity (Example 3.7)\n\nGiven the code to generate the plot with the trend line, how would you view the equation of the trend line?\nVisualize the de-trended series\nCompute the acf of the salmon series and the detrended series. What do you notice?",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture4.html#next-time",
    "href": "LectureNotes/Lecture4.html#next-time",
    "title": "Lecture 4",
    "section": "Next time",
    "text": "Next time\n\nCross-correlation and regression with multiple time series (\\(x_t\\) on x-axis instead of \\(t\\) on x-axis like with the salmon)\nActivities and examples\nSmoothing",
    "crumbs": [
      "Week 2",
      "Lecture 4"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html",
    "href": "LectureNotes/Lecture6.html",
    "title": "Lecture 6",
    "section": "",
    "text": "\\[\n\\newcommand\\E{{\\mathbb{E}}}\n\\]\n\nWe’ve been estimating trends and getting confused about stationarity :)",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#recap",
    "href": "LectureNotes/Lecture6.html#recap",
    "title": "Lecture 6",
    "section": "",
    "text": "\\[\n\\newcommand\\E{{\\mathbb{E}}}\n\\]\n\nWe’ve been estimating trends and getting confused about stationarity :)",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#a-look-ahead",
    "href": "LectureNotes/Lecture6.html#a-look-ahead",
    "title": "Lecture 6",
    "section": "A look ahead",
    "text": "A look ahead\nNext few weeks:\n\nAssignment 3 posted soon, Due Friday, October 18, we will review in class on Monday prior to midterm\nNext Monday: remote class: activity to start your “cheat sheet” and a data analysis challenge\nSometime next week: Practice midterm posted",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#where-is-the-template-file",
    "href": "LectureNotes/Lecture6.html#where-is-the-template-file",
    "title": "Lecture 6",
    "section": "Where is the template file?",
    "text": "Where is the template file?\n\nWe are going to create it, and you will submit a rendered pdf of today’s lecture as your “participation”.",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#getting-organized-with-quarto",
    "href": "LectureNotes/Lecture6.html#getting-organized-with-quarto",
    "title": "Lecture 6",
    "section": "Getting organized with Quarto",
    "text": "Getting organized with Quarto\nCreate a folder somewhere on your computer called “Lecture6”.\nOpen R studio, and create a .qmd File and save inside the “Lecture6” folder as “Yourname_Lecture6.qmd”.\nCreate a header for Activity 1.",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#today",
    "href": "LectureNotes/Lecture6.html#today",
    "title": "Lecture 6",
    "section": "Today",
    "text": "Today\n\nReview: Stationarity, White Noise, Autocovariance, and “temporal structure”\nSmoothing",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#activity-1-in-groups",
    "href": "LectureNotes/Lecture6.html#activity-1-in-groups",
    "title": "Lecture 6",
    "section": "Activity 1: In groups:",
    "text": "Activity 1: In groups:\nChoose two properties from:\n\nStationarity\nWhite Noise\nAutocovariance, and\n“temporal structure”\n\nDefine the properties in symbols and words and describe why they are different. Make sure to put this in your quarto file!\nWhen you’re done, share with another group. Put that group’s answer under another header.",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#go-around-and-share-as-a-class.",
    "href": "LectureNotes/Lecture6.html#go-around-and-share-as-a-class.",
    "title": "Lecture 6",
    "section": "Go around and share as a class.",
    "text": "Go around and share as a class.\n\nAdd one or two observations from other groups",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#data-set-focus-soi",
    "href": "LectureNotes/Lecture6.html#data-set-focus-soi",
    "title": "Lecture 6",
    "section": "Data set focus: SOI",
    "text": "Data set focus: SOI\nCode Source\nMore about SOI on Climate.gov\n\n\nCode\nlibrary(astsa)\ntsplot(soi, ylab=\"\", xlab=\"\", main=\"Southern Oscillation Index\", col=4)\ntext(1970, .91, \"COOL\", col=5)\ntext(1970,-.91, \"WARM\", col=6)",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#activity-2-moving-average-smoother-example-3.16",
    "href": "LectureNotes/Lecture6.html#activity-2-moving-average-smoother-example-3.16",
    "title": "Lecture 6",
    "section": "Activity 2: Moving average smoother (Example 3.16)",
    "text": "Activity 2: Moving average smoother (Example 3.16)\nCode source\n\n\nCode\nlibrary(astsa)\n#library(tidyverse)\n## \nw = c(.5, rep(1,11), .5)/12\nsoif = filter(soi, sides=2, filter=w)\ntsplot(soi, col=astsa.col(4,.7), ylim=c(-1, 1.15))\nlines(soif, lwd=2, col=4)\n# insert\npar(fig = c(.65, 1, .75, 1), new = TRUE)\nw1 = c(rep(0,20), w, rep(0,20))\nplot(w1, type=\"l\", ylim = c(-.02,.1), xaxt=\"n\", yaxt=\"n\", ann=FALSE)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1,1))\n\n## Detrend\n\n## Plot detrended\n\n## acfs\n\n\n\nThe code gives an error. Can you fix it?\nDoes the moving average smoother appear to be doing a good job of capturing the trend?\nDetrend the soi series with respect to the moving average smoother.\nPlot the detrended soi series\nCompute the acf of the soi series and detrended series. What do you notice?",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#activity-2-solution",
    "href": "LectureNotes/Lecture6.html#activity-2-solution",
    "title": "Lecture 6",
    "section": "Activity 2 Solution",
    "text": "Activity 2 Solution\n\n\nCode\nlibrary(astsa)\nlibrary(tidyverse)\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nCode\n## \nw = c(.5, rep(1,11), .5)/12\nsoif = stats::filter(soi, sides=2, filter=w)\ntsplot(soi, col=astsa.col(4,.7), ylim=c(-1, 1.15))\nlines(soif, lwd=2, col=4)\npar(fig = c(.65, 1, .75, 1), new = TRUE)\nw1 = c(rep(0,20), w, rep(0,20))\nplot(w1, type=\"l\", ylim = c(-.02,.1), xaxt=\"n\", yaxt=\"n\", ann=FALSE)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1,1))\n\n## Detrend\ndetrended &lt;- soi-soif\n\n## Plot detrended\ntsplot(detrended)\n\n\n\n\n\n\n\n\n\nCode\n## acfs\nacf1(soif, na.action = na.pass)\n\n\n\n\n\n\n\n\n\n [1]  0.99  0.95  0.91  0.85  0.78  0.70  0.61  0.53  0.44  0.36  0.28  0.20\n[13]  0.14  0.08  0.04  0.00 -0.03 -0.05 -0.07 -0.08 -0.09 -0.10 -0.10 -0.10\n[25] -0.09 -0.08 -0.07 -0.06 -0.05 -0.04 -0.03 -0.01  0.00  0.02  0.04  0.06\n[37]  0.08  0.10  0.11  0.13  0.15  0.17  0.19  0.20  0.21  0.22  0.23  0.24\n\n\nCode\npar(mfrow = c(2,1))\nacf1(soi)\n\n\n [1]  0.60  0.37  0.21  0.05 -0.11 -0.19 -0.18 -0.10  0.05  0.22  0.36  0.41\n[13]  0.31  0.10 -0.06 -0.17 -0.29 -0.37 -0.32 -0.19 -0.04  0.15  0.31  0.35\n[25]  0.25  0.10 -0.03 -0.16 -0.28 -0.37 -0.32 -0.16 -0.02  0.17  0.33  0.39\n[37]  0.30  0.16  0.00 -0.13 -0.24 -0.27 -0.25 -0.13  0.06  0.21  0.38  0.40\n\n\nCode\nacf1(detrended, na.action = na.pass)\n\n\n\n\n\n\n\n\n\n [1]  0.45  0.15 -0.05 -0.27 -0.47 -0.51 -0.42 -0.28 -0.04  0.24  0.47  0.55\n[13]  0.43  0.15 -0.06 -0.22 -0.39 -0.49 -0.41 -0.23 -0.01  0.24  0.46  0.51\n[25]  0.39  0.17 -0.02 -0.20 -0.38 -0.51 -0.43 -0.23 -0.03  0.22  0.46  0.53\n[37]  0.40  0.19 -0.04 -0.22 -0.39 -0.43 -0.41 -0.25  0.00  0.21  0.44  0.48\n\n\n\nThe code gives an error. Can you fix it? We need to specify we want stats::filter not dplyr::filter\nDescribe the trend estimated by the moving average smoother. The moving average smoother smooths out the annual cycles (the shorter oscillations in the observed series) and emphasizes a longer range cycle– this is the El Niño pattern.\nDetrend the soi series with respect to the moving average smoother. Here, we just subtract the estimated trend from the observed series. Note that there will be missing values on the ends, but that’s ok.\nPlot the detrended soi series\nPlot the acf of the soi series, the moving average trend estimate, and detrended series. What do you notice?\n\nThe soi and detrended soi have similar acfs that both appear to reveal the annual cycle, though the magnitude of the correlations are different. The moving average smoother shows the longer term oscillation. Each of these series has temporal structure (does not look like white noise).",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#what-is-a-moving-average-smoother",
    "href": "LectureNotes/Lecture6.html#what-is-a-moving-average-smoother",
    "title": "Lecture 6",
    "section": "What is a moving average smoother?",
    "text": "What is a moving average smoother?\n\\[\nm_t = \\sum_{j = -k}^k a_j x_{t-j}\n\\]\nWhere \\(x_t\\) is any time series and the coefficients add to 1. How is this different from the “moving average model”?\n\nIn the model we’ve seen so far, we assume \\(x_t\\) is white noise and that the weights are equal\n\nWhat were the weights in the last example? How do you know? soif = stats::filter(soi, sides=2, filter=w) is portion of the code that computes the moving average. The weights are in the filter argument, so we need to look at what w is:\n\n\nCode\nw\n\n\n [1] 0.04166667 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333\n [7] 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333 0.08333333\n[13] 0.04166667\n\n\nCode\nplot(-6:6, w, ylim = c(0, .1), ylab = \"Weight\", xlab = \"distance from point we want the moving average estimate for\")",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#activity-3-kernel-smoothing-example-3.17",
    "href": "LectureNotes/Lecture6.html#activity-3-kernel-smoothing-example-3.17",
    "title": "Lecture 6",
    "section": "Activity 3: Kernel smoothing (Example 3.17)",
    "text": "Activity 3: Kernel smoothing (Example 3.17)\nCode source\n\n\nCode\ntsplot(soi, col=4)\nlines(ksmooth(time(soi), soi, \"normal\", bandwidth=1), lwd=2, col=6)\n\n\n\n\n\n\n\n\n\nCode\n## detrend\n\n## plot detrended\n\n## acfs\n\n\n\nDescribe the trend captured by the kernel smoother.\nChange the bandwith to 2 and re-plot the kernel smoother and soi series. Repeat with a bandwidth of 0.5. What do you think the bandwidth parameter does?\nDetrend the soi series with respect to the kernel smoother.\nPlot the detrended soi series\nPlot the acf of the soi series, the kernel trend estimate, and detrended series. What do you notice?",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#activity-3-solution",
    "href": "LectureNotes/Lecture6.html#activity-3-solution",
    "title": "Lecture 6",
    "section": "Activity 3 Solution",
    "text": "Activity 3 Solution\nCode source\n\n\nCode\ntsplot(soi, col=4, ylim = c(-1, 1.15))\nlines(ksmooth(time(soi), soi, \"normal\", bandwidth=1), lwd=2, col=6)\n\n\n\n\n\n\n\n\n\nCode\n## change bandwidth\ntsplot(soi, col=4, ylim = c(-1, 1.15))\nlines(ksmooth(time(soi), soi, \"normal\", bandwidth=2), lwd=2, col=6)\n\n\n\n\n\n\n\n\n\nCode\ntsplot(soi, col=4, ylim = c(-1, 1.15))\nlines(ksmooth(time(soi), soi, \"normal\", bandwidth=0.5), lwd=2, col=6)\n\n\n\n\n\n\n\n\n\nCode\n## Detrend\nsoi_ksmooth &lt;- ksmooth(time(soi), soi, \"normal\", bandwidth=1)\ndetrended &lt;- soi-soi_ksmooth$y\n\n## Plot detrended\ntsplot(detrended)\n\n\n\n\n\n\n\n\n\nCode\n## acfs\nacf1(soi_ksmooth$y, na.action = na.pass)\n\n\n\n\n\n\n\n\n\n [1]  0.99  0.96  0.92  0.86  0.80  0.73  0.65  0.58  0.50  0.43  0.35  0.29\n[13]  0.22  0.16  0.11  0.06  0.02 -0.01 -0.03 -0.05 -0.06 -0.06 -0.06 -0.06\n[25] -0.06 -0.06 -0.06 -0.05 -0.05 -0.04 -0.02  0.00\n\n\nCode\npar(mfrow = c(2,1))\nacf1(soi)\n\n\n [1]  0.60  0.37  0.21  0.05 -0.11 -0.19 -0.18 -0.10  0.05  0.22  0.36  0.41\n[13]  0.31  0.10 -0.06 -0.17 -0.29 -0.37 -0.32 -0.19 -0.04  0.15  0.31  0.35\n[25]  0.25  0.10 -0.03 -0.16 -0.28 -0.37 -0.32 -0.16 -0.02  0.17  0.33  0.39\n[37]  0.30  0.16  0.00 -0.13 -0.24 -0.27 -0.25 -0.13  0.06  0.21  0.38  0.40\n\n\nCode\nacf1(detrended, na.action = na.pass)\n\n\n\n\n\n\n\n\n\n [1]  0.42  0.12 -0.05 -0.23 -0.40 -0.46 -0.41 -0.28 -0.05  0.21  0.42  0.51\n[13]  0.39  0.13 -0.07 -0.20 -0.35 -0.45 -0.38 -0.22 -0.02  0.22  0.44  0.49\n[25]  0.36  0.14 -0.02 -0.18 -0.34 -0.47 -0.41 -0.21 -0.03  0.21  0.43  0.49\n[37]  0.37  0.17 -0.04 -0.21 -0.36 -0.40 -0.39 -0.24  0.01  0.20  0.42  0.45\n\n\n\nDescribe the trend captured by the kernel smoother. The Kernel smoother appears very similar to the moving average smoother, capturing the longer-term El Niño pattern and smoothing out the annual variation\nChange the bandwith to 2 and re-plot the kernel smoother and soi series. Repeat with a bandwidth of 0.5. What do you think the bandwidth parameter does? The higher the bandwidth, the “smoother” the kernel.\nDetrend the soi series with respect to the original kernel smoother with bandwidth 1. Here, we just subtract the estimated trend from the observed series. Note that here we do not have missing values at the ends of the series.\nPlot the detrended soi series. ** It looks pretty stationary**\nPlot the acf of the soi series, the kernel smoother trend estimate, and detrended series. What do you notice? Similar to the moving average smoother, we can see the structure as a result of the smoothing in the acf of the kernel smoother. The acf of the original series and the de-trended series look similar. Again, all have temporal structure.",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#what-is-a-kernel",
    "href": "LectureNotes/Lecture6.html#what-is-a-kernel",
    "title": "Lecture 6",
    "section": "What is a kernel?",
    "text": "What is a kernel?\nA kernel is a moving average smoother that uses a weight function (the kernel) to average the observations: \\[\nm_t = \\sum_{i = 1}^n w_i(t)x_{t_i}\n\\] Where the weight function is\n\\[\nw_i(t) = K \\left ( \\frac{t-t_i}{b}\\right ) / \\sum_{k = 1}^n K \\left(\\frac{t-t_k}{b} \\right )\n\\] Where \\(K(z) = \\exp(-z^2/2)\\) (anyone recognize this?)\nHow does this compare with the Moving Average smoother?",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#interpretation-of-bandwidth",
    "href": "LectureNotes/Lecture6.html#interpretation-of-bandwidth",
    "title": "Lecture 6",
    "section": "Interpretation of bandwidth",
    "text": "Interpretation of bandwidth\nSince time is in years here, and we would expect yearly cycles, we use a bandwidth of 1 to approximately smooth over the year.\nSo, bandwidth does control the degree of smoothness, but the actual value depends on the unit of time measurement (years, months, days, etc.)\nIf the data were monthly, we would use a bandwidth of 12:\n\n\nCode\nSOI = ts(soi, freq = 1) ## convert to monthly\ntsplot(SOI) ## note the x-axis\nlines(ksmooth(time(SOI), SOI, \"normal\", bandwidth = 12), lwd = 2, col = 4)",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#activity-4-loess-example-3.18",
    "href": "LectureNotes/Lecture6.html#activity-4-loess-example-3.18",
    "title": "Lecture 6",
    "section": "Activity 4: Loess (Example 3.18)",
    "text": "Activity 4: Loess (Example 3.18)\nCode source\n\n\nCode\ntsplot(soi, col=astsa.col(4,.6))\nlines(lowess(soi, f=.05), lwd=2, col=4) # El Niño cycle\n# lines(lowess(soi), lty=2, lwd=2, col=2) # trend (with default span)\n#- or -#\n##-- trend with CIs using loess --##\nlo = predict(loess(soi ~ time(soi)), se=TRUE)\ntrnd = ts(lo$fit, start=1950, freq=12) # put back ts attributes\nlines(trnd, col=6, lwd=2)\n L  = trnd - qt(0.975, lo$df)*lo$se\n U  = trnd + qt(0.975, lo$df)*lo$se\n xx = c(time(soi), rev(time(soi)))\n yy = c(L, rev(U))\npolygon(xx, yy, border=8, col=gray(.6, alpha=.4))\n\n\n\n\n\n\n\n\n\nCode\n## detrend wrt \"linear\" trend\n\n\n## detrend wrt \"seasonal\" trend \n\n\n## detrend wrt both\n\n\n\nNotice that there are two trends here. Describe the patterns in each.\nNote that the two smoother estimates use different functions. lowess specifies the f parameter, which is called the bandwidth . What is the name of argument that controls the the span parameter for the function . What does the span parameter control?\nDetrend the soi series with respect to the linear trend smoother and plot it, then do the same for the El Niño trend, then do the same subtracting off both. How do they look?",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#activity-4-solutions",
    "href": "LectureNotes/Lecture6.html#activity-4-solutions",
    "title": "Lecture 6",
    "section": "Activity 4 Solutions",
    "text": "Activity 4 Solutions\nCode source\n\n\nCode\ntsplot(soi, col=astsa.col(4,.6))\nlines(lowess(soi, f=.05), lwd=2, col=4) # El Niño cycle\n# lines(lowess(soi), lty=2, lwd=2, col=2) # trend (with default span)\n#- or -#\n##-- trend with CIs using loess --##\nlo = predict(loess(soi ~ time(soi), span = 1), se=TRUE)\ntrnd = ts(lo$fit, start=1950, freq=12) # put back ts attributes\nlines(trnd, col=6, lwd=2)\n L  = trnd - qt(0.975, lo$df)*lo$se\n U  = trnd + qt(0.975, lo$df)*lo$se\n xx = c(time(soi), rev(time(soi)))\n yy = c(L, rev(U))\npolygon(xx, yy, border=8, col=gray(.6, alpha=.4))\n\n\n\n\n\n\n\n\n\nCode\n## detrend wrt \"linear\" trend\ntsplot(soi - trnd)\n\n\n\n\n\n\n\n\n\nCode\n## detrend wrt \"seasonal\" trend \nsmoother &lt;- lowess(soi, f=.05)\ntsplot(soi - smoother$y)\n\n\n\n\n\n\n\n\n\nCode\n## detrend wrt both\ntsplot(soi- trnd - smoother$y)\n\n\n\n\n\n\n\n\n\n\nNotice that there are two trends here. Describe the patterns in each. There appears to be an approximately linear decreasing trend, and a trend which is similar to the moving average and kernel smoother. The linear-ish trend has a confidence band around it (have we seen that before?)\nNote that the two smoother estimates use different functions. lowess specifies the f parameter, which is called the bandwidth . What is the name of argument that controls the the span parameter for the function . What does the span parameter control?\n\nLooking at the documentation for loess, we see the function has an argument called span, and the defualt is 0.75. Note that the book is incorrect that the default is 2/3. The span controls the degree of smoothing. We will talk about what that means.\n\nDetrend the soi series with respect to the linear trend smoother and plot it, then do the same for the El Niño trend, then do the same subtracting off both. How do they look? They all look fairly similar.",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture6.html#revisiting-the-climate.gov-website",
    "href": "LectureNotes/Lecture6.html#revisiting-the-climate.gov-website",
    "title": "Lecture 6",
    "section": "Revisiting the Climate.gov website",
    "text": "Revisiting the Climate.gov website\nMore about SOI on Climate.gov\nCompare the plot to the smoother estimates we have plotted for the various activities.",
    "crumbs": [
      "Week 3",
      "Lecture 6"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html",
    "href": "LectureNotes/Lecture8.html",
    "title": "Lecture 8 (with solutions)",
    "section": "",
    "text": "SOI : Southern Oscillation Index, a climate variable. Low values correspond to warming and high values correspond to cooling. Plotting a smoother estimate (say, kernel) reveals the El Niño cycle. (we explored this in Lecture 6).\n\n\nCode\nlibrary(astsa)\ntsplot(soi, col=4, ylim = c(-1, 1.15))\nlines(ksmooth(time(soi), soi, \"normal\", bandwidth=1), lwd=2, col=6)",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#recall-the-soi-data",
    "href": "LectureNotes/Lecture8.html#recall-the-soi-data",
    "title": "Lecture 8 (with solutions)",
    "section": "",
    "text": "SOI : Southern Oscillation Index, a climate variable. Low values correspond to warming and high values correspond to cooling. Plotting a smoother estimate (say, kernel) reveals the El Niño cycle. (we explored this in Lecture 6).\n\n\nCode\nlibrary(astsa)\ntsplot(soi, col=4, ylim = c(-1, 1.15))\nlines(ksmooth(time(soi), soi, \"normal\", bandwidth=1), lwd=2, col=6)",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#the-autocorrelation-function",
    "href": "LectureNotes/Lecture8.html#the-autocorrelation-function",
    "title": "Lecture 8 (with solutions)",
    "section": "The autocorrelation function",
    "text": "The autocorrelation function\n\n\nCode\nsoi_ksmooth &lt;- ksmooth(time(soi), soi, \"normal\", bandwidth=1)\ndetrended &lt;- soi-soi_ksmooth$y\n\n## acf of soi/ detrended soi\npar(mfrow = c(2,1))\nacf1(soi)\n\n\n [1]  0.60  0.37  0.21  0.05 -0.11 -0.19 -0.18 -0.10  0.05  0.22  0.36  0.41\n[13]  0.31  0.10 -0.06 -0.17 -0.29 -0.37 -0.32 -0.19 -0.04  0.15  0.31  0.35\n[25]  0.25  0.10 -0.03 -0.16 -0.28 -0.37 -0.32 -0.16 -0.02  0.17  0.33  0.39\n[37]  0.30  0.16  0.00 -0.13 -0.24 -0.27 -0.25 -0.13  0.06  0.21  0.38  0.40\n\n\nCode\nacf1(detrended)\n\n\n\n\n\n\n\n\n\n [1]  0.42  0.12 -0.05 -0.23 -0.40 -0.46 -0.41 -0.28 -0.05  0.21  0.42  0.51\n[13]  0.39  0.13 -0.07 -0.20 -0.35 -0.45 -0.38 -0.22 -0.02  0.22  0.44  0.49\n[25]  0.36  0.14 -0.02 -0.18 -0.34 -0.47 -0.41 -0.21 -0.03  0.21  0.43  0.49\n[37]  0.37  0.17 -0.04 -0.21 -0.36 -0.40 -0.39 -0.24  0.01  0.20  0.42  0.45\n\n\nCode\npar(mfrow = c(1,1))\n\n\nNote the clear seasonal pattern– it’s yearly (monthly frequency divided by 12 means a lag of 1 is a year). Also, values +/-1 year apart have nearly identical correlation structure to those +/- 2 years apart, and so on (the heights of the “bumps” are about the same. I’m showing the detrended acf to emphasize that once we have “detrended” out the El Niño pattern, we still see an annual pattern.\nThe pattern in the detrended acf might be called “residual temporal structure”, specifically, a seasonal pattern.",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#activity-1-example-3.13",
    "href": "LectureNotes/Lecture8.html#activity-1-example-3.13",
    "title": "Lecture 8 (with solutions)",
    "section": "Activity 1 (Example 3.13)",
    "text": "Activity 1 (Example 3.13)\n\n\nCode\nlibrary(astsa)\nlag1.plot(soi, 12, col=4, cex=1)      # Figure 3.10\n\n\n\n\n\n\n\n\n\n\nExplain how the scatterplots below relate to the sample autocorrelation function of soi.\nDo the lagplots/loess trend estimates suggest that sample autocorrelation is a meaningful measurement for the temporal lag relationships?\n\n\n\nIs all the information in the acf represented in the scatterplot matrix of lagplots?",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#activity-1-solutions",
    "href": "LectureNotes/Lecture8.html#activity-1-solutions",
    "title": "Lecture 8 (with solutions)",
    "section": "Activity 1 Solutions",
    "text": "Activity 1 Solutions\n\n\nCode\nlibrary(astsa)\nlag1.plot(soi, 12, col=4, cex=1)      # Figure 3.10\n\n\n\n\n\n\n\n\n\n\nExplain how the scatterplots below relate to the sample autocorrelation function of soi.\nThe sample correlation in the upper right corner of each of the scatterplots match the height of the bar for the corresponding lag.\n\nThe plot in the upper left is the lag1 scatterplot (soi(t) vs. soi(t-1)). The correlation is 0.6, which is the height of the bar for lag 1 (0.08 on x-axis) is in the acf.\nThe plot in the middle left is the lag2 scatterplot (soi(t) vs. soi(t-2)). The correlation is 0.37, which is the height of the bar for lag 2 (0.17 on the x-axis) is in the acf.\n…\nthe third row, first column is the lag7 scatterplot (soi(t) vs. soi(t-7)). The correlation is -0.18, which is the height of the bar for lag 7 (0.58 on x-axis) is in the acf.\n…\nThe plot in the upper left is the lag11 scatterplot (soi(t) vs. soi(t-11)). The correlation is 0.36, which is the height of the bar for lag 11 (.92 on x-axis) is in the acf.\nThe plot in the middle left is the lag12 scatterplot (soi(t) vs. soi(t-12)). The correlation is 0.41, which is the height of the bar for lag 12 (1 on x-axis) is in the acf.\n\nDo the lagplots/loess trend estimates suggest that sample autocorrelation is a meaningful measurement for the temporal lag relationships?\n\nNote that loess is a method for estimating a trend between any two variables (last lecture, one variable was deterministic time) that can allow for nonlinear relationships. Most of the loess fits look fairly linear, meaning that the autocorrelation is meaningful (since correlation is a linear measurement of strength, but we can compute correlation for any two vectors regardless of how inappropriate it is).\n\nIs all the information in the acf represented in the scatterplot matrix of lagplots?\n\nIn an empirical sense, no. We would need to compute the correlation of lags 13, 14, … 4*12 = 48, since that’s how far the x-axis goes on the sample autocorrelation estimate plotted by the acf1 function.\nIn a statistical sense, yes. It seems like the cyclical pattern is very stable (no drop off at later lags), so we could probably represent all the temporal structure with a sinusoid based on the month:\n\n\nCode\nmonth &lt;- seq(from = 0, to = 2*pi, by = 2*pi/12)\nsinusoid &lt;- cos(month)\nplot(1:13, sinusoid, xlab = \"\\'month\\'\", ylab = \"cos(\\\"month\\\")\")\nabline(h = 0)\n\n\n\n\n\n\n\n\n\nThen if we repeat that four times:\n\n\nCode\nmonth &lt;- seq(from = 0, to = 8*pi, by = 2*pi/12)\nsinusoid &lt;- cos(month)\nplot(1:49, sinusoid, xlab = \"Month (#)\", ylab = \"cos(\\\"month\\\")\")\nabline(h = 0)\n\n\n\n\n\n\n\n\n\n… and add vertical lines…\n\n\nCode\nmonth &lt;- seq(from = 0, to = 8*pi, by = 2*pi/12)\nsinusoid &lt;- cos(month)\nplot(1:49, sinusoid, xlab = \"\\'month\\'\", ylab = \"cos(\\\"month\\\")\")\nabline(h = 0)\nsegments(x0 = 1:49, y0= 0, y1 = sinusoid)\n\n\n\n\n\n\n\n\n\n… it starts to look like the acf– the scaling is just off.\n\n\nCode\npar(mfrow = c(2,1))\nacf1(soi)\n\n\n [1]  0.60  0.37  0.21  0.05 -0.11 -0.19 -0.18 -0.10  0.05  0.22  0.36  0.41\n[13]  0.31  0.10 -0.06 -0.17 -0.29 -0.37 -0.32 -0.19 -0.04  0.15  0.31  0.35\n[25]  0.25  0.10 -0.03 -0.16 -0.28 -0.37 -0.32 -0.16 -0.02  0.17  0.33  0.39\n[37]  0.30  0.16  0.00 -0.13 -0.24 -0.27 -0.25 -0.13  0.06  0.21  0.38  0.40\n\n\nCode\nplot(1:49, sinusoid, xlab = \"\\'month\\'\", ylab = \"cos(\\\"month\\\")\")\nabline(h = 0)\nsegments(x0 = 1:49, y0= 0, y1 = sinusoid)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow = c(1,1))\n\n\nThis is explored in chapters 6 and 7 (possible end of quarter topic).",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#activity-2-sort-of-example-3.15",
    "href": "LectureNotes/Lecture8.html#activity-2-sort-of-example-3.15",
    "title": "Lecture 8 (with solutions)",
    "section": "Activity 2 (sort of Example 3.15)",
    "text": "Activity 2 (sort of Example 3.15)\n\n\nCode\nset.seed(807) # so you can reproduce these results\nn &lt;- length(soi)\nx  = 0.5*cos(2*pi*1:n/12) + rnorm(n,0,.25)\n\n\n\nz1 = cos(2*pi*1:n/12)\nz2 = sin(2*pi*1:n/12)\nsummary(fit &lt;- lm(x~ 0 + z1 + z2)) # zero to exclude intercept\n## \n## Call:\n## lm(formula = x ~ 0 + z1 + z2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.70187 -0.16132  0.00053  0.15572  0.62818 \n## \n## Coefficients:\n##     Estimate Std. Error t value Pr(&gt;|t|)    \n## z1  0.494705   0.015859  31.194   &lt;2e-16 ***\n## z2 -0.001529   0.015824  -0.097    0.923    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2384 on 451 degrees of freedom\n## Multiple R-squared:  0.6833, Adjusted R-squared:  0.6819 \n## F-statistic: 486.5 on 2 and 451 DF,  p-value: &lt; 2.2e-16\npar(mfrow=c(2,1))\ntsplot(x, col=4)\ntsplot(x, ylab=expression(hat(x)), col=astsa.col(4, .5))\nlines(fitted(fit), col=2, lwd=2)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nacf(x)\n\n\n\n\n\n\n\n\n\nCode\nacf(resid(fit))\n\n\n\n\n\n\n\n\n\n\nHow does the simulated series x relate to the previous example?\nWhat is the estimated equation for the red line?\nIs it surprising that the red line appears to fit the simulated series so well?\nCompare the autocorrelation function of soi to the autocorrelation function of the simulated series.",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#activity-2-solutions-sort-of-example-3.15",
    "href": "LectureNotes/Lecture8.html#activity-2-solutions-sort-of-example-3.15",
    "title": "Lecture 8 (with solutions)",
    "section": "Activity 2 Solutions (sort of Example 3.15)",
    "text": "Activity 2 Solutions (sort of Example 3.15)\n\n\nCode\nset.seed(807) # so you can reproduce these results\nn &lt;- length(soi)\nx  = 0.5*cos(2*pi*1:n/12) + rnorm(n,0,.25)\n\n\n\nz1 = cos(2*pi*1:n/12)\nz2 = sin(2*pi*1:n/12)\nsummary(fit &lt;- lm(x~ 0 + z1 + z2)) # zero to exclude intercept\n## \n## Call:\n## lm(formula = x ~ 0 + z1 + z2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.70187 -0.16132  0.00053  0.15572  0.62818 \n## \n## Coefficients:\n##     Estimate Std. Error t value Pr(&gt;|t|)    \n## z1  0.494705   0.015859  31.194   &lt;2e-16 ***\n## z2 -0.001529   0.015824  -0.097    0.923    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.2384 on 451 degrees of freedom\n## Multiple R-squared:  0.6833, Adjusted R-squared:  0.6819 \n## F-statistic: 486.5 on 2 and 451 DF,  p-value: &lt; 2.2e-16\npar(mfrow=c(2,1))\ntsplot(x, col=4)\ntsplot(x, ylab=expression(hat(x)), col=astsa.col(4, .5))\nlines(fitted(fit), col=2, lwd=2)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nacf(x)\n\n\n\n\n\n\n\n\n\nCode\nacf(resid(fit))\n\n\n\n\n\n\n\n\n\n\nHow does the simulated series x relate to the previous example?\n\nWe commented that the residual temporal structure is seasonal and could be modeled by a sinusoid. Since the correlation appears to start at a high point, then decrease and reach a maximum at 1 (12 months), we choose a cosine over a sine. Here, we are simulating seasonal data, not analyzing the soi data. We do generate the same number of observations as a\n\nWhat is the estimated equation for the red line?\n\n\\[\n\\hat{x} = 0.495\\cos \\left (\\frac{2\\pi t}{12} \\right) -0.002\\sin \\left (\\frac{2\\pi t}{12} \\right)\n\\]\n\nIs it surprising that the red line appears to fit the simulated series so well?\n\nNo. The estimated equation weights the sine term as essentially 0, and the cosine term near 0.5. The model was generated using \\(0.5\\cos \\left (\\frac{2\\pi t}{12} \\right)\\) for the trend component. Additionally, the acf of the residuals looks like white noise, which is what we simulated (with a smaller standard deviation to mimic the range of the soi data, though).\n\nCompare the autocorrelation function of soi to the autocorrelation function of the simulated series.\n\nThey look very similar.",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#activity-3-kinda-example-3.15",
    "href": "LectureNotes/Lecture8.html#activity-3-kinda-example-3.15",
    "title": "Lecture 8 (with solutions)",
    "section": "Activity 3 (kinda Example 3.15)",
    "text": "Activity 3 (kinda Example 3.15)\n\n\nCode\nz1 = cos(2*pi*1:n/12)\nz2 = sin(2*pi*1:n/12)\nsummary(fit &lt;- lm(soi~ 0 + z1 + z2)) # zero to exclude intercept\n## \n## Call:\n## lm(formula = soi ~ 0 + z1 + z2)\n## \n## Residuals:\n##      Min       1Q   Median       3Q      Max \n## -0.91474 -0.13174  0.08774  0.31774  0.81148 \n## \n## Coefficients:\n##    Estimate Std. Error t value Pr(&gt;|t|)    \n## z1  0.31292    0.02118  14.775  &lt; 2e-16 ***\n## z2  0.07348    0.02113   3.477 0.000556 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3184 on 451 degrees of freedom\n## Multiple R-squared:  0.3385, Adjusted R-squared:  0.3356 \n## F-statistic: 115.4 on 2 and 451 DF,  p-value: &lt; 2.2e-16\npar(mfrow=c(2,1))\ntsplot(ts(soi), col=4)\ntsplot(ts(soi), ylab=expression(hat(soi)), col=astsa.col(4, .5))\nlines(fitted(fit), col=2, lwd=2)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nacf1(ts(resid(fit),freq = 12))\n\n\n\n\n\n\n\n\n##  [1]  0.46  0.31  0.34  0.35  0.31  0.26  0.20  0.13  0.08  0.07  0.09  0.08\n## [13]  0.01 -0.12 -0.09  0.00  0.01 -0.03 -0.03 -0.03 -0.06 -0.04  0.01  0.00\n## [25] -0.07 -0.12 -0.05  0.03  0.03 -0.04 -0.04  0.00 -0.05 -0.02  0.04  0.06\n## [37]  0.00 -0.02 -0.01  0.05  0.07  0.09  0.05  0.05  0.09  0.07  0.15  0.12\n\n\nWhat’s the difference between the model estimated in fit above vs. in Activity 2?\nWhat is the equation of the estimated seasonal trend?\nDoes the temporal structure in soi appear to be captured by the seasonal model?\nWhy convert to a time series before plotting the sample acf?",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#activity-3-solutions-kinda-example-3.15",
    "href": "LectureNotes/Lecture8.html#activity-3-solutions-kinda-example-3.15",
    "title": "Lecture 8 (with solutions)",
    "section": "Activity 3 Solutions (kinda Example 3.15)",
    "text": "Activity 3 Solutions (kinda Example 3.15)\n\n\nCode\nz1 = cos(2*pi*1:n/12)\nz2 = sin(2*pi*1:n/12)\nsoi_fit &lt;-lm(soi~  z1 + z2)\nsummary(soi_fit) # zero to exclude intercept\n## \n## Call:\n## lm(formula = soi ~ z1 + z2)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -0.9967 -0.2122  0.0053  0.2353  0.7295 \n## \n## Coefficients:\n##             Estimate Std. Error t value Pr(&gt;|t|)    \n## (Intercept)  0.08146    0.01448   5.627 3.23e-08 ***\n## z1           0.31378    0.02050  15.310  &lt; 2e-16 ***\n## z2           0.07299    0.02045   3.569 0.000396 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 0.3081 on 450 degrees of freedom\n## Multiple R-squared:  0.3549, Adjusted R-squared:  0.352 \n## F-statistic: 123.8 on 2 and 450 DF,  p-value: &lt; 2.2e-16\npar(mfrow=c(2,1))\ntsplot(ts(soi), col=4)\ntsplot(ts(soi), ylab=expression(hat(x)), col=astsa.col(4, .5))\nlines(soi_fit$fitted.values, col=2, lwd=2)\n\n\n\n\n\n\n\n\n\nCode\npar(mfrow=c(1,1))\nacf1(ts(resid(fit),freq = 12))\n\n\n\n\n\n\n\n\n##  [1]  0.46  0.31  0.34  0.35  0.31  0.26  0.20  0.13  0.08  0.07  0.09  0.08\n## [13]  0.01 -0.12 -0.09  0.00  0.01 -0.03 -0.03 -0.03 -0.06 -0.04  0.01  0.00\n## [25] -0.07 -0.12 -0.05  0.03  0.03 -0.04 -0.04  0.00 -0.05 -0.02  0.04  0.06\n## [37]  0.00 -0.02 -0.01  0.05  0.07  0.09  0.05  0.05  0.09  0.07  0.15  0.12\n\n\nWhat’s the difference between the model estimated in fit above vs. in Activity 2?\n\nHere, we are modeling the real soi data, not the simulated data. However, we are using the same format of the model (just estimating the two amplitudes, one each for the sine and cosine terms).\n\nWhat is the equation of the estimated seasonal trend?\n\n\\[\n\\hat{x} = 0.313\\cos \\left (\\frac{2\\pi t}{12} \\right) +0.073 \\sin \\left (\\frac{2\\pi t}{12} \\right )\n\\]\n\nDoes the temporal structure in soi appear to be captured by the seasonal model?\n\nNo. If the temporal structure were captured, the acf of the residuals would look like white noise, but it does not. (Harmonics??)\n\nWhy convert to a time series before plotting the sample acf?\n\nThis is to make the lag comparable to the other acf plots we were looking at earlier. The residual series resid(fit) is just a vector, not a ts, so the acf will assume a frequency of 1, or monthly. The shape of the plot will look the same, it’s just a labeling consistency thing that captures the annual pattern.",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#choosing-the-seasonal-index",
    "href": "LectureNotes/Lecture8.html#choosing-the-seasonal-index",
    "title": "Lecture 8 (with solutions)",
    "section": "Choosing the seasonal index",
    "text": "Choosing the seasonal index\n\nIf you have quarterly data, period is 4 (freq 1/4)\nIf you have monthly data, period is 12 (freq 1/12)\nIf you have weekly data, period is 52 (freq 1/52)\nIf you have daily data, period is 365.25 (freq 1/365.25)\n\nThere might be cases where the frequency is something else, but maybe not in the context of annual seasonality.",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#activity-4",
    "href": "LectureNotes/Lecture8.html#activity-4",
    "title": "Lecture 8 (with solutions)",
    "section": "Activity 4",
    "text": "Activity 4\nWhat is the period for the following seasonal patterns for data collected every minute?\n\nHourly\nDaily\nWeekly\nMonthly",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#activity-4-solutions",
    "href": "LectureNotes/Lecture8.html#activity-4-solutions",
    "title": "Lecture 8 (with solutions)",
    "section": "Activity 4 Solutions",
    "text": "Activity 4 Solutions\nWhat is the period for the following seasonal patterns for data collected every minute?\n\nHourly: 60\nDaily: 60*24 = 1440\nWeekly: 1440*7 = 10080\nMonthly = 60*24*30 = 43,200 (approximation)\nYearly = 60*24*365.25 = 525,960",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#more-complicated-seasonality",
    "href": "LectureNotes/Lecture8.html#more-complicated-seasonality",
    "title": "Lecture 8 (with solutions)",
    "section": "More complicated “seasonality”",
    "text": "More complicated “seasonality”\n\n\nCode\n#install.packages(\"lubridate\")\nlibrary(lubridate)\n?period",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#a-note-on-terminology",
    "href": "LectureNotes/Lecture8.html#a-note-on-terminology",
    "title": "Lecture 8 (with solutions)",
    "section": "A note on terminology",
    "text": "A note on terminology\n\n\n\n\n\n\n\n\nConcept\nTerm in S&S (red book)\nTerm in FPP (free book)\n\n\n\n\nVariation between a max and a min (fixed period)\ncyclic (introduced as precise seasonality after talking about seasonal differencing in SARIMA model section)\nseasonal\n\n\nVariation between a max and a min (random period)\npseudo*-cyclic\ncyclic\n\n\n\n*pseudo means “false”\nThe same word can mean very different things. Yikes!!! This is why we need a little math (understanding what a period is) to be able to communicate successfully about time series.",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Lecture8.html#example-3.133.14",
    "href": "LectureNotes/Lecture8.html#example-3.133.14",
    "title": "Lecture 8 (with solutions)",
    "section": "Example 3.13/3.14",
    "text": "Example 3.13/3.14\nI skipped over the lagplots of",
    "crumbs": [
      "Week 4",
      "Lecture 8 (with solutions)"
    ]
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#recap",
    "href": "LectureNotes/Slides/Lecture3/index.html#recap",
    "title": "Lecture 3",
    "section": "Recap",
    "text": "Recap\n\n\\[\n\\newcommand\\E{{\\mathbb{E}}}\n\\]\n\n\nVisualizing time series\nResearch questions involving time series\nMean and covariance functions\nMoving average examples\nAlmost got to stationarity"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#today",
    "href": "LectureNotes/Slides/Lecture3/index.html#today",
    "title": "Lecture 3",
    "section": "Today",
    "text": "Today\n\nDecomposing a time series\nStationarity\nAutocorrelation function\nTime series regression"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#first-participation-grade",
    "href": "LectureNotes/Slides/Lecture3/index.html#first-participation-grade",
    "title": "Lecture 3",
    "section": "First “participation” grade",
    "text": "First “participation” grade\n\nconfirm you are good to opt in or out of the textbook, you have to do it by Oct 2 so do it on Oct 1 (tomorrow)."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#lecture-template",
    "href": "LectureNotes/Slides/Lecture3/index.html#lecture-template",
    "title": "Lecture 3",
    "section": "Lecture Template",
    "text": "Lecture Template\n\nDownload “Lecture3Template.qmd” from Canvas\nhas some basic document structure set up to make it easier to follow along in lecture :)"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#another-time-series-model",
    "href": "LectureNotes/Slides/Lecture3/index.html#another-time-series-model",
    "title": "Lecture 3",
    "section": "Another time series model",
    "text": "Another time series model\nSimilar to the signal plus noise model,\n\\[\nX_t = T_t + S_t + W_t\n\\]\n\n\\(T_t\\) is the trend component\n\\(S_t\\) is the seasonal component\n\\(W_t\\) is the error component\n\nThe r function stats::decompose will split a time series \\(X_t\\) into these three components."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-1",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-1",
    "title": "Lecture 3",
    "section": "Activity 1",
    "text": "Activity 1\n\n\n\nlibrary(astsa)\n\n## use the decompose function on the jj series\njj_decomp &lt;- ## your code here\n  \n## plot the decomposition\n## your code here\n\n\nUse the decompose function on the jj series.\nMatch the terms in the equation on the previous slide to each of the components in the chart\nDescribe the trend.\nDoes the bottom plot (“error”) look like white noise?\nLook at the documentation for the decompose function. Can you determine how the “trend” component was computed?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-1-solution",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-1-solution",
    "title": "Lecture 3",
    "section": "Activity 1 (solution)",
    "text": "Activity 1 (solution)\n\n\nCode\nlibrary(astsa)\n\n## use the decompose function on the jj series\njj_decomp &lt;- decompose(jj)\n\n## plot the decomposition\nplot(jj_decomp)"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-2",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-2",
    "title": "Lecture 3",
    "section": "Activity 2",
    "text": "Activity 2\n\n\nRecall the (sinusoidal) signal plus noise model: \\[\nw_t \\sim \\text{iid } N(0, \\sigma^2_w)\\\\\nx_t = 2\\cos\\left (\\frac{2\\pi t}{50} - .6\\right) + w_t\n\\]\n                    \n                    \n                \n\n\nSimulate 500 observations from the signal plus noise model\nApply the decompose function. Does the error portion look like white noise?\n\nHint: The below code gives an error. Compare the “frequency” of the jj series. Can you figure out how to use the ts function to specify the correct frequency?\n\nset.seed(2024)\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\nx_t = cs + w\n\nplot(decompose(x_t))"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-2-solution",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-2-solution",
    "title": "Lecture 3",
    "section": "Activity 2 (solution)",
    "text": "Activity 2 (solution)\n\n\nCode\nset.seed(2024)\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\nx_t = ts(cs + w, frequency = 50)\n\nplot(decompose(x_t))"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#comparing-math-perspective-to-data-perspective",
    "href": "LectureNotes/Slides/Lecture3/index.html#comparing-math-perspective-to-data-perspective",
    "title": "Lecture 3",
    "section": "Comparing “math perspective” to “data perspective”",
    "text": "Comparing “math perspective” to “data perspective”\n\n\n\\[\nw_t \\sim N(0, \\sigma^2_w), t = 1, \\dots, n\\\\\nx_t = 2\\cos\\left (\\frac{2\\pi t}{50} - .6\\right) + w_t\n\\]\n\n\n\n\n\n\n\n\n\n\n\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\nx_t = ts(cs + w, frequency = 50)\n\nplot(decompose(x_t))\n\n\n\n\n\n\n\n\nDoes this function give us an estimate of the form of the mean function?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#review-autocovariance-function",
    "href": "LectureNotes/Slides/Lecture3/index.html#review-autocovariance-function",
    "title": "Lecture 3",
    "section": "Review: autocovariance function",
    "text": "Review: autocovariance function"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#error-covariance-at-different-time-points",
    "href": "LectureNotes/Slides/Lecture3/index.html#error-covariance-at-different-time-points",
    "title": "Lecture 3",
    "section": "Error covariance at different time points",
    "text": "Error covariance at different time points"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#error-covariance-at-different-time-points-time-dependence",
    "href": "LectureNotes/Slides/Lecture3/index.html#error-covariance-at-different-time-points-time-dependence",
    "title": "Lecture 3",
    "section": "Error Covariance at Different Time Points (time dependence)",
    "text": "Error Covariance at Different Time Points (time dependence)"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#stationarity",
    "href": "LectureNotes/Slides/Lecture3/index.html#stationarity",
    "title": "Lecture 3",
    "section": "Stationarity",
    "text": "Stationarity\nA time series is stationary if\n\nthe mean function (\\(\\mu_t\\)) is constant and does not depend on time \\(t\\)\nthe autocovariance function (\\(\\gamma(s,t)\\)) depends on \\(s\\) and \\(t\\) only though their difference\n\nAnd nonstationary otherwise."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#steps-to-determine-whether-a-time-series-x_t-is-stationary",
    "href": "LectureNotes/Slides/Lecture3/index.html#steps-to-determine-whether-a-time-series-x_t-is-stationary",
    "title": "Lecture 3",
    "section": "Steps to determine whether a time series \\(x_t\\) is stationary:",
    "text": "Steps to determine whether a time series \\(x_t\\) is stationary:\n\nCompute the mean function.\nCompute the autocovariance function.\nIf both do not depend on \\(t\\), then \\(x_t\\) is stationary. If \\(\\gamma\\) depends on \\(s\\) and \\(t\\) just through the value \\(s-t\\), then \\(x_t\\) is stationary. Otherwise, \\(x_t\\) is nonstationary."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-3-example-2.14-stationarity-of-a-random-walk",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-3-example-2.14-stationarity-of-a-random-walk",
    "title": "Lecture 3",
    "section": "Activity 3: Example 2.14 Stationarity of a Random Walk",
    "text": "Activity 3: Example 2.14 Stationarity of a Random Walk\n\\[\nx_t = x_{t-1} + w_t\n\\]\nLast, time, we saw that the mean function is \\(\\E(x_t) = 0\\), and the autocovariance function is \\(\\gamma_x(s, t) = \\min\\{s,t\\}\\sigma^2_w\\)\n\n\n\nIs \\(x_t\\) stationary?\nWhat if there was drift?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-3-solution-example-2.14-stationarity-of-a-random-walk",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-3-solution-example-2.14-stationarity-of-a-random-walk",
    "title": "Lecture 3",
    "section": "Activity 3 Solution (Example 2.14 Stationarity of a Random Walk)",
    "text": "Activity 3 Solution (Example 2.14 Stationarity of a Random Walk)\n\nIs \\(x_t\\) stationary?\n\nNo, the autcovariance function depends on \\(t\\) (there’s a \\(t\\) in the equation): \\[\n\\gamma_x(s, t) = \\min\\{s,t\\}\\sigma^2_w\n\\]\nMore concretely: consider if we want to know the correlation between the random walk at times \\(s = 2, t = 5\\), \\[\n\\gamma(2,5) = \\min\\{2,5\\}\\sigma^2_w = 2\\sigma^2_w\n\\] But \\(\\gamma(3,5) = 3\\sigma^2_w\\). So the autocovariance is different depending on which points in time you are considering.\n\nWhat if there was drift?\n\nAgain, no. The mean function of the random walk with drift is \\(\\mu_t = \\delta t\\), which depends on \\(t\\)."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#gammast-for-a-random-walk",
    "href": "LectureNotes/Slides/Lecture3/index.html#gammast-for-a-random-walk",
    "title": "Lecture 3",
    "section": "\\(\\gamma(s,t)\\) for a random walk",
    "text": "\\(\\gamma(s,t)\\) for a random walk"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#is-white-noise-stationary",
    "href": "LectureNotes/Slides/Lecture3/index.html#is-white-noise-stationary",
    "title": "Lecture 3",
    "section": "Is white noise stationary?",
    "text": "Is white noise stationary?\n\nMean function of white noise is \\(\\E(w_t) = 0\\)\nAutocovariance function is \\[\n\\gamma_w(s, t) = cov(w_s, w_t) =  \\begin{cases} \\sigma^2_w & \\text{ if } s = t\\\\ 0 & \\text{ if } s \\ne t \\end{cases}\n\\] Since neither depends on \\(t\\), white noise is stationary."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#gammast-for-white-noise",
    "href": "LectureNotes/Slides/Lecture3/index.html#gammast-for-white-noise",
    "title": "Lecture 3",
    "section": "\\(\\gamma(s,t)\\) for white noise",
    "text": "\\(\\gamma(s,t)\\) for white noise"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#break",
    "href": "LectureNotes/Slides/Lecture3/index.html#break",
    "title": "Lecture 3",
    "section": "Break",
    "text": "Break"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-4",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-4",
    "title": "Lecture 3",
    "section": "Activity 4",
    "text": "Activity 4\nWhich of the following time series are stationary?\n\n\n\n\n\nFrom Forecasting Principles and Practice Chapter 9"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-4-solution",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-4-solution",
    "title": "Lecture 3",
    "section": "Activity 4 (solution)",
    "text": "Activity 4 (solution)\n\n(a), (c), (e), (f) (i) are clearly non-stationary in the mean.\n(d), (h) have seasonal patterns\n(i) has increasing variance\n(b) and (g) are stationary"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#why-is-stationarity-important",
    "href": "LectureNotes/Slides/Lecture3/index.html#why-is-stationarity-important",
    "title": "Lecture 3",
    "section": "Why is stationarity important?",
    "text": "Why is stationarity important?\n\nIn order to measure correlation between contiguous time points\nTo avoid spurious correlations in a regression setting\nSimplifies how we can write the autocovariance and autocorrelation functions"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#autocorrelation-function",
    "href": "LectureNotes/Slides/Lecture3/index.html#autocorrelation-function",
    "title": "Lecture 3",
    "section": "Autocorrelation function",
    "text": "Autocorrelation function\nThe autocorrelation function (acf) of a time series is: \\[\n\\rho(s, t) = \\frac{\\gamma(s,t)}{\\sqrt{\\gamma(s,s)\\gamma(t,t)}}\n\\] i.e. the autocovariance divided by the standard deviation of the process at each time point."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#autocovariance-and-autocorrelation-for-stationary-time-series",
    "href": "LectureNotes/Slides/Lecture3/index.html#autocovariance-and-autocorrelation-for-stationary-time-series",
    "title": "Lecture 3",
    "section": "Autocovariance and Autocorrelation for Stationary Time series",
    "text": "Autocovariance and Autocorrelation for Stationary Time series\nSince for stationary time series the autocovariance depends on \\(s\\) and \\(t\\) only through their difference, we can write the covariance as: \\[\n\\gamma(s,t) = \\gamma(h) = cov(x_{t+h}, x_t) = \\E[(x_{t+h} - \\mu)(x_t-\\mu)]\n\\] and the correlation as: \\[\n\\rho(s,t) = \\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)}\n\\] \\(h = s-t\\) is called the lag."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#autocorrelation-function-of-a-three-point-moving-average",
    "href": "LectureNotes/Slides/Lecture3/index.html#autocorrelation-function-of-a-three-point-moving-average",
    "title": "Lecture 3",
    "section": "Autocorrelation function of a three-point moving average",
    "text": "Autocorrelation function of a three-point moving average\n\\(\\gamma_v(s, t) = cov(v_s, v_t) =  \\begin{cases}\\frac{3}{9}\\sigma^2_w & \\text{ if } s = t\\\\ \\frac{2}{9}\\sigma^2_w & \\text{ if } \\vert s-t \\vert = 1 \\\\\\frac{1}{9}\\sigma^2_w & \\text{ if } \\vert s-t \\vert =2 \\\\0 & \\text{ if } \\vert s - t\\vert &gt; 2\\end{cases}\\)\n\n\nSince \\(v\\) is stationary, we can write\n\\(\\gamma_v(h) = \\begin{cases}\\frac{3}{9}\\sigma^2_w & \\text{ if } h = 0\\\\ \\frac{2}{9}\\sigma^2_w & \\text{ if } h = \\pm1 \\\\\\frac{1}{9}\\sigma^2_w & \\text{ if }h = \\pm 2 \\\\0 & \\text{ if } h&gt; 2\\end{cases}\\)\n\nAnd the autocorrelation is:\n\\(\\rho(h) = \\begin{cases}1 & \\text{ if } h = 0\\\\ \\frac{2}{3} & \\text{ if } h = \\pm1 \\\\\\frac{1}{3} & \\text{ if }h = \\pm 2 \\\\0 & \\text{ if } h&gt; 2\\end{cases}\\)"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#autocorrelation-function-of-a-three-point-moving-average-1",
    "href": "LectureNotes/Slides/Lecture3/index.html#autocorrelation-function-of-a-three-point-moving-average-1",
    "title": "Lecture 3",
    "section": "Autocorrelation function of a three-point moving average",
    "text": "Autocorrelation function of a three-point moving average\nIn R, we can plot \\(\\rho(h)\\)\n\nACF = c(0,0,0,1,2,3,2,1,0,0,0)/3\nLAG = -5:5\ntsplot(LAG, ACF, type=\"h\", lwd=3, xlab=\"LAG\")   \nabline(h=0)\npoints(LAG[-(4:8)], ACF[-(4:8)], pch=20)\naxis(1, at=seq(-5, 5, by=2))"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-5",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-5",
    "title": "Lecture 3",
    "section": "Activity 5",
    "text": "Activity 5\n\nPredict what the acf will look like for the ar(1) process?\nSimulate an ar(1) process and compute the acf. Were you correct?\nWhat is the lag 0 autocorrelation? Explain why its value makes sense.\n\n\n# simulate from an ar(1)\n\n# use acf() function to plot acf\n\n# save output of acf and inspect"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-5-solution",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-5-solution",
    "title": "Lecture 3",
    "section": "Activity 5 (solution)",
    "text": "Activity 5 (solution)\n\n# simulate from an ar(1)\nw &lt;- rnorm(500)\nar_1 &lt;- stats::filter(w, filter = 0.8, method = \"recursive\")\n# use acf() function\nacf(ar_1)\n\n## what is the lag 1 correlation?\nacf_output &lt;- acf(ar_1, plot = F)\nacf_output$acf[2] ## lag 1 autocorrelation\n\n[1] 0.7846967"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-6-problem-2.3",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-6-problem-2.3",
    "title": "Lecture 3",
    "section": "Activity 6 (Problem 2.3)",
    "text": "Activity 6 (Problem 2.3)\nWhen smoothing time series data, it is sometimes advantageous to give decreasing amounts of weights to values farther away from the center. Consider the simple two-sided moving average smoother of the form: \\[\nv_t = \\frac{1}{4}(w_{t-1} + 2w_t + w_{t+1})\n\\] Where \\(w_t\\) are white noise. The autocovariance as a function of \\(h\\) is: \\[\\gamma_v(s, t) = cov(v_s, v_t) =  \\begin{cases}\\frac{6}{16}\\sigma^2_w & \\text{ if } h = 0\\\\ \\frac{4}{16}\\sigma^2_w & \\text{ if } h = \\pm 1 \\\\\\frac{1}{16}\\sigma^2_w & \\text{ if } h = \\pm 2 \\\\0 & \\text{ if } h&gt; 2\\end{cases}\\] 1. Compare to the autocovariance equation for the unweighted 3 point moving average from Lecture 2. Comment on the differences.\n\nWrite down the autocorrelation function."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-6-solution",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-6-solution",
    "title": "Lecture 3",
    "section": "Activity 6 Solution",
    "text": "Activity 6 Solution\n\n6/16 &gt; 3/9, the “present” is weighted higher in the weighted average which impacts the covariance.\nDivide each term by the variance (\\(\\gamma(0)\\)): \\[\\rho_v(s, t) = cor(v_s, v_t) =  \\begin{cases}1 & \\text{ if } h = 0\\\\ \\frac{4}{6} & \\text{ if } h = \\pm 1 \\\\\\frac{1}{6} & \\text{ if } h = \\pm 2 \\\\0 & \\text{ if } h&gt; 2\\end{cases}\\]"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-7",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-7",
    "title": "Lecture 3",
    "section": "Activity 7",
    "text": "Activity 7\nRecall the decomposition of the Johnson and Johnson quarterly earnings.\n\nplot(decompose(jj)) ## plot decomposition\n\n\n\nIs the series stationary?\nDoes the acf of the random component look like white noise?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#activity-7-solution",
    "href": "LectureNotes/Slides/Lecture3/index.html#activity-7-solution",
    "title": "Lecture 3",
    "section": "Activity 7 Solution",
    "text": "Activity 7 Solution\n\njj_decomp &lt;- decompose(jj)\n\npar(mfrow=2:1)\nacf(jj_decomp$random, na.action = na.pass) ## acf of random component\nacf(rnorm(length(jj))) ## acf of white noise of same length"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture3/index.html#coming-up",
    "href": "LectureNotes/Slides/Lecture3/index.html#coming-up",
    "title": "Lecture 3",
    "section": "Coming up:",
    "text": "Coming up:\n\nAssignment 1 due at midnight\nAssignment 2 posted later\nPart of this will be involve “reading” the textbook! (collecting data on how you feel about the math)\nNext Lecture:\n\nRegression with time\nCross-correlation\nInducing stationarity"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#link-to-template",
    "href": "LectureNotes/Slides/Lecture5/index.html#link-to-template",
    "title": "Lecture 5",
    "section": "Link to template",
    "text": "Link to template\n\n\\[\n\\newcommand\\E{{\\mathbb{E}}}\n\\]\n\n\n\n\nLink to Notes Template\n\n\nLecture5Template.qmd"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#last-time",
    "href": "LectureNotes/Slides/Lecture5/index.html#last-time",
    "title": "Lecture 5",
    "section": "Last time",
    "text": "Last time\n\nTrend Stationarity model\nSalmon price example\nSimulating a time series to understand the autocovariance function"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#today",
    "href": "LectureNotes/Slides/Lecture5/index.html#today",
    "title": "Lecture 5",
    "section": "Today",
    "text": "Today\n\nTime series at ENVR Conference\nDetrending\nActivities\nDifferencing"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#a-look-ahead",
    "href": "LectureNotes/Slides/Lecture5/index.html#a-look-ahead",
    "title": "Lecture 5",
    "section": "A look ahead",
    "text": "A look ahead\n\nWeeks 1-2: terminology of time series models, working with various R functions relating to time series (Chapters 1-2)\nWeek 3: Trends, trends, trends (smoothing) (Chapters 2-3 in Shumway and Stoffer, Ch 8 in FPP)\nWeek 4: Time series regression (trends that depend on predictor variables), Forecasting (Chapter 5 and 7 in FPP)\nWeek 5: Time series data science process, and midterm (Chapter 5 in FPP, various parts in Shumway and Stoffer)\nWeek 6: Partial correlation and ARMA models (Chapter 4 in Shumway and Stoffer)\nWeek 7: ARIMA models (what they are, when to use them, and how to know if yours is trash) (Chapter 4 in Shumway and Stoffer, Chapter 9 in FPP)\nWeek 8: Cross-correlation and Multiple time series (Ch 2, Ch 3 in Shumway and Stoffer)\nWeek 9: Wiggle room/class choice\nWeek 10: Wiggle room/class choice"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#is-a-trend-necessarily-linear",
    "href": "LectureNotes/Slides/Lecture5/index.html#is-a-trend-necessarily-linear",
    "title": "Lecture 5",
    "section": "Is a “trend” necessarily linear?",
    "text": "Is a “trend” necessarily linear?\n\n\nOh no, I have to talk to the intimidating experts\nThe very first talk: “A trend doesn’t have to be linear”- Robert Lund\nThe penultimate talk: “seasonal trend”\n\n\nConclusion:\n\n\nNope! Seems like “trend” = “mean function”"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#lots-of-examples-of-vocab-weve-learned",
    "href": "LectureNotes/Slides/Lecture5/index.html#lots-of-examples-of-vocab-weve-learned",
    "title": "Lecture 5",
    "section": "Lots of examples of vocab we’ve learned…",
    "text": "Lots of examples of vocab we’ve learned…"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#notes-on-robert-lunds-talk",
    "href": "LectureNotes/Slides/Lecture5/index.html#notes-on-robert-lunds-talk",
    "title": "Lecture 5",
    "section": "Notes on Robert Lund’s Talk",
    "text": "Notes on Robert Lund’s Talk"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#notes-on-robert-lunds-talk-1",
    "href": "LectureNotes/Slides/Lecture5/index.html#notes-on-robert-lunds-talk-1",
    "title": "Lecture 5",
    "section": "Notes on Robert Lund’s Talk",
    "text": "Notes on Robert Lund’s Talk"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#notes-from-matthias-katzfusss-talk",
    "href": "LectureNotes/Slides/Lecture5/index.html#notes-from-matthias-katzfusss-talk",
    "title": "Lecture 5",
    "section": "Notes from Matthias Katzfuss’s talk",
    "text": "Notes from Matthias Katzfuss’s talk"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#notes-from-dan-cooleys-talk",
    "href": "LectureNotes/Slides/Lecture5/index.html#notes-from-dan-cooleys-talk",
    "title": "Lecture 5",
    "section": "Notes from Dan Cooley’s talk",
    "text": "Notes from Dan Cooley’s talk\n\nMath anxiety rating: 70"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#notes-from-dan-cooleys-talk-1",
    "href": "LectureNotes/Slides/Lecture5/index.html#notes-from-dan-cooleys-talk-1",
    "title": "Lecture 5",
    "section": "Notes from Dan Cooley’s talk",
    "text": "Notes from Dan Cooley’s talk"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#detrending-1",
    "href": "LectureNotes/Slides/Lecture5/index.html#detrending-1",
    "title": "Lecture 5",
    "section": "Detrending",
    "text": "Detrending\nIf a process is trend stationary (nonstationary in the mean, but stationary in the variance), can we just subtract off the trend and get back a stationary time series?\nSometimes (assuming we are able to estimate it), and that’s called detrending."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#goal",
    "href": "LectureNotes/Slides/Lecture5/index.html#goal",
    "title": "Lecture 5",
    "section": "Goal:",
    "text": "Goal:\nAssuming trend stationarity (\\(x_t = \\mu_t + y_t\\), where \\(y_t\\) is stationary), find an estimate \\(\\widehat{\\mu}_t\\) and compute\n\\[\n\\begin{align}\n\\widehat{y_t} &= x_t - \\widehat{\\mu_t}\\\\\n\\text{Estimated Stationary process} &= \\text{Data - trend estimate}\n\\end{align}\n\\] Note: Does \\(y_t\\) remind you of anything from regression?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#example-subtracting-off-the-trend",
    "href": "LectureNotes/Slides/Lecture5/index.html#example-subtracting-off-the-trend",
    "title": "Lecture 5",
    "section": "Example: Subtracting off the trend",
    "text": "Example: Subtracting off the trend\n\n\n\nDark yellow line: the trend estimate\nblack: The observed data\nshaded region: 95% confidence bands on trend estimates.\n\nCan we make the time series stationary by subtracting off the trend?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#example-subtracting-off-the-trend-1",
    "href": "LectureNotes/Slides/Lecture5/index.html#example-subtracting-off-the-trend-1",
    "title": "Lecture 5",
    "section": "Example: Subtracting off the trend",
    "text": "Example: Subtracting off the trend\nDoes this time series appear stationary? In the mean, yes."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#example-subtracting-off-the-trend-2",
    "href": "LectureNotes/Slides/Lecture5/index.html#example-subtracting-off-the-trend-2",
    "title": "Lecture 5",
    "section": "Example: Subtracting off the trend",
    "text": "Example: Subtracting off the trend\nHave we captured the temporal structure in the time series? Yes (note: we will learn about ACF hypothesis tests/p-values during the “time series data analysis process”)"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#aside-managing-a-time-series-project-code-base",
    "href": "LectureNotes/Slides/Lecture5/index.html#aside-managing-a-time-series-project-code-base",
    "title": "Lecture 5",
    "section": "Aside: managing a time series project code base",
    "text": "Aside: managing a time series project code base\n\nI manage the GitHub for Houston Wastewater Epidemiology\nCheck out the “issues”\nSection 1: Demo adding new issue (“basic” time series methods)\nSection 2: Demo adding new issue (link to definition of online estimation)"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#activity-1-detrending-a-commodity-example-3.7",
    "href": "LectureNotes/Slides/Lecture5/index.html#activity-1-detrending-a-commodity-example-3.7",
    "title": "Lecture 5",
    "section": "Activity 1: Detrending a commodity (Example 3.7)",
    "text": "Activity 1: Detrending a commodity (Example 3.7)\n\nGiven the code to generate the plot with the trend line, how would you view the equation of the trend line?\nVisualize the de-trended series. Does it appear stationary?\nCompute the acf of the salmon series and the detrended series. What do you notice?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#activity-1-solutions",
    "href": "LectureNotes/Slides/Lecture5/index.html#activity-1-solutions",
    "title": "Lecture 5",
    "section": "Activity 1 Solutions",
    "text": "Activity 1 Solutions\n\nGiven the code to generate the plot with the trend line, how would you view the equation of the trend line?\n\n\n\n\n\n\n\n\n\n\n\nCall:\nlm(formula = salmon ~ time(salmon), na.action = NULL)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.69187 -0.62453 -0.07024  0.51561  2.34959 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -503.08947   34.44164  -14.61   &lt;2e-16 ***\ntime(salmon)    0.25290    0.01713   14.76   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8814 on 164 degrees of freedom\nMultiple R-squared:  0.5706,    Adjusted R-squared:  0.568 \nF-statistic: 217.9 on 1 and 164 DF,  p-value: &lt; 2.2e-16\n\n\n\nVisualize the de-trended series. Does it appear stationary?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompute the acf of the salmon series and the detrended series. What do you notice?\n\n\n\n\n\n\n\n\n\n\n [1] 0.93 0.84 0.75 0.68 0.61 0.54 0.50 0.47 0.45 0.42 0.39 0.33 0.25 0.18 0.12\n[16] 0.08 0.05 0.02 0.03 0.05 0.08 0.11 0.15 0.18 0.19 0.18 0.18 0.18 0.19 0.20\n[31] 0.24 0.29 0.36 0.41 0.45 0.46 0.43 0.38 0.34 0.30 0.26 0.23 0.21 0.21 0.20\n[46] 0.19 0.16 0.12\n\n\n\n\n\n\n\n\n\n [1]  0.89  0.72  0.56  0.42  0.29  0.20  0.14  0.10  0.08  0.06  0.01 -0.07\n[13] -0.20 -0.33 -0.43 -0.50 -0.54 -0.58 -0.56 -0.50 -0.44 -0.35 -0.26 -0.19\n[25] -0.17 -0.17 -0.17 -0.17 -0.13 -0.09  0.01  0.14  0.27  0.37  0.45  0.48\n[37]  0.42  0.34  0.27  0.21  0.17  0.14  0.14  0.15  0.16  0.14  0.10  0.05"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#activity-2-assuming-we-are-able-to-estimate-it",
    "href": "LectureNotes/Slides/Lecture5/index.html#activity-2-assuming-we-are-able-to-estimate-it",
    "title": "Lecture 5",
    "section": "Activity 2: “assuming we are able to estimate it”",
    "text": "Activity 2: “assuming we are able to estimate it”\n\n\n\nLook at pages 37-41 of the textbook\nwhat is “it” in this context? (what are we estimating?)\nIf this is review, where did you first see these ideas?\nPut a dot on the math anxiety rating distribution on the back board"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#activity-2solutions-assuming-we-are-able-to-estimate-it",
    "href": "LectureNotes/Slides/Lecture5/index.html#activity-2solutions-assuming-we-are-able-to-estimate-it",
    "title": "Lecture 5",
    "section": "Activity 2Solutions: “assuming we are able to estimate it”",
    "text": "Activity 2Solutions: “assuming we are able to estimate it”\n\nLook at pages 37-41 of the textbook ✅\nwhat is “it” in this context? (what are we estimating?)\n\nThe trend (\\(\\beta_0\\) and \\(\\beta_1\\)), variance of the errors (\\(\\sigma^2_w\\)). The\n\nHow are the estimates derived?\n\nOptimization (calculus)\n\nIf this is review, where did you first see these ideas?\n\nYou may have seen this in Regression or Stat 426 (estimation and sampling theory), or Advanced Econometrics\n\nPut a dot on the math anxiety rating distribution on the back board. Color the dot based on whether you seen the math/ideas before."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#motivationmodel",
    "href": "LectureNotes/Slides/Lecture5/index.html#motivationmodel",
    "title": "Lecture 5",
    "section": "Motivation/model",
    "text": "Motivation/model\nConsider the trend stationary model (\\(y_t\\) is stationary). \\[\nx_t = \\mu_t + y_t\n\\] We saw how to estimate a fixed trend using a linear regression for the mean (\\(\\mu_t = \\beta_0 + \\beta_1t\\))\nWe then subtract off the estimate of the trend (detrend), \\(\\widehat{\\mu_t}\\) so that we are working with a stationary time series:\n\\[\n\\widehat{y_t} = x_t - \\widehat{\\mu_t}\n\\]\nWhat if the trend was not fixed? (dependent on \\(t\\) beyond just “\\(t\\) as a constant”)"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#a-stochastic-trend-model",
    "href": "LectureNotes/Slides/Lecture5/index.html#a-stochastic-trend-model",
    "title": "Lecture 5",
    "section": "A stochastic trend model",
    "text": "A stochastic trend model\nChange the model for the mean to incorporate a stochastic component (random walk with drift):\n\\[\n\\mu_t = \\delta + \\mu_{t-1} + w_t\n\\] Where \\(w_t\\) is white noise independent of \\(y_t\\).\nIs \\(\\mu_t\\) stationary? No (it’s a random walk, nonstationary in both mean and covariance)"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#how-to-get-back-to-a-stationary-time-series",
    "href": "LectureNotes/Slides/Lecture5/index.html#how-to-get-back-to-a-stationary-time-series",
    "title": "Lecture 5",
    "section": "How to “get back” to a stationary time series?",
    "text": "How to “get back” to a stationary time series?\nSince the stochastic component depends on just one past time point, consider the series \\(x_t - x_{t-1}\\).\nThis series is called the differenced series and the process is called differencing."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#computing-the-difference-series-in-terms-of-the-stochastic-trend-model",
    "href": "LectureNotes/Slides/Lecture5/index.html#computing-the-difference-series-in-terms-of-the-stochastic-trend-model",
    "title": "Lecture 5",
    "section": "Computing the difference series in terms of the (stochastic) trend model",
    "text": "Computing the difference series in terms of the (stochastic) trend model\n\\[\n\\begin{align}\nx_{t} - x_{t-1} &= (\\mu_t + y_t) - (\\mu_{t-1} + y_{t-1})\\\\\n&= (\\delta + \\mu_{t-1} + w_t + y_t) - (\\mu_{t-1} - y_{t-1})\\\\\n& = \\delta + w_t + y_t - y_{t-1}\n\\end{align}\n\\] Need to compute mean function \\(\\E(x_t - x_{t-1}\\) and autocovariance function \\(cov(x_t - x_{t-1}, x_s- x_{s-1})\\) and check if they do not depend on \\(t\\) (mean) and just depend on the lag \\(h = s-t\\).\n…But the answer is we do get a stationary series!"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#activity-3-simulating-a-random-walk-and-then-differencing-it",
    "href": "LectureNotes/Slides/Lecture5/index.html#activity-3-simulating-a-random-walk-and-then-differencing-it",
    "title": "Lecture 5",
    "section": "Activity 3: Simulating a random walk and then differencing it",
    "text": "Activity 3: Simulating a random walk and then differencing it\n\nSimulate a random walk with no drift and plot it.\n\n\nUse the diff function to difference the simulated series. Plot the result.\n\n\nDoes this series appear stationary? How do you know?\nVisualize the ACF of the differenced series. Does it look like white noise?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#activity-3-solutions-simulating-a-random-walk-and-then-differencing-it",
    "href": "LectureNotes/Slides/Lecture5/index.html#activity-3-solutions-simulating-a-random-walk-and-then-differencing-it",
    "title": "Lecture 5",
    "section": "Activity 3 Solutions: Simulating a random walk and then differencing it",
    "text": "Activity 3 Solutions: Simulating a random walk and then differencing it\n\nSimulate a random walk with no drift and plot it."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#activity-3-solutions-simulating-a-random-walk-and-then-differencing-it-1",
    "href": "LectureNotes/Slides/Lecture5/index.html#activity-3-solutions-simulating-a-random-walk-and-then-differencing-it-1",
    "title": "Lecture 5",
    "section": "Activity 3 Solutions: Simulating a random walk and then differencing it",
    "text": "Activity 3 Solutions: Simulating a random walk and then differencing it\n\nUse the diff function to difference the simulated series. Plot the result.\n\n\n\nDoes this series appear stationary? How do you know? Looks like some pseudo-cyclic behavior, so no."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#activity-3-solutions-simulating-a-random-walk-and-then-differencing-it-2",
    "href": "LectureNotes/Slides/Lecture5/index.html#activity-3-solutions-simulating-a-random-walk-and-then-differencing-it-2",
    "title": "Lecture 5",
    "section": "Activity 3 Solutions: Simulating a random walk and then differencing it",
    "text": "Activity 3 Solutions: Simulating a random walk and then differencing it\n\nVisualize the ACF of the differenced series. Does it look like white noise?\n\nNo– looks like it could be an AR(1)?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#activity-4-differencing-salmon-prices",
    "href": "LectureNotes/Slides/Lecture5/index.html#activity-4-differencing-salmon-prices",
    "title": "Lecture 5",
    "section": "Activity 4: Differencing Salmon Prices",
    "text": "Activity 4: Differencing Salmon Prices\n\nCompute and plot the differenced salmon series.\n\n\nDoes the series appear stationary?\nVisualize the acf of the differenced series. Does it look like white noise?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#activity-4-solutions-differencing-salmon-prices",
    "href": "LectureNotes/Slides/Lecture5/index.html#activity-4-solutions-differencing-salmon-prices",
    "title": "Lecture 5",
    "section": "Activity 4 Solutions: Differencing Salmon Prices",
    "text": "Activity 4 Solutions: Differencing Salmon Prices\n\nCompute and plot the differenced salmon series.\n\n\n\n\n\n\n\n\n\n\n\nDoes the series appear stationary?\n\nIn the mean– yes, maybe?– maybe a pseudo-cyclic type pattern? We should check the ACF\n\nVisualize the acf of the differenced series. Does it look like white noise?\n\n\n\n\n\n\n\n\n\n\n [1]  0.26 -0.02 -0.08 -0.10 -0.19 -0.11 -0.10 -0.06  0.02  0.07  0.15  0.24\n[13]  0.00 -0.15 -0.14 -0.07 -0.04 -0.23 -0.15 -0.05 -0.07 -0.06  0.13  0.21\n[25]  0.11 -0.01  0.00 -0.16 -0.09 -0.23 -0.10  0.04  0.12  0.09  0.23  0.34\n[37]  0.08 -0.06 -0.06 -0.08 -0.07 -0.09 -0.06  0.06  0.10  0.06  0.04  0.12\n\n\nThe acf of the differenced series appears to have patterns indicating annual cycles."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#activity-5-comparing-differencing-and-detrending",
    "href": "LectureNotes/Slides/Lecture5/index.html#activity-5-comparing-differencing-and-detrending",
    "title": "Lecture 5",
    "section": "Activity 5: Comparing Differencing and Detrending",
    "text": "Activity 5: Comparing Differencing and Detrending\nCompare the Acfs of the differenced and detrended salmon series. What do you notice?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#activity-5-solutions-comparing-differencing-and-detrending",
    "href": "LectureNotes/Slides/Lecture5/index.html#activity-5-solutions-comparing-differencing-and-detrending",
    "title": "Lecture 5",
    "section": "Activity 5 Solutions: Comparing Differencing and Detrending",
    "text": "Activity 5 Solutions: Comparing Differencing and Detrending\nCompare the Acfs of the differenced and detrended salmon series. What do you notice?\n\n\n [1]  0.89  0.72  0.56  0.42  0.29  0.20  0.14  0.10  0.08  0.06  0.01 -0.07\n[13] -0.20 -0.33 -0.43 -0.50 -0.54 -0.58 -0.56 -0.50 -0.44 -0.35 -0.26 -0.19\n[25] -0.17 -0.17 -0.17 -0.17 -0.13 -0.09  0.01  0.14  0.27  0.37  0.45  0.48\n[37]  0.42  0.34  0.27  0.21  0.17  0.14  0.14  0.15  0.16  0.14  0.10  0.05\n\n\n\n [1]  0.26 -0.02 -0.08 -0.10 -0.19 -0.11 -0.10 -0.06  0.02  0.07  0.15  0.24\n[13]  0.00 -0.15 -0.14 -0.07 -0.04 -0.23 -0.15 -0.05 -0.07 -0.06  0.13  0.21\n[25]  0.11 -0.01  0.00 -0.16 -0.09 -0.23 -0.10  0.04  0.12  0.09  0.23  0.34\n[37]  0.08 -0.06 -0.06 -0.08 -0.07 -0.09 -0.06  0.06  0.10  0.06  0.04  0.12\n\n\nThe detrended series and the differenced series both show cycles, but the structure of those cycles appears different."
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#next-time-smoothing",
    "href": "LectureNotes/Slides/Lecture5/index.html#next-time-smoothing",
    "title": "Lecture 5",
    "section": "Next time: Smoothing",
    "text": "Next time: Smoothing\nWe’ve seen three explicity ways of modeling a trend (moving average (hw 1), and regression with time and random walk).\nHow else could we model a trend?"
  },
  {
    "objectID": "LectureNotes/Slides/Lecture5/index.html#visual-example",
    "href": "LectureNotes/Slides/Lecture5/index.html#visual-example",
    "title": "Lecture 5",
    "section": "Visual example:",
    "text": "Visual example:"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-1",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-1",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 1",
    "text": "Marginal and Joint Distributions t=8 and s = 1"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-1",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-1",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-2",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-2",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 2",
    "text": "Marginal and Joint Distributions t=8 and s = 2"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-2",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-2",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-3",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-3",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 3",
    "text": "Marginal and Joint Distributions t=8 and s = 3"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-3",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-3",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-4",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-4",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 4",
    "text": "Marginal and Joint Distributions t=8 and s = 4"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-4",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-4",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-5",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-5",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 5",
    "text": "Marginal and Joint Distributions t=8 and s = 5"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-5",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-5",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-6",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-6",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 6",
    "text": "Marginal and Joint Distributions t=8 and s = 6"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-6",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-6",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-7",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-7",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 7",
    "text": "Marginal and Joint Distributions t=8 and s = 7"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-7",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-7",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-8",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-8",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 8",
    "text": "Marginal and Joint Distributions t=8 and s = 8"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-8",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-8",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-9",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-9",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 9",
    "text": "Marginal and Joint Distributions t=8 and s = 9"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-9",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#all-simulated-time-series-9",
    "title": "Moving average autocovariance",
    "section": "All Simulated Time Series",
    "text": "All Simulated Time Series"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-10",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#marginal-and-joint-distributions-t8-and-s-10",
    "title": "Moving average autocovariance",
    "section": "Marginal and Joint Distributions t=8 and s = 10",
    "text": "Marginal and Joint Distributions t=8 and s = 10"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#visualizing-the-correlations",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#visualizing-the-correlations",
    "title": "Moving average autocovariance",
    "section": "Visualizing the correlations",
    "text": "Visualizing the correlations"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#are-these-all-the-correlations",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#are-these-all-the-correlations",
    "title": "Moving average autocovariance",
    "section": "Are these all the correlations?",
    "text": "Are these all the correlations?\nNo, just pairwise with \\(x_8\\). We could do all possible pairs:"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#distribution-of-all-the-correlations",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#distribution-of-all-the-correlations",
    "title": "Moving average autocovariance",
    "section": "Distribution of all the correlations:",
    "text": "Distribution of all the correlations:"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#d-version-of-histogram-includes-s-t-plane",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#d-version-of-histogram-includes-s-t-plane",
    "title": "Moving average autocovariance",
    "section": "3D version of histogram (includes \\(s-t\\) plane)",
    "text": "3D version of histogram (includes \\(s-t\\) plane)"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#simulated-auto-correlation-function-hatgamma_xh-with-a-blanket",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#simulated-auto-correlation-function-hatgamma_xh-with-a-blanket",
    "title": "Moving average autocovariance",
    "section": "Simulated auto correlation function \\(\\hat{\\gamma}_x(h)\\) (with a blanket)",
    "text": "Simulated auto correlation function \\(\\hat{\\gamma}_x(h)\\) (with a blanket)"
  },
  {
    "objectID": "LectureNotes/Slides/SimulateNonstationary/index.html#theoretical-hatgamma_xh-using-the-derived-formula-with-a-blanket",
    "href": "LectureNotes/Slides/SimulateNonstationary/index.html#theoretical-hatgamma_xh-using-the-derived-formula-with-a-blanket",
    "title": "Moving average autocovariance",
    "section": "Theoretical \\(\\hat{\\gamma}_x(h)\\) (using the derived formula (with a blanket)",
    "text": "Theoretical \\(\\hat{\\gamma}_x(h)\\) (using the derived formula (with a blanket)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  }
]