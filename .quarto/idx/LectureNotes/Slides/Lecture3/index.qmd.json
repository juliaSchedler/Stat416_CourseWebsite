{"title":"Lecture 3","markdown":{"yaml":{"title":"Lecture 3","author":"Julia Schedler","format":{"revealjs":{"slide-number":true,"scrollable":true}},"filters":["timer"]},"headingText":"Recap","containsRefs":false,"markdown":"\n\n\n::: hidden\n$$\n\\newcommand\\E{{\\mathbb{E}}}\n$$\n:::\n\n-   Visualizing time series\n-   Research questions involving time series\n-   Mean and covariance functions\n-   Moving average examples\n-   Almost got to stationarity\n\n## Today\n\n-   Decomposing a time series\n\n-   Stationarity\n\n-   Autocorrelation function\n\n-   Time series regression\n\n## First \"participation\" grade\n\n-   confirm you are good to opt in or out of the textbook, you have to do it by Oct 2 so do it on Oct 1 (tomorrow).\n\n## Lecture Template\n\n-   Download \"Lecture3Template.qmd\" from Canvas\n-   has some basic document structure set up to make it easier to follow along in lecture :)\n\n## Another time series model\n\nSimilar to the signal plus noise model,\n\n$$\nX_t = T_t + S_t + W_t\n$$\n\n-   $T_t$ is the trend component\n-   $S_t$ is the seasonal component\n-   $W_t$ is the error component\n\nThe `r` function `stats::decompose` will split a time series $X_t$ into these three components.\n\n## Activity 1 {.smaller}\n\n::: columns\n::: {.column width=\"60%\"}\n```{r}\n#| label: activity-1\n#| echo: true\n#| eval: false\n\nlibrary(astsa)\n\n## use the decompose function on the jj series\njj_decomp <- ## your code here\n  \n## plot the decomposition\n## your code here\n```\n\n1.  Use the `decompose` function on the `jj` series.\n\n2.  Match the terms in the equation on the previous slide to each of the components in the chart\n\n3.  Describe the trend.\n\n4.  Does the bottom plot (\"error\") look like white noise?\n\n5.  Look at the documentation for the `decompose` function. Can you determine how the \"trend\" component was computed?\n:::\n\n::: {.column width=\"10%\"}\n::: {#DecomposeJJ .timer seconds=\"300\" starton=\"interaction\"}\n:::\n:::\n:::\n\n## Activity 1 (solution)\n\n```{r}\n#| label: activity-1-solution\n#| echo: true\n#| code-fold: true\n\nlibrary(astsa)\n\n## use the decompose function on the jj series\njj_decomp <- decompose(jj)\n\n## plot the decomposition\nplot(jj_decomp)\n```\n\n## Activity 2 {.smaller}\n\n::: columns\n::: column\nRecall the (sinusoidal) signal plus noise model: $$\nw_t \\sim \\text{iid } N(0, \\sigma^2_w)\\\\\nx_t = 2\\cos\\left (\\frac{2\\pi t}{50} - .6\\right) + w_t\n$$\n\n::: {#SimulateDecomp .timer seconds=\"300\" starton=\"interaction\"}\n:::\n:::\n\n::: column\n1.  Simulate 500 observations from the signal plus noise model\n2.  Apply the `decompose` function. Does the error portion look like white noise?\n\nHint: The below code gives an error. Compare the \"frequency\" of the `jj` series. Can you figure out how to use the `ts` function to specify the correct frequency?\n\n```{r}\n#| echo: true\n#| eval: false\nset.seed(2024)\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\nx_t = cs + w\n\nplot(decompose(x_t))\n\n```\n:::\n:::\n\n## Activity 2 (solution)\n\n```{r}\n#| echo: true\n#| code-fold: show\n\nset.seed(2024)\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\nx_t = ts(cs + w, frequency = 50)\n\nplot(decompose(x_t))\n\n```\n\n## [Comparing \"math perspective\" to \"data perspective\"]{.r-fit-text}\n\n::: columns\n::: column\n$$\nw_t \\sim N(0, \\sigma^2_w), t = 1, \\dots, n\\\\\nx_t = 2\\cos\\left (\\frac{2\\pi t}{50} - .6\\right) + w_t\n$$\n\n```{r}\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\ntsplot(cs + w)\nlines(cs, col = \"blueviolet\", type = \"l\", lwd = 4)\n```\n:::\n\n::: column\n```{r}\n#| label: decompose\n#| echo: true\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\nx_t = ts(cs + w, frequency = 50)\n\nplot(decompose(x_t))\n```\n\nDoes this function give us an estimate of the *form* of the mean function?\n:::\n:::\n\n# Motivating Stationarity\n\n## Review: autocovariance function\n\n```{r}\n#| label: show-error-dists\nlibrary(ggplot2)\n\nt <- seq(1, 16, 1)\nx_t <- 0.5*t \n#x <- x - mean(x)\n#y <- y - mean(y)\n\ndf <- data.frame(t, x_t)\n\n# For every row in `df`, compute a rotated normal density centered at `y` and shifted by `x`\ncurves <- lapply(seq_len(NROW(df)), function(i) {\n  mu <- df$x_t[i]\n  range <- mu + c(-1.5, 1.5)\n  seq <- seq(range[1], range[2], length.out = 100)\n  data.frame(\n    t = -1 * dnorm(seq, mean = mu, sd = 0.5) + df$t[i],\n    x_t = seq,\n    grp = i\n  )\n})\n# Combine above densities in one data.frame\ncurves <- do.call(rbind, curves)\n\nnew.x = seq(from = 1, to = 16, by = .1)\nnew.y = .5*new.x\ntrend_line <- data.frame(x = new.x,\n                       y = new.y)\nggplot(df, aes(t, x_t)) +\n  geom_point(col = \"blueviolet\", pch = 17) +\n  #geom_line() +\n  # The path draws the curve\n  geom_path(data = curves, aes(group = grp)) +\n  geom_line(data = trend_line, aes(x=x,y=y), col = \"blueviolet\") +\n  lims(y = c(-2,10)) +\n  scale_x_continuous(breaks = seq(1, 16, by = 1)) +\n  theme_minimal() + \n  theme( # remove the vertical grid lines\n           panel.grid = element_blank() ,\n           # explicitly set the horizontal lines (or they will disappear too)\n           panel.grid.major.x = element_line( size=.1, color=\"black\" )) +   \n  geom_rect(aes(xmin = 3.1, xmax = 4.1, ymin = 0, ymax = 4), fill = NA, col = \"blue\")+   \n  geom_rect(aes(xmin = 7.1, xmax = 8.1, ymin = 2, ymax = 6), fill = NA, col = \"magenta\")\n  # The polygon does the shading. We can use `oob_squish()` to set a range.\n  #geom_polygon(data = curves, aes(y = scales::oob_squish(y, c(0, Inf)),group = grp))\n```\n\n## [Error covariance at different time points]{.r-fit-text}\n\n```{r}\n#| label: errors-regression\n# install.packages(\"ggplot2\")\n# install.packages(\"ggExtra\")\nlibrary(ggplot2)\nlibrary(ggExtra)\nset.seed(50)\nx1 <- rnorm(100,x_t[4], .5)\nx2 <- rnorm(100,x_t[8], .5)\n\nx <- data.frame(x1, x2)\n# Save the scatter plot in a variable\np <- ggplot(x, aes(x = x1, y = x2)) +\n  geom_point() + xlim(0,6) + ylim(0,6)+ \n  geom_text(aes(x = 4, y = 2, label = \"gamma(2,8) = \\n cov(x_2, x_8) = 0\"), size = 6) + coord_fixed() \n\n# Arguments for each marginal histogram\nggMarginal(p, type = \"density\", adjust = 2,\n           xparams = list(col = \"blue\", fill = \"blue\"),\n           yparams = list(col = \"magenta\", fill = \"magenta\"))\n```\n\n## [Error Covariance at Different Time Points (time dependence)]{.r-fit-text}\n\n```{r}\n#| label: time_series_autocov\n#install.packages(\"MASS\")\nlibrary(MASS)\nset.seed(100)\nmu <- c(x_t[4], x_t[8])\nvarcov <- matrix(c(.5, .3, .3, .5), \n                 ncol = 2)\nx<- mvrnorm(100, mu = mu, Sigma =varcov)\nx <- data.frame(x1 = x[,1], x2 = x[,2])\n# Save the scatter plot in a variable\np <- ggplot(x, aes(x = x1, y = x2)) +\n  geom_point() + xlim(0,6) + ylim(0,6) + \n  geom_text(aes(x = 4, y = 2, label = \"gamma(2,8) = \\n cov(x_2, x_8) = 0.2\"), size = 6) + coord_fixed() \n\n# Arguments for each marginal histogram\nggMarginal(p, type = \"density\", adjust = 2,\n           xparams = list(col = \"blue\", fill = \"blue\"),\n           yparams = list(col = \"magenta\", fill = \"magenta\"))\n```\n\n## Stationarity\n\nA time series is **stationary** if\n\n-   the mean function ($\\mu_t$) is constant and does not depend on time $t$\n-   the autocovariance function ($\\gamma(s,t)$) depends on $s$ and $t$ only though their difference\n\nAnd **nonstationary** otherwise.\n\n## [Steps to determine whether a time series $x_t$ is stationary:]{.r-fit-text}\n\n1.  Compute the mean function.\n2.  Compute the autocovariance function.\n3.  If both do not depend on $t$, then $x_t$ is stationary. If $\\gamma$ depends on $s$ and $t$ just through the value $s-t$, then $x_t$ is stationary. Otherwise, $x_t$ is nonstationary.\n\n## [Activity 3: Example 2.14 Stationarity of a Random Walk]{.r-fit-text}\n\n$$\nx_t = x_{t-1} + w_t\n$$\n\nLast, time, we saw that the mean function is $\\E(x_t) = 0$, and the autocovariance function is $\\gamma_x(s, t) = \\min\\{s,t\\}\\sigma^2_w$\n\n::: columns\n::: {.column width=\"60%\"}\n1.  **Is** $x_t$ stationary?\n2.  **What if there was drift?**\n:::\n\n::: {.column width=\"10%\"}\n::: {#Stationary .timer seconds=\"180\" starton=\"interaction\"}\n:::\n:::\n:::\n\n## [Activity 3 Solution (Example 2.14 Stationarity of a Random Walk)]{.r-fit-text} {.smaller}\n\n1.  **Is** $x_t$ stationary?\n\nNo, the autcovariance function depends on $t$ (there's a $t$ in the equation): $$\n\\gamma_x(s, t) = \\min\\{s,t\\}\\sigma^2_w\n$$\n\nMore concretely: consider if we want to know the correlation between the random walk at times $s = 2, t = 5$, $$\n\\gamma(2,5) = \\min\\{2,5\\}\\sigma^2_w = 2\\sigma^2_w\n$$ But $\\gamma(3,5) = 3\\sigma^2_w$. So the autocovariance is different depending on which points in time you are considering.\n\n2.  **What if there was drift?**\n\nAgain, no. The mean function of the random walk with drift is $\\mu_t = \\delta t$, which depends on $t$.\n\n## $\\gamma(s,t)$ for a random walk\n\n```{r}\nsigma_w <- 5 #define variance of the white noise\ncoords <- expand.grid(1:20, 1:20)\nnames(coords) <- c(\"s\", \"t\") ## the coordinates represent different possible time points\ncoords$gamma <- pmin(coords$s, coords$t)*sigma_w\n\n\nlibrary(plotly)\nplot_ly(coords,\n  x= ~s, y=~t, z=~gamma, \n  type = 'scatter3d', mode = \"markers\", size = .1)\n```\n\n## Is white noise stationary?\n\n-   Mean function of white noise is $\\E(w_t) = 0$\n-   Autocovariance function is $$\n    \\gamma_w(s, t) = cov(w_s, w_t) =  \\begin{cases} \\sigma^2_w & \\text{ if } s = t\\\\ 0 & \\text{ if } s \\ne t \\end{cases}\n    $$ Since neither depends on $t$, white noise is stationary.\n\n## $\\gamma(s,t)$ for white noise\n\n```{r}\n# plot the autocov's we computed last time, show on model of time series\n```\n\n```{r}\n\n## white noise\nsigma_w <- 5 #define variance of the white noise\ncoords <- expand.grid(1:20, 1:20)\nnames(coords) <- c(\"s\", \"t\") ## the coordinates represent different possible time points\ncoords$gamma <- 0\ncoords$gamma[coords[,1] == coords[,2]] <- sigma_w ## covariance is sigma_w if s = t, 0 otherwise\n\n\nlibrary(plotly)\nplot_ly(coords,\n  x= ~s, y=~t, z=~gamma, \n  type = 'scatter3d', mode = \"markers\", size = .1)\n```\n\n## Break\n\n::: {#Break .timer seconds=\"360\" starton=\"interaction\"}\n:::\n\n## Activity 4\n\nWhich of the following time series are stationary?\n\n::: columns\n::: {.column width=\"60%\"}\n![From Forecasting Principles and Practice Chapter 9](https://otexts.com/fpp3/fpp_files/figure-html/stationary-1.png)\n:::\n\n::: {.column width=\"20%\"}\n::: {#VisualStationary .timer seconds=\"300\" starton=\"interaction\"}\n:::\n:::\n:::\n\n## Activity 4 (solution)\n\n-   (a), (c), (e), (f) (i) are clearly non-stationary in the mean.\n-   (d), (h) have seasonal patterns\n-   \\(i\\) has increasing variance\n-   \\(b\\) and (g) are stationary\n\n## Why is stationarity important?\n\n-   In order to measure correlation between contiguous time points\n-   To avoid spurious correlations in a regression setting\n-   Simplifies how we can write the autocovariance and autocorrelation functions\n\n## Autocorrelation function\n\nThe autocorrelation function (acf) of a time series is: $$\n\\rho(s, t) = \\frac{\\gamma(s,t)}{\\sqrt{\\gamma(s,s)\\gamma(t,t)}}\n$$ i.e. the autocovariance divided by the standard deviation of the process at each time point.\n\n## [Autocovariance and Autocorrelation for Stationary Time series]{.r-fit-text} {.smaller}\n\nSince for stationary time series the autocovariance depends on $s$ and $t$ only through their difference, we can write the covariance as: $$\n\\gamma(s,t) = \\gamma(h) = cov(x_{t+h}, x_t) = \\E[(x_{t+h} - \\mu)(x_t-\\mu)]\n$$ and the correlation as: $$\n\\rho(s,t) = \\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)}\n$$ $h = s-t$ is called the **lag**.\n\n## [Autocorrelation function of a three-point moving average]{.r-fit-text} {.smaller}\n\n$\\gamma_v(s, t) = cov(v_s, v_t) =  \\begin{cases}\\frac{3}{9}\\sigma^2_w & \\text{ if } s = t\\\\ \\frac{2}{9}\\sigma^2_w & \\text{ if } \\vert s-t \\vert = 1 \\\\\\frac{1}{9}\\sigma^2_w & \\text{ if } \\vert s-t \\vert =2 \\\\0 & \\text{ if } \\vert s - t\\vert > 2\\end{cases}$\n\n::: columns\n::: column\nSince $v$ is stationary, we can write\n\n$\\gamma_v(h) = \\begin{cases}\\frac{3}{9}\\sigma^2_w & \\text{ if } h = 0\\\\ \\frac{2}{9}\\sigma^2_w & \\text{ if } h = \\pm1 \\\\\\frac{1}{9}\\sigma^2_w & \\text{ if }h = \\pm 2 \\\\0 & \\text{ if } h> 2\\end{cases}$\n:::\n\n::: column\nAnd the autocorrelation is:\n\n$\\rho(h) = \\begin{cases}1 & \\text{ if } h = 0\\\\ \\frac{2}{3} & \\text{ if } h = \\pm1 \\\\\\frac{1}{3} & \\text{ if }h = \\pm 2 \\\\0 & \\text{ if } h> 2\\end{cases}$\n:::\n:::\n\n## [Autocorrelation function of a three-point moving average]{.r-fit-text}\n\nIn R, we can plot $\\rho(h)$\n\n```{r}\n#| echo: true\n\nACF = c(0,0,0,1,2,3,2,1,0,0,0)/3\nLAG = -5:5\ntsplot(LAG, ACF, type=\"h\", lwd=3, xlab=\"LAG\")   \nabline(h=0)\npoints(LAG[-(4:8)], ACF[-(4:8)], pch=20)\naxis(1, at=seq(-5, 5, by=2))  \n\n```\n\n## Activity 5 {.smaller}\n\n1.  Predict what the acf will look like for the ar(1) process?\n2.  Simulate an ar(1) process and compute the acf. Were you correct?\n3.  What is the lag 0 autocorrelation? Explain why its value makes sense.\n\n```{r}\n#| echo: true\n# simulate from an ar(1)\n\n# use acf() function to plot acf\n\n# save output of acf and inspect\n\n```\n\n::: {#activity4 .timer seconds=\"300\" starton=\"interaction\"}\n:::\n\n## Activity 5 (solution)\n\n```{r}\n#| echo: true\n# simulate from an ar(1)\nw <- rnorm(500)\nar_1 <- stats::filter(w, filter = 0.8, method = \"recursive\")\n# use acf() function\nacf(ar_1)\n\n## what is the lag 1 correlation?\nacf_output <- acf(ar_1, plot = F)\nacf_output$acf[2] ## lag 1 autocorrelation\n```\n\n# Questions on the quiz?\n\n## Activity 6 (Problem 2.3) {.smaller}\n\nWhen smoothing time series data, it is sometimes advantageous to give decreasing amounts of weights to values farther away from the center. Consider the simple two-sided moving average smoother of the form: $$\nv_t = \\frac{1}{4}(w_{t-1} + 2w_t + w_{t+1})\n$$ Where $w_t$ are white noise. The autocovariance as a function of $h$ is: $$\\gamma_v(s, t) = cov(v_s, v_t) =  \\begin{cases}\\frac{6}{16}\\sigma^2_w & \\text{ if } h = 0\\\\ \\frac{4}{16}\\sigma^2_w & \\text{ if } h = \\pm 1 \\\\\\frac{1}{16}\\sigma^2_w & \\text{ if } h = \\pm 2 \\\\0 & \\text{ if } h> 2\\end{cases}$$ 1. Compare to the [autocovariance equation for the unweighted 3 point moving average from Lecture 2](https://juliaschedler.github.io/Stat416Fall24/LectureNotes/Lecture2.html#example-2.8-autocovariance-of-a-moving-average-1). Comment on the differences.\n\n2.  Write down the autocorrelation function.\n\n::: {#activity5 .timer seconds=\"300\" starton=\"interaction\"}\n:::\n\n## Activity 6 Solution\n\n1.  6/16 \\> 3/9, the \"present\" is weighted higher in the weighted average which impacts the covariance.\n2.  Divide each term by the variance ($\\gamma(0)$): $$\\rho_v(s, t) = cor(v_s, v_t) =  \\begin{cases}1 & \\text{ if } h = 0\\\\ \\frac{4}{6} & \\text{ if } h = \\pm 1 \\\\\\frac{1}{6} & \\text{ if } h = \\pm 2 \\\\0 & \\text{ if } h> 2\\end{cases}$$\n\n## Activity 7\n\nRecall the decomposition of the Johnson and Johnson quarterly earnings.\n\n```{r}\n#| echo: true\n\nplot(decompose(jj)) ## plot decomposition\n```\n\n1.  Is the series stationary?\n2.  Does the acf of the random component look like white noise?\n\n::: {#activity6 .timer seconds=\"300\" starton=\"interaction\"}\n:::\n\n## Activity 7 Solution\n\n```{r}\n#| echo: true\n\njj_decomp <- decompose(jj)\n\npar(mfrow=2:1)\nacf(jj_decomp$random, na.action = na.pass) ## acf of random component\nacf(rnorm(length(jj))) ## acf of white noise of same length\n```\n\n## Coming up:\n\n-   Assignment 1 due at midnight\n-   Assignment 2 posted later\n-   Part of this will be involve \"reading\" the textbook! (collecting data on how you feel about the math)\n-   Next Lecture:\n    -   Regression with time\n    -   Cross-correlation\n    -   Inducing stationarity\n","srcMarkdownNoYaml":"\n\n## Recap\n\n::: hidden\n$$\n\\newcommand\\E{{\\mathbb{E}}}\n$$\n:::\n\n-   Visualizing time series\n-   Research questions involving time series\n-   Mean and covariance functions\n-   Moving average examples\n-   Almost got to stationarity\n\n## Today\n\n-   Decomposing a time series\n\n-   Stationarity\n\n-   Autocorrelation function\n\n-   Time series regression\n\n## First \"participation\" grade\n\n-   confirm you are good to opt in or out of the textbook, you have to do it by Oct 2 so do it on Oct 1 (tomorrow).\n\n## Lecture Template\n\n-   Download \"Lecture3Template.qmd\" from Canvas\n-   has some basic document structure set up to make it easier to follow along in lecture :)\n\n## Another time series model\n\nSimilar to the signal plus noise model,\n\n$$\nX_t = T_t + S_t + W_t\n$$\n\n-   $T_t$ is the trend component\n-   $S_t$ is the seasonal component\n-   $W_t$ is the error component\n\nThe `r` function `stats::decompose` will split a time series $X_t$ into these three components.\n\n## Activity 1 {.smaller}\n\n::: columns\n::: {.column width=\"60%\"}\n```{r}\n#| label: activity-1\n#| echo: true\n#| eval: false\n\nlibrary(astsa)\n\n## use the decompose function on the jj series\njj_decomp <- ## your code here\n  \n## plot the decomposition\n## your code here\n```\n\n1.  Use the `decompose` function on the `jj` series.\n\n2.  Match the terms in the equation on the previous slide to each of the components in the chart\n\n3.  Describe the trend.\n\n4.  Does the bottom plot (\"error\") look like white noise?\n\n5.  Look at the documentation for the `decompose` function. Can you determine how the \"trend\" component was computed?\n:::\n\n::: {.column width=\"10%\"}\n::: {#DecomposeJJ .timer seconds=\"300\" starton=\"interaction\"}\n:::\n:::\n:::\n\n## Activity 1 (solution)\n\n```{r}\n#| label: activity-1-solution\n#| echo: true\n#| code-fold: true\n\nlibrary(astsa)\n\n## use the decompose function on the jj series\njj_decomp <- decompose(jj)\n\n## plot the decomposition\nplot(jj_decomp)\n```\n\n## Activity 2 {.smaller}\n\n::: columns\n::: column\nRecall the (sinusoidal) signal plus noise model: $$\nw_t \\sim \\text{iid } N(0, \\sigma^2_w)\\\\\nx_t = 2\\cos\\left (\\frac{2\\pi t}{50} - .6\\right) + w_t\n$$\n\n::: {#SimulateDecomp .timer seconds=\"300\" starton=\"interaction\"}\n:::\n:::\n\n::: column\n1.  Simulate 500 observations from the signal plus noise model\n2.  Apply the `decompose` function. Does the error portion look like white noise?\n\nHint: The below code gives an error. Compare the \"frequency\" of the `jj` series. Can you figure out how to use the `ts` function to specify the correct frequency?\n\n```{r}\n#| echo: true\n#| eval: false\nset.seed(2024)\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\nx_t = cs + w\n\nplot(decompose(x_t))\n\n```\n:::\n:::\n\n## Activity 2 (solution)\n\n```{r}\n#| echo: true\n#| code-fold: show\n\nset.seed(2024)\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\nx_t = ts(cs + w, frequency = 50)\n\nplot(decompose(x_t))\n\n```\n\n## [Comparing \"math perspective\" to \"data perspective\"]{.r-fit-text}\n\n::: columns\n::: column\n$$\nw_t \\sim N(0, \\sigma^2_w), t = 1, \\dots, n\\\\\nx_t = 2\\cos\\left (\\frac{2\\pi t}{50} - .6\\right) + w_t\n$$\n\n```{r}\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\ntsplot(cs + w)\nlines(cs, col = \"blueviolet\", type = \"l\", lwd = 4)\n```\n:::\n\n::: column\n```{r}\n#| label: decompose\n#| echo: true\ncs = 2*cos(2*pi*(1:500)/50 + .6*pi)\nw  = rnorm(500,0,1)\nx_t = ts(cs + w, frequency = 50)\n\nplot(decompose(x_t))\n```\n\nDoes this function give us an estimate of the *form* of the mean function?\n:::\n:::\n\n# Motivating Stationarity\n\n## Review: autocovariance function\n\n```{r}\n#| label: show-error-dists\nlibrary(ggplot2)\n\nt <- seq(1, 16, 1)\nx_t <- 0.5*t \n#x <- x - mean(x)\n#y <- y - mean(y)\n\ndf <- data.frame(t, x_t)\n\n# For every row in `df`, compute a rotated normal density centered at `y` and shifted by `x`\ncurves <- lapply(seq_len(NROW(df)), function(i) {\n  mu <- df$x_t[i]\n  range <- mu + c(-1.5, 1.5)\n  seq <- seq(range[1], range[2], length.out = 100)\n  data.frame(\n    t = -1 * dnorm(seq, mean = mu, sd = 0.5) + df$t[i],\n    x_t = seq,\n    grp = i\n  )\n})\n# Combine above densities in one data.frame\ncurves <- do.call(rbind, curves)\n\nnew.x = seq(from = 1, to = 16, by = .1)\nnew.y = .5*new.x\ntrend_line <- data.frame(x = new.x,\n                       y = new.y)\nggplot(df, aes(t, x_t)) +\n  geom_point(col = \"blueviolet\", pch = 17) +\n  #geom_line() +\n  # The path draws the curve\n  geom_path(data = curves, aes(group = grp)) +\n  geom_line(data = trend_line, aes(x=x,y=y), col = \"blueviolet\") +\n  lims(y = c(-2,10)) +\n  scale_x_continuous(breaks = seq(1, 16, by = 1)) +\n  theme_minimal() + \n  theme( # remove the vertical grid lines\n           panel.grid = element_blank() ,\n           # explicitly set the horizontal lines (or they will disappear too)\n           panel.grid.major.x = element_line( size=.1, color=\"black\" )) +   \n  geom_rect(aes(xmin = 3.1, xmax = 4.1, ymin = 0, ymax = 4), fill = NA, col = \"blue\")+   \n  geom_rect(aes(xmin = 7.1, xmax = 8.1, ymin = 2, ymax = 6), fill = NA, col = \"magenta\")\n  # The polygon does the shading. We can use `oob_squish()` to set a range.\n  #geom_polygon(data = curves, aes(y = scales::oob_squish(y, c(0, Inf)),group = grp))\n```\n\n## [Error covariance at different time points]{.r-fit-text}\n\n```{r}\n#| label: errors-regression\n# install.packages(\"ggplot2\")\n# install.packages(\"ggExtra\")\nlibrary(ggplot2)\nlibrary(ggExtra)\nset.seed(50)\nx1 <- rnorm(100,x_t[4], .5)\nx2 <- rnorm(100,x_t[8], .5)\n\nx <- data.frame(x1, x2)\n# Save the scatter plot in a variable\np <- ggplot(x, aes(x = x1, y = x2)) +\n  geom_point() + xlim(0,6) + ylim(0,6)+ \n  geom_text(aes(x = 4, y = 2, label = \"gamma(2,8) = \\n cov(x_2, x_8) = 0\"), size = 6) + coord_fixed() \n\n# Arguments for each marginal histogram\nggMarginal(p, type = \"density\", adjust = 2,\n           xparams = list(col = \"blue\", fill = \"blue\"),\n           yparams = list(col = \"magenta\", fill = \"magenta\"))\n```\n\n## [Error Covariance at Different Time Points (time dependence)]{.r-fit-text}\n\n```{r}\n#| label: time_series_autocov\n#install.packages(\"MASS\")\nlibrary(MASS)\nset.seed(100)\nmu <- c(x_t[4], x_t[8])\nvarcov <- matrix(c(.5, .3, .3, .5), \n                 ncol = 2)\nx<- mvrnorm(100, mu = mu, Sigma =varcov)\nx <- data.frame(x1 = x[,1], x2 = x[,2])\n# Save the scatter plot in a variable\np <- ggplot(x, aes(x = x1, y = x2)) +\n  geom_point() + xlim(0,6) + ylim(0,6) + \n  geom_text(aes(x = 4, y = 2, label = \"gamma(2,8) = \\n cov(x_2, x_8) = 0.2\"), size = 6) + coord_fixed() \n\n# Arguments for each marginal histogram\nggMarginal(p, type = \"density\", adjust = 2,\n           xparams = list(col = \"blue\", fill = \"blue\"),\n           yparams = list(col = \"magenta\", fill = \"magenta\"))\n```\n\n## Stationarity\n\nA time series is **stationary** if\n\n-   the mean function ($\\mu_t$) is constant and does not depend on time $t$\n-   the autocovariance function ($\\gamma(s,t)$) depends on $s$ and $t$ only though their difference\n\nAnd **nonstationary** otherwise.\n\n## [Steps to determine whether a time series $x_t$ is stationary:]{.r-fit-text}\n\n1.  Compute the mean function.\n2.  Compute the autocovariance function.\n3.  If both do not depend on $t$, then $x_t$ is stationary. If $\\gamma$ depends on $s$ and $t$ just through the value $s-t$, then $x_t$ is stationary. Otherwise, $x_t$ is nonstationary.\n\n## [Activity 3: Example 2.14 Stationarity of a Random Walk]{.r-fit-text}\n\n$$\nx_t = x_{t-1} + w_t\n$$\n\nLast, time, we saw that the mean function is $\\E(x_t) = 0$, and the autocovariance function is $\\gamma_x(s, t) = \\min\\{s,t\\}\\sigma^2_w$\n\n::: columns\n::: {.column width=\"60%\"}\n1.  **Is** $x_t$ stationary?\n2.  **What if there was drift?**\n:::\n\n::: {.column width=\"10%\"}\n::: {#Stationary .timer seconds=\"180\" starton=\"interaction\"}\n:::\n:::\n:::\n\n## [Activity 3 Solution (Example 2.14 Stationarity of a Random Walk)]{.r-fit-text} {.smaller}\n\n1.  **Is** $x_t$ stationary?\n\nNo, the autcovariance function depends on $t$ (there's a $t$ in the equation): $$\n\\gamma_x(s, t) = \\min\\{s,t\\}\\sigma^2_w\n$$\n\nMore concretely: consider if we want to know the correlation between the random walk at times $s = 2, t = 5$, $$\n\\gamma(2,5) = \\min\\{2,5\\}\\sigma^2_w = 2\\sigma^2_w\n$$ But $\\gamma(3,5) = 3\\sigma^2_w$. So the autocovariance is different depending on which points in time you are considering.\n\n2.  **What if there was drift?**\n\nAgain, no. The mean function of the random walk with drift is $\\mu_t = \\delta t$, which depends on $t$.\n\n## $\\gamma(s,t)$ for a random walk\n\n```{r}\nsigma_w <- 5 #define variance of the white noise\ncoords <- expand.grid(1:20, 1:20)\nnames(coords) <- c(\"s\", \"t\") ## the coordinates represent different possible time points\ncoords$gamma <- pmin(coords$s, coords$t)*sigma_w\n\n\nlibrary(plotly)\nplot_ly(coords,\n  x= ~s, y=~t, z=~gamma, \n  type = 'scatter3d', mode = \"markers\", size = .1)\n```\n\n## Is white noise stationary?\n\n-   Mean function of white noise is $\\E(w_t) = 0$\n-   Autocovariance function is $$\n    \\gamma_w(s, t) = cov(w_s, w_t) =  \\begin{cases} \\sigma^2_w & \\text{ if } s = t\\\\ 0 & \\text{ if } s \\ne t \\end{cases}\n    $$ Since neither depends on $t$, white noise is stationary.\n\n## $\\gamma(s,t)$ for white noise\n\n```{r}\n# plot the autocov's we computed last time, show on model of time series\n```\n\n```{r}\n\n## white noise\nsigma_w <- 5 #define variance of the white noise\ncoords <- expand.grid(1:20, 1:20)\nnames(coords) <- c(\"s\", \"t\") ## the coordinates represent different possible time points\ncoords$gamma <- 0\ncoords$gamma[coords[,1] == coords[,2]] <- sigma_w ## covariance is sigma_w if s = t, 0 otherwise\n\n\nlibrary(plotly)\nplot_ly(coords,\n  x= ~s, y=~t, z=~gamma, \n  type = 'scatter3d', mode = \"markers\", size = .1)\n```\n\n## Break\n\n::: {#Break .timer seconds=\"360\" starton=\"interaction\"}\n:::\n\n## Activity 4\n\nWhich of the following time series are stationary?\n\n::: columns\n::: {.column width=\"60%\"}\n![From Forecasting Principles and Practice Chapter 9](https://otexts.com/fpp3/fpp_files/figure-html/stationary-1.png)\n:::\n\n::: {.column width=\"20%\"}\n::: {#VisualStationary .timer seconds=\"300\" starton=\"interaction\"}\n:::\n:::\n:::\n\n## Activity 4 (solution)\n\n-   (a), (c), (e), (f) (i) are clearly non-stationary in the mean.\n-   (d), (h) have seasonal patterns\n-   \\(i\\) has increasing variance\n-   \\(b\\) and (g) are stationary\n\n## Why is stationarity important?\n\n-   In order to measure correlation between contiguous time points\n-   To avoid spurious correlations in a regression setting\n-   Simplifies how we can write the autocovariance and autocorrelation functions\n\n## Autocorrelation function\n\nThe autocorrelation function (acf) of a time series is: $$\n\\rho(s, t) = \\frac{\\gamma(s,t)}{\\sqrt{\\gamma(s,s)\\gamma(t,t)}}\n$$ i.e. the autocovariance divided by the standard deviation of the process at each time point.\n\n## [Autocovariance and Autocorrelation for Stationary Time series]{.r-fit-text} {.smaller}\n\nSince for stationary time series the autocovariance depends on $s$ and $t$ only through their difference, we can write the covariance as: $$\n\\gamma(s,t) = \\gamma(h) = cov(x_{t+h}, x_t) = \\E[(x_{t+h} - \\mu)(x_t-\\mu)]\n$$ and the correlation as: $$\n\\rho(s,t) = \\rho(h) = \\frac{\\gamma(h)}{\\gamma(0)}\n$$ $h = s-t$ is called the **lag**.\n\n## [Autocorrelation function of a three-point moving average]{.r-fit-text} {.smaller}\n\n$\\gamma_v(s, t) = cov(v_s, v_t) =  \\begin{cases}\\frac{3}{9}\\sigma^2_w & \\text{ if } s = t\\\\ \\frac{2}{9}\\sigma^2_w & \\text{ if } \\vert s-t \\vert = 1 \\\\\\frac{1}{9}\\sigma^2_w & \\text{ if } \\vert s-t \\vert =2 \\\\0 & \\text{ if } \\vert s - t\\vert > 2\\end{cases}$\n\n::: columns\n::: column\nSince $v$ is stationary, we can write\n\n$\\gamma_v(h) = \\begin{cases}\\frac{3}{9}\\sigma^2_w & \\text{ if } h = 0\\\\ \\frac{2}{9}\\sigma^2_w & \\text{ if } h = \\pm1 \\\\\\frac{1}{9}\\sigma^2_w & \\text{ if }h = \\pm 2 \\\\0 & \\text{ if } h> 2\\end{cases}$\n:::\n\n::: column\nAnd the autocorrelation is:\n\n$\\rho(h) = \\begin{cases}1 & \\text{ if } h = 0\\\\ \\frac{2}{3} & \\text{ if } h = \\pm1 \\\\\\frac{1}{3} & \\text{ if }h = \\pm 2 \\\\0 & \\text{ if } h> 2\\end{cases}$\n:::\n:::\n\n## [Autocorrelation function of a three-point moving average]{.r-fit-text}\n\nIn R, we can plot $\\rho(h)$\n\n```{r}\n#| echo: true\n\nACF = c(0,0,0,1,2,3,2,1,0,0,0)/3\nLAG = -5:5\ntsplot(LAG, ACF, type=\"h\", lwd=3, xlab=\"LAG\")   \nabline(h=0)\npoints(LAG[-(4:8)], ACF[-(4:8)], pch=20)\naxis(1, at=seq(-5, 5, by=2))  \n\n```\n\n## Activity 5 {.smaller}\n\n1.  Predict what the acf will look like for the ar(1) process?\n2.  Simulate an ar(1) process and compute the acf. Were you correct?\n3.  What is the lag 0 autocorrelation? Explain why its value makes sense.\n\n```{r}\n#| echo: true\n# simulate from an ar(1)\n\n# use acf() function to plot acf\n\n# save output of acf and inspect\n\n```\n\n::: {#activity4 .timer seconds=\"300\" starton=\"interaction\"}\n:::\n\n## Activity 5 (solution)\n\n```{r}\n#| echo: true\n# simulate from an ar(1)\nw <- rnorm(500)\nar_1 <- stats::filter(w, filter = 0.8, method = \"recursive\")\n# use acf() function\nacf(ar_1)\n\n## what is the lag 1 correlation?\nacf_output <- acf(ar_1, plot = F)\nacf_output$acf[2] ## lag 1 autocorrelation\n```\n\n# Questions on the quiz?\n\n## Activity 6 (Problem 2.3) {.smaller}\n\nWhen smoothing time series data, it is sometimes advantageous to give decreasing amounts of weights to values farther away from the center. Consider the simple two-sided moving average smoother of the form: $$\nv_t = \\frac{1}{4}(w_{t-1} + 2w_t + w_{t+1})\n$$ Where $w_t$ are white noise. The autocovariance as a function of $h$ is: $$\\gamma_v(s, t) = cov(v_s, v_t) =  \\begin{cases}\\frac{6}{16}\\sigma^2_w & \\text{ if } h = 0\\\\ \\frac{4}{16}\\sigma^2_w & \\text{ if } h = \\pm 1 \\\\\\frac{1}{16}\\sigma^2_w & \\text{ if } h = \\pm 2 \\\\0 & \\text{ if } h> 2\\end{cases}$$ 1. Compare to the [autocovariance equation for the unweighted 3 point moving average from Lecture 2](https://juliaschedler.github.io/Stat416Fall24/LectureNotes/Lecture2.html#example-2.8-autocovariance-of-a-moving-average-1). Comment on the differences.\n\n2.  Write down the autocorrelation function.\n\n::: {#activity5 .timer seconds=\"300\" starton=\"interaction\"}\n:::\n\n## Activity 6 Solution\n\n1.  6/16 \\> 3/9, the \"present\" is weighted higher in the weighted average which impacts the covariance.\n2.  Divide each term by the variance ($\\gamma(0)$): $$\\rho_v(s, t) = cor(v_s, v_t) =  \\begin{cases}1 & \\text{ if } h = 0\\\\ \\frac{4}{6} & \\text{ if } h = \\pm 1 \\\\\\frac{1}{6} & \\text{ if } h = \\pm 2 \\\\0 & \\text{ if } h> 2\\end{cases}$$\n\n## Activity 7\n\nRecall the decomposition of the Johnson and Johnson quarterly earnings.\n\n```{r}\n#| echo: true\n\nplot(decompose(jj)) ## plot decomposition\n```\n\n1.  Is the series stationary?\n2.  Does the acf of the random component look like white noise?\n\n::: {#activity6 .timer seconds=\"300\" starton=\"interaction\"}\n:::\n\n## Activity 7 Solution\n\n```{r}\n#| echo: true\n\njj_decomp <- decompose(jj)\n\npar(mfrow=2:1)\nacf(jj_decomp$random, na.action = na.pass) ## acf of random component\nacf(rnorm(length(jj))) ## acf of white noise of same length\n```\n\n## Coming up:\n\n-   Assignment 1 due at midnight\n-   Assignment 2 posted later\n-   Part of this will be involve \"reading\" the textbook! (collecting data on how you feel about the math)\n-   Next Lecture:\n    -   Regression with time\n    -   Cross-correlation\n    -   Inducing stationarity\n"},"formats":{"revealjs":{"identifier":{"display-name":"RevealJS","target-format":"revealjs","base-format":"revealjs"},"execute":{"fig-width":10,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":false,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":true,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","html-math-method":{"method":"mathjax","url":"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML-full"},"slide-level":2,"to":"revealjs","filters":["timer"],"output-file":"index.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words"},"metadata":{"lang":"en","fig-responsive":false,"quarto-version":"1.4.555","auto-stretch":true,"editor":"visual","title":"Lecture 3","author":"Julia Schedler","slideNumber":true,"scrollable":true}}},"projectFormats":["html"]}